{
  "0": "Essays on data-driven decision support:\nApplications in HRM and methodological advances\nDissertation presented to obtain the degree of\nDoctor in Business Economics at KU Leuven\nby\nSimon De Vos\nNumber 000\nSeptember 2025\n  Since the theses in the series published by the Faculty of Economics and Business\nare the personal work of their authors, only the latter bear full responsibility.\niii\n  Doctoral Committee\nPromotor\nProf. Dr. Wouter Verbeke\nKU Leuven\nDoctoral committee\nProf. Dr. Johannes De Smedt\nKU Leuven\nProf. Dr. Marijke Verbruggen\nKU Leuven\nProf. Dr. Chris Wuytens\nKU Leuven\nProf. Dr. Stefan Lessmann\nHumboldt University of Berlin\nChair\nProf. Dr. Robert Boute\nKU Leuven\nv\n  Acknowledgements\nLeft empty for acknowledgements\nSimon De Vos\nvii\n  page for dedication\n  Summary\nData plays an increasingly important role in managerial decision-making.\nImproved data availability, greater computing resources, and advances in\nmachine learning (ML) enable organizations to derive insights from histor-\nical data through pattern recognition, helping them anticipate future out-\ncomes. Analytical methods must match the decision problem and context at\nhand. This holds especially for applications in human resource management\n(HRM), where decisions impact individuals directly and legal and ethical\nconsiderations are critical.\nThis dissertation builds on three types of analytics: descriptive to sum-\nmarize past patterns, predictive to estimate future outcomes, and prescrip-\ntive to recommend actions. Each serves a different role in decision-making\nand shapes the contributions in both parts of the dissertation.\nPart I covers HR analytics applications, focusing on internal mobility,\nemployee turnover, and internal job matching. The work was developed in\nclose collaboration with Acerta, an HR services provider, through an iterative\nprocess of discussion, implementation, and validation.\nPart II presents methodological contributions that generalize beyond HR\nto bring ML closer to the complexities of real-world decision-making.\nIt\nintroduces robust techniques for instance-dependent cost-sensitive classifi-\ncation, fairness-aware learning for resource allocation, and a predict-then-\noptimize framework for uplift modeling with continuous treatments.\nBy combining applied and methodological work, the dissertation helps\nclose the gap between technical developments and practical needs. It offers\nconcrete tools for organizations aiming to make better decisions with data.\nxi\n  Contents\nDoctoral Committee\nv\nAcknowledgements\nvii\nSummary\nxi\nPrologue\n1\n1\nIntroduction\n3\n1.1\nDecision support: From descriptive\nto predictive and prescriptive analytics . . . . . . . . . . . . .\n3\n1.2\nPart I: HR analytics applications . . . . . . . . . . . . . . . .\n5\n1.3\nPart II: Methodological advances . . . . . . . . . . . . . . . .\n6\n1.4\nPublications and outline . . . . . . . . . . . . . . . . . . . . .\n8\nI\nHR analytics applications\n13\n2\nLeveraging process mining to optimize internal employee\nmobility strategies\n15\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.2\nSituation faced . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.2.1\nNeed for strategic decision support . . . . . . . . . . .\n17\n2.2.2\nProblems in current internal mobility management . .\n18\n2.3\nAction taken\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.3.1\nProcess mining as a solution\n. . . . . . . . . . . . . .\n19\n2.3.2\nData requirements . . . . . . . . . . . . . . . . . . . .\n21\n2.4\nResults achieved\n. . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2.4.1\nGeneral results . . . . . . . . . . . . . . . . . . . . . .\n22\n2.4.2\nConcrete HR Cases . . . . . . . . . . . . . . . . . . . .\n23\n2.5\nLessons learned . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.5.1\nChallenges\n. . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.5.2\nInsights gained . . . . . . . . . . . . . . . . . . . . . .\n26\n2.5.3\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . .\n27\nxiii\n Contents\n3\nPredicting employee turnover:\nScoping and benchmarking the state-of-the-art\n29\n3.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.2\nRelated literature . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3.3\nScoping predictive analytics for employee turnover\n. . . . . .\n34\n3.3.1\nReview methodology . . . . . . . . . . . . . . . . . . .\n34\n3.3.2\nThe scope of predictive analytics for employee turnover 36\n3.3.3\nResearch gaps in predictive analytics\nfor employee turnover\n. . . . . . . . . . . . . . . . . .\n39\n3.4\nExperimental design . . . . . . . . . . . . . . . . . . . . . . .\n39\n3.4.1\nClassification methods . . . . . . . . . . . . . . . . . .\n40\n3.4.2\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.4.3\nData preprocessing and partitioning . . . . . . . . . .\n42\n3.4.4\nPerformance metrics . . . . . . . . . . . . . . . . . . .\n43\n3.4.5\nStatistical tests . . . . . . . . . . . . . . . . . . . . . .\n43\n3.5\nEmpirical results . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n3.5.1\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n3.5.2\nEffect of class balancing and feature selection . . . . .\n45\n3.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4\nData-driven internal mobility:\nSimilarity regularization gets the job done\n53\n4.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n4.2\nRelated work . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n4.2.1\nPredictive HR analytics . . . . . . . . . . . . . . . . .\n57\n4.2.2\nMatching\n. . . . . . . . . . . . . . . . . . . . . . . . .\n57\n4.2.3\nPost-hire setting\n. . . . . . . . . . . . . . . . . . . . .\n59\n4.3\nMethodology\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n4.3.1\nProblem definition . . . . . . . . . . . . . . . . . . . .\n59\n4.3.2\nEvent log as starting point\n. . . . . . . . . . . . . . .\n61\n4.3.3\nCollaborative filtering . . . . . . . . . . . . . . . . . .\n62\n4.3.4\nMatrix factorization with similarity regularization\n. .\n63\n4.4\nExperimental evaluation . . . . . . . . . . . . . . . . . . . . .\n65\n4.4.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n4.4.2\nExperimental setup . . . . . . . . . . . . . . . . . . . .\n68\n4.5\nResults and discussion . . . . . . . . . . . . . . . . . . . . . .\n70\n4.5.1\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n4.5.2\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n4.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\nxiv\n Contents\nII\nMethodological advances\n77\n5\nRobust instance-dependent cost-sensitive classification\n79\n5.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n5.2\nRelated work . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n5.2.1\nIDCS learning, cslogit, and robustness . . . . . . . . .\n81\n5.2.2\nPreliminaries . . . . . . . . . . . . . . . . . . . . . . .\n82\n5.3\nSensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . .\n84\n5.3.1\nSimulation setup . . . . . . . . . . . . . . . . . . . . .\n84\n5.3.2\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.4\nRobust IDCS . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.5\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n5.5.1\nSynthetic data\n. . . . . . . . . . . . . . . . . . . . . .\n91\n5.5.2\nSensitivity analysis on real data . . . . . . . . . . . . .\n93\n5.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n6\nDecision-centric fairness: Evaluation\nand optimization for resource allocation problems\n97\n6.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n6.2\nBackground and related work . . . . . . . . . . . . . . . . . . 100\n6.2.1\nClassification and resource allocation . . . . . . . . . . 101\n6.2.2\nFairness notions\n. . . . . . . . . . . . . . . . . . . . . 101\n6.2.3\nDemographic parity in resource allocation . . . . . . . 103\n6.3\nDecision-centric demographic parity\n. . . . . . . . . . . . . . 104\n6.3.1\nEvaluating decision-centric fairness . . . . . . . . . . . 104\n6.3.2\nInducing decision-centric fairness . . . . . . . . . . . . 105\n6.4\nExperimental design . . . . . . . . . . . . . . . . . . . . . . . 107\n6.4.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.4.2\nEvaluation metrics . . . . . . . . . . . . . . . . . . . . 108\n6.4.3\nProblem and hyperparameter configurations . . . . . . 110\n6.5\nResults and discussion . . . . . . . . . . . . . . . . . . . . . . 111\n6.5.1\nQ1: Impact of decision-centric versus global fairness\napproach on predictive performance\n. . . . . . . . . . 111\n6.5.2\nQ2: Impact of decision-making region size and level of\ndiscriminatory bias in historical data . . . . . . . . . . 113\n6.5.3\nQ3: Impact of decision-centric fairness metric used for\nevaluation and model selection\n. . . . . . . . . . . . . 114\n6.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7\nUplift modeling with continuous treatments:\nA predict-then-optimize approach\n119\n7.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n7.2\nUplift modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 122\nxv\n Contents\n7.2.1\nPurpose and definition . . . . . . . . . . . . . . . . . . 122\n7.2.2\nTreatment effects . . . . . . . . . . . . . . . . . . . . . 123\n7.2.3\nAllocation task . . . . . . . . . . . . . . . . . . . . . . 124\n7.3\nProblem formulation . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3.1\nNotation . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3.2\nPrediction step . . . . . . . . . . . . . . . . . . . . . . 128\n7.3.3\nOptimization step\n. . . . . . . . . . . . . . . . . . . . 129\n7.4\nMethodology\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n7.4.1\nPredictive model for CADR estimation . . . . . . . . . 131\n7.4.2\nILP for the dose-allocation problem\n. . . . . . . . . . 132\n7.5\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.5.1\nData . . . . . . . . . . . . . . . . . . . .",
  "1": "proach\n119\n7.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n7.2\nUplift modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 122\nxv\n Contents\n7.2.1\nPurpose and definition . . . . . . . . . . . . . . . . . . 122\n7.2.2\nTreatment effects . . . . . . . . . . . . . . . . . . . . . 123\n7.2.3\nAllocation task . . . . . . . . . . . . . . . . . . . . . . 124\n7.3\nProblem formulation . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3.1\nNotation . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3.2\nPrediction step . . . . . . . . . . . . . . . . . . . . . . 128\n7.3.3\nOptimization step\n. . . . . . . . . . . . . . . . . . . . 129\n7.4\nMethodology\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n7.4.1\nPredictive model for CADR estimation . . . . . . . . . 131\n7.4.2\nILP for the dose-allocation problem\n. . . . . . . . . . 132\n7.5\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.5.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.5.2\nEvaluation metrics . . . . . . . . . . . . . . . . . . . . 135\n7.5.3\nResults and discussion . . . . . . . . . . . . . . . . . . 136\n7.6\nConclusion, limitations, and further research . . . . . . . . . . 142\nEpilogue\n145\n8\nIndustry co-creation\n147\n8.1\nEmployee journey mapping\n. . . . . . . . . . . . . . . . . . . 148\n8.2\nTurnover prediction\n. . . . . . . . . . . . . . . . . . . . . . . 149\n8.3\nInternal mobility recommender system . . . . . . . . . . . . . 152\n8.4\nPotential implementation of other chapters\n. . . . . . . . . . 155\n9\nConclusion\n159\n9.1\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n9.2\nManagerial implications . . . . . . . . . . . . . . . . . . . . . 160\n9.3\nLimitations and future work . . . . . . . . . . . . . . . . . . . 162\nReferences\n165\nAppendices\n201\nA Predicting employee turnover:\nScoping and benchmarking the state-of-the-art\n203\nA.1 Search query\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 204\nA.2 Established classifiers and their\ncorresponding studies\n. . . . . . . . . . . . . . . . . . . . . . 205\nA.3 Hyperparameter search space . . . . . . . . . . . . . . . . . . 206\nA.4 Detailed results per dataset . . . . . . . . . . . . . . . . . . . 207\nxvi\n Contents\nB Data-driven internal mobility:\nSimilarity regularization gets the job done\n211\nB.1\nEmployee journey map . . . . . . . . . . . . . . . . . . . . . . 212\nB.2\nCollaborative filtering\nthrough matrix factorization . . . . . . . . . . . . . . . . . . . 213\nB.3\nHyperparameter tuning\n. . . . . . . . . . . . . . . . . . . . . 214\nC Robust instance-dependent cost-sensitive classification\n215\nC.1 Results on synthetic data\n. . . . . . . . . . . . . . . . . . . . 216\nD Decision-centric fairness: Evaluation and optimization\nfor resource allocation problems\n219\nD.1 Dataset details . . . . . . . . . . . . . . . . . . . . . . . . . . 220\nD.1.1\nLabel flipping for inducing additional bias . . . . . . . 220\nD.1.2\nBaseline discriminatory behavior . . . . . . . . . . . . 220\nD.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . 222\nD.3 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . 223\nE Uplift modeling with continuous treatments:\nA predict-then-optimize approach\n227\nE.1\nNotation overview\n. . . . . . . . . . . . . . . . . . . . . . . . 228\nE.2\nAssumptions and mathematical justification\nof CADE identification . . . . . . . . . . . . . . . . . . . . . . 229\nE.3\nDetails regarding semi-synthetic data . . . . . . . . . . . . . . 230\nE.4\nNumber of dose bins δ . . . . . . . . . . . . . . . . . . . . . . 231\nE.5\nHyperparameter tuning\n. . . . . . . . . . . . . . . . . . . . . 232\nE.6\nMore details on Experiment 1 . . . . . . . . . . . . . . . . . . 233\nE.6.1\nDose-response estimation\n. . . . . . . . . . . . . . . . 233\nE.6.2\nScalability . . . . . . . . . . . . . . . . . . . . . . . . . 233\nF Industry co-creation\n235\nF.1\nImplementation of EJMs . . . . . . . . . . . . . . . . . . . . . 236\nF.2\nImplementation of turnover prediction . . . . . . . . . . . . . 237\nPublication list\n239\nCode availability\n243\nUse of generative AI\n245\nxvii\n  Prologue\n1\n  1\nIntroduction\nData-driven approaches play an increasingly significant role in managerial\ndecision-making [1].\nImproved data availability, increased computing re-\nsources, and advances in machine learning (ML) enable organizations to\nderive insights from historical data, identify patterns, anticipate future out-\ncomes, and inform actions [2]. Other than effective data management —\ncovering aspects such as collection practices, assuring data quality, storage,\nand governance — it is essential to rely on analytical methods suited to the\nspecific problem at hand. Reliable insights require a precise alignment be-\ntween problem characteristics and analytical approaches. Given the growing\nenthusiasm for ML and AI — and the accompanying risk of inflated expecta-\ntions — it is crucial to carefully develop, evaluate, and select methods that\ngenuinely address organizational needs. The alignment of analytical meth-\nods with domain-specific goals, constraints, and challenges is particularly\nimportant in sensitive fields like human resource (HR) management, where\ndata is often more sensitive, decisions have direct implications for individ-\nuals, and privacy and fairness concerns are heightened compared to other\nmanagerial decision-making domains.\n1.1\nDecision support: From descriptive\nto predictive and prescriptive analytics\nData-driven decision-making can be supported by three progressively sophis-\nticated categories of analytics:\n1. Descriptive analytics summarizes historical data, uncovering pat-\nterns that explain past events. It addresses questions like: ‘What happened\nin the past, and what are the key trends or patterns in the data?’ Formally,\ngiven dataset D = {x1, ..., xk}N\ni=1, descriptive analytics employs an operator\nS ∶D →P, mapping raw data into a set of summary statistics or patterns\nP. For Chapter 2, this includes discovering employee mobility patterns from\nhistorical HR data.\n3\n Chapter 1: Introduction\n2.\nPredictive analytics goes a step further by using statistical and\nML methods to forecast future outcomes based on historical data. It aims\nto answer the question ‘What is likely to happen in the future based on his-\ntorical data?’. Here, an outcome Y is approximated by a predictive model as\nY = f(X), where input features X predict outcome Y . The function f is es-\ntimated based on historical data. Chapter 3 employs predictive analytics to\nestimate employee turnover probability. Chapters 5 and 6 methodologically\nintegrate additional managerial considerations — cost-sensitivity and fair-\nness, respectively — to better align model predictions with organizational\nobjectives.\n3. Prescriptive analytics recommends actionable decisions by opti-\nmizing outcomes given input features and constraints. It aims to answer the\nquestion ‘What actions should be taken to achieve a desired outcome?’ and\nis formalized as:\nY = f(X, T),\nT ∗= arg max\nT ∈T Y (X, T),\nwhere T represents the feasible set of decisions and T ∗represents an\noptimal decision. The complexity of the argmax operator can vary strongly\ndepending on the use case’s context. Chapter 4 focuses on recommending\na next career step by predicting a job-employee fit score and selecting the\nhighest-ranking option as a recommendation. In contrast, Chapter 7 also\nconsiders cost-sensitivity and fairness in the decision-making step, making\nthe argmax operator more complex. In such cases, more advanced optimiza-\ntion techniques are required to effectively identify an optimal decision.\nFrom analysis to decisions\nThe analytical methods differ in how they support decision-making: de-\nscriptive analytics informs without making predictions or prescribing actions;\npredictive analytics forecasts outcomes, indirectly influencing decisions; pre-\nscriptive analytics explicitly integrates decision-making into analytical mod-\nels, directly guiding optimal actions.\nRegardless of these distinctions, translating analytical outputs into man-\nagerial decisions remains challenging. This difficulty primarily arises from\nuncertainty: future states and their associated outcomes often cannot be\nprecisely estimated or fully quantified. Analytical insights — whether de-\nscriptive, predictive, or prescriptive — are therefore always conditional on\nassumptions, approximations, and data quality. Effective use of analytics\nthus demands rigorous problem formulation, explicit alignment of analytical\nmethods with organizational goals and constraints, and careful consideration\nof how uncertainty impacts decisions.\n4\n 1.2. Part I: HR analytics applications\nResearch objectives and structure\nThis dissertation addresses two central research questions:\n1. How can analytics effectively support decision-making within HR man-\nagement, particularly regarding internal mobility and employee turnover?\n2. How can analytical methods be adapted — specifically by incorpo-\nrating fairness and cost-sensitivity — to better align with managerial\nobjectives?\nTo systematically approach these questions, this dissertation is organized\nin two parts. Part I focuses on HR analytics applications, showcasing con-\ntributions to optimize HR-related decisions. Part II introduces methodolog-\nical advances on robustness, fairness, cost-sensitivity, and continuous-valued\ntreatments to better align analytics with the intricacies of real-world man-\nagerial contexts.\nTogether, these contributions in applied and methodological research are\nintended to empower organizations to make effective, efficient, and ethical\ndecisions that support their managerial objectives.\n1.2\nPart I: HR analytics applications\nAs defined by [3]: ‘HR analytics involves the systematic application of an-\nalytical techniques — including statistical modeling, data mining, and ML\n— to address challenges in HR management and establish measurable busi-\nness impact through data-driven decisions.’ In this dissertation,",
  "2": "irness and cost-sensitivity — to better align with managerial\nobjectives?\nTo systematically approach these questions, this dissertation is organized\nin two parts. Part I focuses on HR analytics applications, showcasing con-\ntributions to optimize HR-related decisions. Part II introduces methodolog-\nical advances on robustness, fairness, cost-sensitivity, and continuous-valued\ntreatments to better align analytics with the intricacies of real-world man-\nagerial contexts.\nTogether, these contributions in applied and methodological research are\nintended to empower organizations to make effective, efficient, and ethical\ndecisions that support their managerial objectives.\n1.2\nPart I: HR analytics applications\nAs defined by [3]: ‘HR analytics involves the systematic application of an-\nalytical techniques — including statistical modeling, data mining, and ML\n— to address challenges in HR management and establish measurable busi-\nness impact through data-driven decisions.’ In this dissertation, HR is ex-\nplicitly considered a strategic organizational partner rather than merely an\nadministrative function [4]. Driven by intensified competition for talent and\nrecognition of human capital as a crucial organizational asset [5], HR ana-\nlytics leverages employee-related data to support decisions on, for example,\nhiring, retention, internal mobility, and talent development. As discussed in\nSection 1.1, because of improved data availability, increased computational\nresources, and methodological advancements, also the potential of HR an-\nalytics has been transformed from traditional descriptive reporting toward\nsophisticated predictive and prescriptive approaches [6].\nDespite these opportunities, implementing HR analytics remains chal-\nlenging. Employee data often resides in fragmented and incompatible sys-\ntems, complicating integration and meaningful analysis [5].\nAdditionally,\nethical debates and regulations such as GDPR and the European AI Act im-\npose significant compliance requirements regarding fairness, transparency,\nand bias mitigation in data-driven HR decisions [7], [8]. Furthermore, HR\noutcomes are often intangible, delayed, difficult to measure, or context-\ndependent, hindering straightforward modeling and evaluation. Consequently,\n5\n Chapter 1: Introduction\nwhile some organizations successfully apply predictive and prescriptive an-\nalytics, many continue to rely primarily on descriptive methods [3], [5]. A\nnotable gap thus persists between academic research — which often pri-\noritizes methodological innovation — and industry practice, emphasizing\ninterpretability, practical usability, and immediate applicability.\nThis dissertation bridges these perspectives by presenting practical HR\nanalytics applications in Part I, focusing on internal mobility patterns (Chap-\nter 2), predicting employee turnover (Chapter 3), and optimizing internal job\nrecommendations (Chapter 4). Collectively, these chapters demonstrate con-\ncrete analytical approaches that organizations can adopt, emphasizing both\nmethodological rigor and practical relevance.\n1.3\nPart II: Methodological advances\nStandard analytical methods typically optimize traditional performance met-\nrics, such as predictive accuracy or precision [2]. However, organizational\ndecision-making frequently demands incorporating additional considerations\nbeyond these metrics, such as cost-sensitivity and algorithmic fairness [9],\n[10]. Cost-sensitive analytics and algorithmic fairness are relevant in many\ndomains, including HR, where decisions carry direct ethical implications\nand financial consequences [11], [12]. To tackle these requirements, Part II\nof this dissertation introduces methodological advances specifically focused\non cost-sensitive analytics and fairness-aware decision-making.\nAlthough\nthese methods are illustrated in application contexts other than HR — such\nas fraud detection, marketing, healthcare, or lending — the contributions\npresented are fundamentally methodological and remain broadly applicable.\nCost-sensitive learning is an ML paradigm that explicitly accounts\nfor costs or benefits associated with prediction errors or correct decisions\n[13]. Unlike conventional approaches that typically maximize accuracy-based\nmetrics, cost-sensitive learning aims to minimize total incurred costs — or,\nconversely, maximize overall benefits — associated with model predictions or\ndecisions. This is particularly valuable when misclassification consequences\nare asymmetric or when specific outcomes entail higher costs than oth-\ners. For instance, in fraud detection, the primary objective is not simply\nmaximizing the detection rate but minimizing total financial losses arising\nfrom fraudulent activities [14]. Prioritizing the investigation of high-value\ntransactions, even at the cost of overlooking less consequential cases, better\naligns modeling objectives with real-world business priorities.\nThe cost-\nsensitive approach can be incorporated either directly during the training\nphase (predict-and-optimize, Chapter 5) or through a separate post-training\noptimization step (predict-then-optimize, Chapter 7).\n6\n 1.3. Part II: Methodological advances\nIn a similar vein, cost-sensitive learning is relevant to HR analytics ap-\nplications. In a predictive context, it enables modeling employee turnover\nwith greater nuance. Although Chapter 3 addresses turnover prediction as\na binary classification task, not all employee departures are equally undesir-\nable — some turnover instances may even have beneficial effects [15]. Cost-\nsensitive methods, as the methodological focus of Chapter 5, incorporate\nthese nuances by assigning differentiated costs or benefits to turnover cases.\nIn a prescriptive setting, cost-sensitive analytics guides resource allocation\ntoward higher-impact HR decisions. For example, retention strategies might\nprioritize functions that are difficult or costly to replace, while recruitment\nefforts may target candidates promising the highest returns, as illustrated\nmethodologically in Chapter 7.\nFairness in algorithmic decision-making addresses the ethical concern\nthat data-driven methods, like ML models, should not perpetuate or amplify\ndiscriminatory biases present in historical data [16].\nIn HR applications,\nfairness is particularly relevant because these systems often involve sensitive\npersonal data and have direct implications for individuals.\nFor example,\nbiased algorithms can lead to unfair hiring or promotion practices, which\nis evident from the infamous Amazon hiring case, which was discriminatory\ntoward women [17]. Moreover, beyond the ethical aspects, there are also legal\nimplications. The European AI Act [18] categorizes most AI applications in\nHR as high-risk. This means that organizations using such tools in the EU for\ndecision-making have ‘strict obligations’ to ‘minimize risks of discriminatory\noutcomes’ [18].\nBy incorporating fairness considerations into their modeling process, or-\nganizations can ensure that their decisions are not only effective in terms of\nstandard metrics like accuracy or profit but, also align with broader business\nobjectives and adhere to fairness principles. However, fairness in algorithmic\ndecision-making is inherently complex, making the selection of an appropri-\nate fairness concept challenging [19]. Many commonly used fairness metrics,\nlike demographic parity and equal opportunity, are incompatible with each\nother unless under very specific conditions [20]. Although the literature on\nalgorithmic fairness presents a wide range of fairness notions and metrics\n[16], [21], we focus specifically on fairness through independence concepts.\nThis dissertation focuses on fairness in a predictive setting (Chapter 6),\nwhere we consider a predict-and-optimize approach to include fairness dur-\ning the model training process, and in a prescriptive setting (Chapter 7),\nwhere fairness is considered a constraint in a predict-then-optimize setting.\n7\n Chapter 1: Introduction\n1.4\nPublications and outline\nWe conclude this introduction by highlighting the main contributions of\nthis dissertation and outlining its structure. The dissertation’s structure is\npresented in Figure 1.1. The main body consists of Chapters 2-7, organized\ninto Part I and Part II.\nChapter 2 presents a case study with Acerta, where internal mobility is ex-\namined using process analytics. Specifically, process discovery techniques are\napplied to HR event logs to generate employee journey maps that visualize\ncareer paths within the organization. These maps reveal that internal mobil-\nity is often more complex than assumed, highlighting discrepancies between\nexpected career trajectories and actual mobility patterns. By highlighting\nuncommon growth paths, stepping stones to certain functions, and describ-\ning hard-to-fill positions, the study provides a descriptive tool for managing\ninternal employee mobility. This chapter has been published as [22]:\nS. De Vos, J. De Smedt, C. Wuytens, et al., “Leveraging process mining\nto optimize internal employee mobility strategies,” in Business Process\nManagement Cases Vol. 3: Implementation in Practice, Springer, 2025,\npp. 15–28\nChapter 3 addresses employee turnover using predictive analytics. Em-\nployee turnover imposes substantial costs on organizations, not only due to\nrecruitment and training expenses but also through the loss of expertise and\nreduced productivity, making the prediction of which employees are at risk of\nleaving crucial for improving retention strategies and ensuring workforce sta-\nbility. To address the fragmented nature of existing research in this area, the\ncontribution of this chapter is twofold. It first presents a scoping review that\nhighlights inconsistencies in existing research and provides a comprehensive\nconsolidation of existing literature. Next, it provides a benchmarking exper-\niment with 14 classification methods on 9 datasets. The combination of both\ncontributions provides a structured overview of the current ",
  "3": "g process mining\nto optimize internal employee mobility strategies,” in Business Process\nManagement Cases Vol. 3: Implementation in Practice, Springer, 2025,\npp. 15–28\nChapter 3 addresses employee turnover using predictive analytics. Em-\nployee turnover imposes substantial costs on organizations, not only due to\nrecruitment and training expenses but also through the loss of expertise and\nreduced productivity, making the prediction of which employees are at risk of\nleaving crucial for improving retention strategies and ensuring workforce sta-\nbility. To address the fragmented nature of existing research in this area, the\ncontribution of this chapter is twofold. It first presents a scoping review that\nhighlights inconsistencies in existing research and provides a comprehensive\nconsolidation of existing literature. Next, it provides a benchmarking exper-\niment with 14 classification methods on 9 datasets. The combination of both\ncontributions provides a structured overview of the current state-of-the-art,\nidentifies key challenges in employee turnover prediction, and provides a fo-\ncal point on employee turnover prediction research. This chapter has been\npublished as [23]:\nS. De Vos, C. Bockel-Rickermann, J. Van Belle, et al., “Predicting employee\nturnover: Scoping and benchmarking the state-of-the-art,” Business &\nInformation Systems Engineering, pp. 1–20, 2024\nChapter 4 explores how prescriptive analytics can support internal mo-\nbility within organizations, taking the process perspective as introduced by\nChapter 2 as a starting point. It presents a data-driven recommender system\n8\n 1.4. Publications and outline\nPrescriptive\nPredictive\nDescriptive\nChapter 2\nLeveraging process mining to optimize\ninternal employee mobility strategies\nChapter 3\nPredicting employee turnover: Scoping\nand benchmarking the state-of-the-art\nChapter 4\nData-driven internal mobility: Similarity\nregularization gets the job done\nPrologue\nChapter 1\nIntroduction\nPart I\nHR analytics\napplications\nPart II\nMethodological\nadvances\nEpilogue\nChapter 9\nConclusion\nChapter 5\nRobust instance-dependent \ncost-sensitive classification\nChapter 6\n Decision-centric fairness: Evaluation and\noptimization for classification problems\nChapter 7\nUplift modeling with continuous\ntreatments: A predict-then-optimize\napproach\nChapter 8\nIndustry co-creation\nFigure 1.1: Overview of this dissertation’s structure. The main body consists\nof Chapters 2-7, organized into Part I and Part II.\n9\n Chapter 1: Introduction\nthat matches employees to internal job opportunities. The system builds on\ncollaborative filtering techniques, enhanced with a similarity-based regular-\nization term incorporating employee characteristics. This addition addresses\nthe cold start issue often faced in internal placement systems. The approach\nis evaluated using three real-life datasets, showing strong performance com-\npared to established benchmarks. In addition to the technical contributions,\nwe publish a new HR dataset for further research. This chapter has been\npublished as [24]:\nS. De Vos, J. De Smedt, M. Verbruggen, et al., “Data-driven internal mo-\nbility: Similarity regularization gets the job done,” Knowledge-Based\nSystems, vol. 295, p. 111 824, 2024\nChapter 5 presents a methodological contribution to cost-sensitive pre-\ndictive analytics.\nSpecifically, we focus on improving instance-dependent\ncost-sensitive (IDCS) learning methods. While such methods are useful for\nbinary classification tasks with varying misclassification costs, we experimen-\ntally show that they are often not robust to noise and outliers concerning\ncost information. This chapter introduces an end-to-end method to enhance\ntheir robustness: detecting outliers, correcting their cost data if needed,\nand integrating the adjusted information into the IDCS method. When im-\nplemented with cslogit, this approach results in r-cslogit, a more resilient\nmethod that achieves better performance across different noise levels. This\nchapter has been published as [25]:\nS. De Vos, T. Vanderschueren, T. Verdonck, et al., “Robust instance-dependent\ncost-sensitive classification,” Advances in Data Analysis and Classifica-\ntion, vol. 17, no. 4, pp. 1057–1079, 2023\nChapter 6 provides a methodological contribution to predictive analytics\nby proposing a decision-centric approach to fairness in binary classification\nproblems. Recognizing that traditional fairness metrics do not adequately\ncapture the nuances of practical decision-making contexts, this chapter in-\ntroduces an approach that explicitly targets fairness within actionable de-\ncision regions. This ensures that fairness considerations align closely with\nreal-world business practices, maintaining model performance and versatil-\nity across varied decision thresholds within the specified decision area. The\neffectiveness of the proposed method is demonstrated empirically, highlight-\ning substantial benefits for applications where fairness is critical, such as\nvarious HR applications. This chapter is under review at European Journal\nof Operational Research. A preprint has been published online as [26]:\nS. De Vos, J. Van Belle, A. Algaba, et al., “Decision-centric fairness: Evalu-\nation and optimization for resource allocation problems,” arXiv preprint\narXiv:2504.20642, 2025\n10\n 1.4. Publications and outline\nChapter 7 introduces a novel methodological framework within prescrip-\ntive analytics, specifically addressing uplift modeling with continuous-valued\ntreatments. It extends traditional binary-treatment approaches by proposing\na predict-then-optimize method that first estimates conditional average dose\nresponses using causal ML techniques, followed by solving a dose-allocation\nproblem through integer linear programming.\nThis flexible optimization\napproach integrates fairness constraints and instance-dependent costs, en-\nabling efficient, utility-maximizing resource allocation. Experimental evalu-\nations demonstrate the framework’s versatility and effectiveness, showcasing\nits practical applicability and advantages across diverse fields such as health-\ncare, lending, and HR management. This chapter is currently under revision\nat European Journal of Operational Research. A preprint has been published\nonline as [27]:\nS. De Vos, C. Bockel-Rickermann, S. Lessmann, et al., “Uplift modeling\nwith continuous treatments: A predict-then-optimize approach,” arXiv\npreprint arXiv:2412.09232, 2024\nChapter 8 reflects on the co-creation process with Acerta, involving iter-\native feedback loops between industry and academia. It discusses how this\ncollaboration shaped the research in Part I and evaluates its practical value,\nwhile also exploring the potential for further implementation of the method-\nological advances from Part II in an applied setting.\nChapter 9 concludes this dissertation with a general discussion, offering a\nreflection on our contributions and their limitations. We also outline poten-\ntial future research directions to extend and build upon our findings.\n11\n  Part I\nHR analytics\napplications\n13\n  2\nLeveraging process mining to\noptimize internal employee\nmobility strategies\nThe significance of human resource (HR) analytics in facilitating data-driven\ndecision-making for managing internal employee mobility has been empha-\nsized by the recent increasing competition in attracting and retaining the\nbest employees referred to as the war for talent. Existing HR analytics meth-\nods typically provide support for operational and tactical decision-making.\nHowever, there is a need for long-term strategic decision support. Addition-\nally, as current methods for managing internal mobility are being challenged,\nthe development of new appropriate HR analytics methods is necessary.\nIn collaboration with KU Leuven, Acerta Consult implemented process\nmining techniques to address this issue. Specifically, process discovery tech-\nniques were applied to the event logs of HR data to generate employee jour-\nney maps (EJMs) that depict the different historic paths employees have\ntaken within an organization.\nThese EJMs demonstrated the difference between idealized career paths\nand the actual complexity of employee mobility. These discrepancies have\nthe potential to reshape the incorrect assumptions held by HR managers.\nThe data-driven insight gained through these EJMs can assist HR profes-\nsionals by providing decision support for a wide range of cases including the\nidentification of infrequent growth paths, analyzing hard-to-fill positions,\nand better understanding the causes of turnover.\nThe process perspective on internal mobility provides valuable insights\nfor HR managers and was able to shed light on the general complexity of\ncareers. As a result, this perspective can serve as a foundation for further\nanalyses, including predictive and prescriptive modeling, while taking into\naccount HR-specific constraints and challenges.\n15\n Chapter 2: Leveraging process mining to optimize internal employee\nmobility strategies\n2.1\nIntroduction\nIn recent years, many organizations have focused on utilizing the vast amount\nof data available to support decision-making in their daily activities in order\nto gain a competitive advantage. Like other business areas, HR departments\nare now attempting to use data to support their operational, tactical, and\nstrategic decisions.\nAs reported by the Financial Times, the global trend of increasing com-\npetition for talent in the job market is evidenced by the all-time low unem-\nployment rates in the eurozone (6.6% of the workforce) and the high number\nof job openings in the US (roughly two per unemployed worker) [28]. In re-\nsponse, companies are offering higher wages and benefits to attract and retain\nemployees, leading to increases in the cost of goods and services. Employers\nare implementing strategies to address high turnover rates, which include in-\ncentives such as bonuses and career development opportunities. To support\nthe development of these strategies, HR analytics is more relevan",
  "4": "n recent years, many organizations have focused on utilizing the vast amount\nof data available to support decision-making in their daily activities in order\nto gain a competitive advantage. Like other business areas, HR departments\nare now attempting to use data to support their operational, tactical, and\nstrategic decisions.\nAs reported by the Financial Times, the global trend of increasing com-\npetition for talent in the job market is evidenced by the all-time low unem-\nployment rates in the eurozone (6.6% of the workforce) and the high number\nof job openings in the US (roughly two per unemployed worker) [28]. In re-\nsponse, companies are offering higher wages and benefits to attract and retain\nemployees, leading to increases in the cost of goods and services. Employers\nare implementing strategies to address high turnover rates, which include in-\ncentives such as bonuses and career development opportunities. To support\nthe development of these strategies, HR analytics is more relevant than ever\nin terms of attracting and retaining good employees.\nOrganizations like Acerta are facing these challenges in HR analytics and\ninvestigate questions such as How can companies optimize their employees’\ncareer paths?\nor What paradigm should they start from?\nThis business\ncase takes a process perspective on internal mobility, specifically the imple-\nmentation of process mining techniques, to support HR managers at Acerta\nConsult. Together with their research partners at KU Leuven, they are in-\nternally exploring the capabilities of applying process mining techniques to\nHR data. With the insight from this case, they aim to further broaden the\nrange of HR analytics services offered to their clients.\nAcerta is a major HR service provider in Belgium with 25 offices across\nthe country. They serve a diverse range of customers, including starters, self-\nemployed, SMEs, and large companies, managing administration for one\nmillion employees in the Belgian market.\nAcerta has 1600 employees, a\n20% market share, and services 350,000 self-employed workers and 40,000\ncompanies in Belgium, resulting in a yearly turnover of 260 million euros.\nAcerta Consult, the company’s consulting branch with 450 employees, offers\nservices such as recruitment and selection, outplacement, legal, and training.\nSpecifically, we applied process mining techniques to longitudinal career\ndata to discover employee journey maps (EJMs) and as such examine internal\nemployee mobility. The technical details of this approach are explained in\nSection 2.3.1 The behavioral nature of careers requires the use of dynamic\nmethods for modeling, making process mining techniques a suitable method\nfor analyzing internal mobility. These techniques can be effectively combined\nwith existing human experience to provide insight based on data to support,\nrather than automate, decision-making.\n16\n 2.2. Situation faced\nThis article summarizes how to conceptualize and implement considera-\ntions of internal mobility as a process, through some example HR-related use\ncases from Acerta. Section 2.2 first provides an introduction to the field of\nHR analytics and explains why data-driven methods are needed for strategic\ndecision support. Next, it presents the specific problem statement concern-\ning the current internal mobility management methods and discusses why\na process perspective is beneficial. Section 2.3 describes the actions taken,\nthe methods used to implement the process perspective envisioned for man-\naging internal mobility, and the data structure. The results are discussed in\nSection 2.4 and the challenges faced and lessons learned from the case are\ndiscussed in Section 2.5.\n2.2\nSituation faced\nTo maintain its position as one of Belgium’s leading providers of HR services,\nAcerta Consult stays current with the latest developments in data-driven\nsupport for strategic HR decisions and maintains a focus on employee mo-\nbility.\nThe first part of this section explains why we need a solution for\nstrategic decision support and discusses the growth and evolution of HR\ntech, accompanied by the challenges of adoption. To highlight the useful-\nness of our process analytics approach as a solution, the second part outlines\nthe issues with current methods of internal mobility management in relation\nto the specific case study of Acerta.\n2.2.1\nNeed for strategic decision support\nThe use of technology within the HR field is not a new phenomenon, but the\nHR tech domain has evolved significantly in recent years and is becoming\nactive in an ever-expanding range of applications. In the 1990s, HR pro-\ncesses were successfully integrated within enterprise resource planning sys-\ntems (ERPs), whereas we now see HR-specific information systems (HRISs)\noften being used. This has led to the development of solutions for specific\noperational domains, such as recruitment, performance management, on-\nboarding, training and development, payroll administration, compensation\nand benefits, alternative workforce planning, reporting, and internal com-\nmunication. In short, operational HR applications and their corresponding\ntech industry are booming.\nHowever, the adoption of HR tech for strategic decision support remains\nlimited [29], [30]. Based on a comprehensive review of the relevant literature\n[29], [30] and the extensive experience of Acerta as an HR services provider,\nwe have identified three challenges that contribute to the lagging adoption\nof data-driven decision support in HR compared to other fields.\n17\n Chapter 2: Leveraging process mining to optimize internal employee\nmobility strategies\nFirst, there is a self-evident gap in the knowledge and abilities of the data\nscience teams who create analytical tools and the HR departments that are\nto use them. This disparity often leads to challenges in both the development\nand adoption of these tools. Second, employee data is sensitive and the data\nthat is available on employees is often limited, making it difficult to apply\nadvanced methods that require large amounts of data, for example, machine\nlearning. For example, to construct a system for automated CV screening, a\nvast amount of training data is necessary. However, on account of the sensi-\ntive data and limited availability, this requirement might not be fulfilled in\npractice. Third, HR professionals are typically charged with making tactical\nand strategic decisions over the medium to long term, with outcomes that\nare difficult to quantify and observe.\nThese decisions are typically made\nbased on unstructured and varied sources of information, as well as exper-\ntise and experience. In contrast, data-driven methods are typically used to\nsupport repetitive operational decisions with clear short-term outcomes, for\nwhich homogeneous and structured data sources are readily available. As a\nresult, HR professionals may be difficult to support with the arsenal of tools\nthat is typically used by data scientists in other domains.\nIn response to these challenges, we propose applying process mining tech-\nniques to longitudinal HR data. The EJMs offer three key advantages that\nalign with HR practices and resources. First, the process mining tooling in\nthe form of dashboards is easy to use and allows HR practitioners to work\nwith analytical instruments by bridging the gap between the data-related\nknowledge and skills of the HR departments and the data science teams.\nSecond, as we were not searching for the best process model but using EJMs\nfor data representation that will be interpreted by an HR professional, pro-\ncess mining techniques can be applied to smaller amounts of data. Third,\nby analyzing longitudinal data, this approach, complemented by the exper-\ntise of HR professionals, is well suited to supporting medium- to long-term\ndecision-making with difficult-to-quantify outcomes. In summary, the pro-\nposed approach offers a useful solution that addresses three typical challenges\nthat we have identified in the use of data-driven techniques for strategic HR\ndecision-making.\n2.2.2\nProblems in current internal mobility manage-\nment\nCurrently, internal mobility at Acerta is managed using a certain amount of\nHR professional expertise and intuition in combination with evidence-based\ntechniques from more traditional academic HR literature. However, with\nthis management approach, we uncovered two additional challenges specific\nto internal mobility. In addition to the general issues in HR tech previously\n18\n 2.3. Action taken\nmentioned, these two challenges highlight the need for data-driven insight\ninto internal mobility.\nThe first challenge we pinpointed was the recent changes in manage-\nrial formats. Like many organizations, Acerta is increasingly moving away\nfrom rigid organizational structures and delayering hierarchy. Therefore, the\ntraditional career ladders that steered the internal flows of employees have\nbecome less evident and less standardized. In addition, a shift toward focus-\ning on the skills of individuals and the management of their competencies\nis taking place, reinforcing the aforementioned trend toward less standard-\nization in internal mobility. As a result, traditional methods of managing\nemployee pathways are being increasingly challenged.\nThe second challenge that we identified is known as the fragmented pro-\ncess knowledge problem [31]. This problem arises when domain experts, such\nas HR professionals, have a limited understanding of the overall process of\nmanaging mobility within an organization. However, HR professionals typ-\nically have a very detailed understanding of their specific responsibilities.\nThe HR-employee ratio can vary depending on factors such as the size and\ncomplexity of an organization, but is roughly 1:50, i.e., 1 HR professional per\n50 employees, in Acerta’s case. Furthermore, Acerta’s HR data reveals that\nthe average tenure of an HR professional is around 4 years, based on their\nspecific role and seniority level. The fragmented process knowledge of inter-\nnal mobility among",
  "5": " of their competencies\nis taking place, reinforcing the aforementioned trend toward less standard-\nization in internal mobility. As a result, traditional methods of managing\nemployee pathways are being increasingly challenged.\nThe second challenge that we identified is known as the fragmented pro-\ncess knowledge problem [31]. This problem arises when domain experts, such\nas HR professionals, have a limited understanding of the overall process of\nmanaging mobility within an organization. However, HR professionals typ-\nically have a very detailed understanding of their specific responsibilities.\nThe HR-employee ratio can vary depending on factors such as the size and\ncomplexity of an organization, but is roughly 1:50, i.e., 1 HR professional per\n50 employees, in Acerta’s case. Furthermore, Acerta’s HR data reveals that\nthe average tenure of an HR professional is around 4 years, based on their\nspecific role and seniority level. The fragmented process knowledge of inter-\nnal mobility among various HR professionals hampers centralized control,\nhindering its effective management within the organization.\n2.3\nAction taken\nKU Leuven and Acerta took action by considering internal employee mobil-\nity as a process. In this section, we first explain how HR concepts can be\ntranslated into process concepts and how we used process discovery tech-\nniques. Then we discuss the data required from a technical and practical\nperspective.\n2.3.1\nProcess mining as a solution\nProcess mining incorporates the process perspective into data mining and\nmachine learning and helps organizations further comprehend their business\nprocesses [32]. It involves analyzing event logs to uncover the underlying\nprocess models, bridging the gap between traditional model-based process\nanalysis and data-centric analysis methods. By analyzing historical event\ndata, process owners like HR professionals can gain insight into business\nprocesses, such as, in this case study, internal mobility, where career paths\nare the individual traces.\n19\n Chapter 2: Leveraging process mining to optimize internal employee\nmobility strategies\nIn order to understand the behavior contained within an event log, au-\ntomatic process discovery techniques were utilized to construct a model.\nVarious approaches could have been employed for this purpose [32]. How-\never, implementing process mining in real-world scenarios is a challenging\ntask. In the case of internal mobility, we see many process variants and the\ndata is typically censored because process instances, i.e., careers, take a long\ntime. Additionally, traces for internal mobility processes usually contain few\nactivities compared to those found in other settings [33]. As a result, its im-\nplementation at Acerta was a highly iterative process and we emphasized the\nneed for close collaboration between process analysts and business experts.\nThis case study deploys process discovery techniques for data represen-\ntation and visualization rather than for building the best model. Moreover,\nin the context of employee journey mapping, it is not only important to\nidentify common paths taken by employees but also important to consider\nless frequent and obvious paths. Thus, directly-follows graphs (DFGs) were\nutilized as the preferred method of representation as they are the de facto\nstandard in the industry [34]. However, as there are some potential risks\nassociated with using DFGs and frequency-based simplification, it is impor-\ntant for practitioners to have a thorough understanding of how these process\nmodels are generated [34]. To address this, Acerta arranged information ses-\nsions to provide active guidance during deployment.\nWe generated EJMs on internal mobility by translating concepts from\nthe field of process mining into HR concepts as follows:\n⋅A case is conceptually equivalent to an employee. Over time, the em-\nployee can transition from one function to another, undertaking a jour-\nney.\n⋅An activity translates to occupying a function within an organization.\nIt is carried out by an employee and is characterized by a timestamp\nindicating when the activity starts and ends. In our particular HR con-\ntext, there is no overlap or parallelism in the activities of an employee\nas each employee can only have one function at a time.\n⋅A trace covers all the activities performed in a particular process in-\nstance by a specific case. Therefore, it is equivalent to an employee\njourney. Each trace is defined as the ordered set of subsequent func-\ntions that an employee occupies throughout their career within an\norganization.\n⋅Applying process discovery techniques to an event log consisting of lon-\ngitudinal HR data is conceptually the same as discovering EJMs. The\nevent log is defined as the collection of traces and, therefore, contains\ninformation on all employee journeys. An EJM is the aggregate of all\nthe paths employees have taken within an organization.\n20\n 2.3. Action taken\nTable 2.1: Synthetic example data D in the format of an event log.\nu\nts\nte\nv\nx1\nx2\nx3\nx4\nx5\ny\n1\n10/2014\n06/2016\nJob 1\nMSc\nPhysics\nF\n1975\n1\n0.4\n⎫⎪⎪⎪⎬⎪⎪⎪⎭\nD1\n1\n07/2016\n02/2019\nJob 3\nMSc\nPhysics\nF\n1975\n1\n0.8\n1\n03/2019\n07/2022\nJob 5\nMSc\nPhysics\nF\n1975\n1\n0.5\n2\n09/2009\n02/2016\nJob 1\nBSc\nFinance\nM\n1981\n0.8\n0.9\n}D2\n2\n03/2016\n07/2022\nJob 4\nBSc\nFinance\nM\n1981\n0.8\n0.3\n3\n06/2016\n03/2019\nJob 2\nPhD\nElectronics\nM\n1977\n1\n0.8\n}D3\n3\n04/2019\n07/2022\nJob 3\nPhD\nElectronics\nM\n1977\n1\n0.3\n4\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n2.3.2\nData requirements\nTypically, an event log in the setting of HR has the structure as visualized\nin Table 2.1. An event in a career path is a tuple (ui, ts\ni, te\ni, vi, xi) where ui\nis a unique identifier for an employee, holding the function vi from time ts\ni\nto time te\ni. On top of control flow information, employee ui has features xi\nwhich can include amongst others degree, branch of study, gender, date of\nbirth, and the full-time equivalent.\nAn employee profile Di ⊂D of the person ui consists of the combination\nof ∣Di∣tuples where ∣Di∣is the number of functions that this employee has\noccupied within this organization.\nConsequently, an employee profile Di\nconsists of the visited jobs and personal information xi of person ui.\nIn order to gain access to this career data in event log format, Acerta\nfaced several challenges when undertaking the HR analytics project. The\nfirst hurdle to overcome was the issue of data protection. Several stakehold-\ners were involved in the project and each had their own interests to defend.\nFor example, HR professionals were enthusiastic about moving forward with\nthe project, while the legal department was, understandably, more cautious\nabout sharing confidential data. In order to comply with the General Data\nProtection Regulation (GDPR), employment contracts had to be amended\nto allow for the use of employee data for analytical purposes. Second, there\nwas the issue of data collection. Despite the substantial amount of available\ndata, it was dispersed across several sources. For instance, data on previ-\nous employment was stored in different locations to the personal employee\ninformation and assessment interview results. As a result, this required an\nadditional step of pooling the data together. Third was the issue of data\ncleaning. Processes in other contexts tend to be relatively short compared\nto career path data, which was set to a time frame of 10 years. As this\nperiod is longer than other typical data analysis projects, impurities entered\nthe data as a result of, among other things, internal restructurings, changes\nin job titles, and the merging or division of positions.\n21\n Chapter 2: Leveraging process mining to optimize internal employee\nmobility strategies\n2.4\nResults achieved\nIn this section, we discuss the outcomes attained using our proposed ap-\nproach at Acerta.\nFirst, the achieved results are discussed in their most\ngeneral form. Our approach allows for the automatic discovery of EJMs,\nwhich describe and quantify internal mobility within an organization in a\ndata-driven way. This leads to new insight for HR professionals. Then, we\ndemonstrate how such EJMs can be used to solve specific HR cases.\n2.4.1\nGeneral results\nOur approach takes the descriptive character of EJMs as a starting point.\nThese maps are mainly useful for control flow analysis, i.e., the internal\nmovement of employees. Additionally, extra filtering techniques can be ap-\nplied to personal features like educational details, office location, and salary\nbracket.\nFigures 2.1 and 2.2 present two EJMs, represented by DFGs, that contain\nthe activity func 177. To generate these visuals, we use the tool Disco by\nFluxicon. They are anonymized examples of internal employee mobility at\nAcerta and demonstrate the usefulness of a good EJM. In addition to other\nfunctions, we include two special activities: before_data_capture indicates\nthat the individual was already an employee of the organization prior to the\nstart date of data capturing, and leave represents an employee leaving the\norganization and occurs instantaneously. The transition between a function\nand the sink suggests that the individual is still active in this function at\nthe end of the data-capturing period.\nFigure 2.1 shows what internal mobility, according to HR professionals,\nlooks like by design. In this figure, we filter on traces containing the activity\nfunc 177 and display only 15% of the most frequent activities. Hence, the\nEJM only shows a selection of the most frequent traces. When we showed\nthis to the HR professionals at Acerta, they agreed that this was a plausi-\nble map of the internal mobility in that company. To add nuance to this,\nalthough HR professionals often recognize that this representation is a sim-\nplification of reality, they find it challenging to pinpoint what exactly the\nmore complex reality looks like. This figure shows a typical traditional, lin-\near career path from junior to senior to expert positions. These are typical\nvertical movements with little v",
  "6": "loyee leaving the\norganization and occurs instantaneously. The transition between a function\nand the sink suggests that the individual is still active in this function at\nthe end of the data-capturing period.\nFigure 2.1 shows what internal mobility, according to HR professionals,\nlooks like by design. In this figure, we filter on traces containing the activity\nfunc 177 and display only 15% of the most frequent activities. Hence, the\nEJM only shows a selection of the most frequent traces. When we showed\nthis to the HR professionals at Acerta, they agreed that this was a plausi-\nble map of the internal mobility in that company. To add nuance to this,\nalthough HR professionals often recognize that this representation is a sim-\nplification of reality, they find it challenging to pinpoint what exactly the\nmore complex reality looks like. This figure shows a typical traditional, lin-\near career path from junior to senior to expert positions. These are typical\nvertical movements with little variation.\nHowever, in reality, Figure 2.2 shows that paths are more complex. We\nfilter on traces with the activity func 177 and displayed 50% of the most\nfrequent activities. The data reveals a wide range of career paths observed\nwithin the organization with less linear and more varied behavior.\nMost\nemployees follow unique paths, frequently transitioning between unexpected\n22\n 2.4. Results achieved\nand less obvious activities, rather than progressing through the traditional\ncareer ladders depicted in Figure 2.1.\nGrowth paths do not always align with expectations, as Figure 2.2 shows\na more diverse set of transitions than Figure 2.1. In this way, EJMs can help\nbreak HR managers’ incorrect assumptions. It should be noted that even\nFigure 2.2 is still a simplification of reality as it only displays the 50% most\nfrequent activities. The full EJMs are too detailed to be included as a figure\nin this article.\nWhen both a process model and an event log are provided, the discrep-\nancy between the two highlights the value of data-driven techniques. This\ndiscrepancy can be linked to conformance checking, where the goal is to find\nsimilarities and differences between the behavior modeled and the behavior\nobserved.\nAfter making this connection with conformance checking, process re-\ndesign became the logical next step. The differences between Figures 2.1\nand 2.2 allowed the HR professionals to face the facts and deal with the as-\nis situation. This, in turn, allows them to implement well-thought-out and\nwell-informed changes to their strategic HR management concerning internal\nmobility by, for example, focusing on active efforts to stimulate transitions\nbetween management functions and more technical functions. There are two\nmain reasons why redesigning a business process can be beneficial, as dis-\ncussed by [31]. The first reason is that the nature of organizations is often\norganic, and business processes naturally evolve over time, in general, be-\ncoming more complex, which leads to a decline in performance. This also\napplies to policies related to internal mobility that change over time. The\nsecond reason is that business environments are constantly changing, and\nHR strategies must be flexible in response.\n2.4.2\nConcrete HR Cases\nIn this subsection, we present a condensed overview of the insight obtained\nfrom the analysis presented in the previous sections in relation to the imple-\nmentation at Acerta and their expert input. We make this tangible with a\nselection of examples.\n1. Detection of infrequent and less obvious paths. In process analytics, the\noptimal workflow from the first to the final step is often the most effi-\ncient and effective. Process managers are interested in identifying and\nanalyzing this workflow in order to optimize the process. In contrast,\nHR professionals are regularly interested in the opposite, in identifying\ninfrequent and less obvious paths to explain questions such as unex-\npected success stories of internal mobility, transferability of skills, and\natypical growth paths.\n23\n Chapter 2: Leveraging process mining to optimize internal employee\nmobility strategies\n21\n7\n60\n2\n10\n41\n62\n6\n2\n20\n73\n116\n80\n57\n1\nfunc_255\n21\nbefore_data_capture\n73\nfunc_177\n227\nfunc_007\n10\nleave\n57\nfunc_216\n78\nfunc_105\n11\nfunc_096\n14\nFigure 2.1: A simplified EJM with career paths that contain the activity\nfunc 177.\n21\n3\n3\n5\n7\n1\n10\n41\n62\n1\n1\n1\n2\n1\n1\n1\n2\n1\n2\n1\n3\n2\n3\n1\n1\n2\n2\n1\n2\n60\n8\n2\n1\n1\n1\n1\n3\n2\n1\n2\n1\n3\n73\n20\n116\n5\n2\n1\n1\n1\n57\n4\n9\n1\n1\n1\nfunc_255\n21\nfunc_177\n227\nfunc_007\n10\nleave\n57\nfunc_216\n78\nfunc_163\n4\nfunc_229\n2\nfunc_228\n8\nfunc_096\n14\nfunc_001\n4\nbefore_data_capture\n73\nfunc_157\n4\nfunc_208\n3\nfunc_215\n2\nfunc_166\n3\nfunc_223\n3\nfunc_041\n2\nfunc_052\n4\nFigure 2.2: An EJM displaying what career paths containing the activity\nfunc 177 truly look like.\n24\n 2.5. Lessons learned\n2. Hard-to-fill positions. Analysis of the in- and outflows of selected po-\nsitions can identify favorable employee properties or profiles to attract\ntalent in the future. For instance, when it is difficult to hire someone\nfor a business intelligence team leader role, our longitudinal approach\ncan look at the past experiences and future plans of employees who\npreviously held that role. This will help us identify the qualities of\nemployees who are likely to stay in the position for a long time.\n3. Vertical vs. lateral movements. Both types of career moves can be\nbeneficial for employees.\nAcerta stimulates both the exploration of\ndifferent paths and the level of interaction between them. Examples\nof how our method can help here include detecting the prevalence\nof vertical movement and the typical positions between which lateral\nmovement occurs.\n4. Paths that lead to turnover.\nInternal mobility can impact turnover\nbehavior in various ways.\nIf employees have opportunities to move\nwithin a company and find more fulfilling or suitable roles, they may\nbe less likely to leave the company altogether. Furthermore, if inter-\nnal mobility opportunities are limited or employees are not satisfied\nwith the available roles, they may be more likely to seek opportunities\nelsewhere, resulting in higher turnover rates. Our approach helps Ac-\nerta understand and quantify the link between internal mobility and\nemployee turnover behavior.\n2.5\nLessons learned\nThis story of co-creation between KU Leuven and Acerta is unique in terms\nof the field of application of process mining. To close this chapter, we discuss\nour learnings and the challenges we faced when adopting process-driven HR\nanalytics and then conclude with a short closing word.\n2.5.1\nChallenges\nData\nThe data in this case study on process discovery techniques in HR\nanalytics is very sparse as it covers 1600 employees, 250 job titles, and only\naround 4000 job transitions over a 10-year period.\nThis means that the\nmajority of employees have only one or two jobs in their internal career,\nresulting in very short traces. Additionally, the trace length distribution\nis highly skewed, which can further complicate the analysis.\nHence, we\nemphasize that DFGs are used for data visualization and representation\npurposes rather than for finding the best process model. Another difficulty\nis the observability of the data. While 10 years of event data may seem like\na long time, careers are often much longer, with some starting before the\n25\n Chapter 2: Leveraging process mining to optimize internal employee\nmobility strategies\ndata capture began. This resulted in a large number of unobserved prefixes\nin the traces, which can potentially result in incorrect EJMs. Conversely,\nmany cases will still be ongoing when the data collection ends, causing these\ncases to be censored. A third challenge is the availability and quality of the\ndata. In order to accurately represent employee journeys, a long period of\ndata capture is required. However, data from this far back in time is often\nnot available, and even when it is, it may contain inconsistencies because of\norganizational changes and the splitting or merging of certain functions.\nIn addition to these technical challenges, there are also legal consider-\nations to take into account. The GDPR sets strict constraints on the use\nof personal data and the upcoming European AI Act will impose additional\nrestrictions on the use of AI-driven applications in HR, such as employment,\nmanagement of workers, and CV-sorting software [8]. These constraints must\nbe carefully considered in order to ensure compliance and avoid legal issues.\nAdoption\nProcess discovery techniques are effective for analyzing inter-\nnal mobility but implementing them can be challenging on account of stake-\nholder interests. In any organization, there are numerous stakeholders with\ndifferent interests and concerns. This can make it challenging to gain consen-\nsus and move forward with projects, particularly when it comes to sharing\nsensitive data. In this case study, the HR manager was eager to get started\nbut the legal department was understandably cautious about sharing confi-\ndential data. Only after several discussions and explanations of the tool and\nits capabilities did the stakeholders agree to move forward. This experience\nhighlights that simply having a method available is not enough; stakehold-\ners must understand it and see its value. A solution here is to start with a\nspecific problem and make the deliverable tangible.\n2.5.2\nInsights gained\nIt is important to note that there may be differences in the focus and interests\nof traditional process analysts and HR professionals when it comes to these\ntechniques. While process analysts may be more interested in the technical\naspects of the processes themselves, HR professionals may be more focused\non the implications for employees and the organization as a whole.\nThese techniques have been shown to partly support the existing knowl-\nedge and practices at Acerta. However, they have also provided new insight\nin the sense that they were able to correct several incorrect assumptions the\nmanagers had. Thi",
  "7": "nfi-\ndential data. Only after several discussions and explanations of the tool and\nits capabilities did the stakeholders agree to move forward. This experience\nhighlights that simply having a method available is not enough; stakehold-\ners must understand it and see its value. A solution here is to start with a\nspecific problem and make the deliverable tangible.\n2.5.2\nInsights gained\nIt is important to note that there may be differences in the focus and interests\nof traditional process analysts and HR professionals when it comes to these\ntechniques. While process analysts may be more interested in the technical\naspects of the processes themselves, HR professionals may be more focused\non the implications for employees and the organization as a whole.\nThese techniques have been shown to partly support the existing knowl-\nedge and practices at Acerta. However, they have also provided new insight\nin the sense that they were able to correct several incorrect assumptions the\nmanagers had. This highlights the complementary nature of process discov-\nery techniques and existing knowledge and expertise in HR. However, it is\nalso important to recognize that the use of analytics in HR is not without\nits challenges and constraints.\n26\n 2.5. Lessons learned\nAs such, it is crucial to carefully manage expectations when using pro-\ncess discovery techniques and other analytical approaches in HR. It is im-\nportant to make it clear beforehand what these techniques can and cannot\ndo and, thereby, avoid making overly optimistic or unrealistic claims about\ntheir capabilities. While these techniques can provide valuable insight based\non existing data, they should not be seen as a substitute for creative and\nthoughtful HR policies. Instead, they should be viewed as a tool that can\nhelp HR professionals better understand and optimize their existing pro-\ncesses.\n2.5.3\nConclusion\nKU Leuven and Acerta Consult collaborated on the development of an HR\nanalytics method by treating internal employee mobility as a process and\nproviding a more objective approach to strategic decision-making. The ar-\nticle discussed the method’s development, challenges, and lessons learned,\nhighlighting its positive reception at Acerta and potential for future devel-\nopment as a service. The method can provide valuable insight for managing\nemployee mobility in HR analytics where the adoption of data-driven tech-\nniques is currently lagging behind other fields.\n27\n  3\nPredicting employee turnover:\nScoping and benchmarking the\nstate-of-the-art\nEmployee turnover presents a significant challenge to organizations. High\nturnover rates impose substantial costs on organizations, e.g., direct costs\nresulting from rehiring efforts and training new employees, and indirect costs\nresulting from the loss of expertise and declining organizational productivity.\nHence, predicting employee turnover is an important task for human resource\ndepartments and organizations as a whole, as it can help to proactively ap-\nproach employees at risk of churning to improve retention and workforce\nstability. With ever more data at hand and increasing competition in the\nlabor market, analytical tools are inevitable to improve workforce manage-\nment and aid human resource managers in their decision-making. Yet, the\nexisting literature on predictive analytics for employee turnover is scattered\nand fails to present a coherent and holistic view. To find common ground in\nthe established literature, the paper provides a scoping and benchmarking of\nthe state-of-the-art. The scoping concludes that established research results\nare difficult to compare due to inconsistent methodologies and experimental\nsetups. To overcome these issues, an extensive benchmarking experiment is\nconducted including 14 classification methods and 9 datasets. The results\nprovide a unique focal point for research on employee turnover prediction\nand aim to benefit academic research and industry practitioners. The code\nand public datasets are available on Github to facilitate further extension of\nthe research.\n29\n Chapter 3: Predicting employee turnover\n3.1\nIntroduction\nEmployee turnover, describing the unplanned departure of employees [35],\npresents a substantial challenge to organizations. High or unforeseen turnover\nrates can impose substantial costs through recruitment, hiring, and training\nexpenses for new employees. These costs are typically estimated to surpass\nannual salary costs [36]. Losing employees can have several other detrimental\neffects, such as a decline in organizational productivity and competitiveness\ndue to a decrease in employee morale, reduced engagement, and a loss of\nskills and expertise [37]. Hence, identifying employees at risk of leaving [38]\nis a critical task for human resource (HR) departments as it enables orga-\nnizations to counter employee turnover through targeted retention efforts,\nand provides essential information for strategic workforce planning, talent\nmanagement, and succession planning, all of which are critical aspects of\nmodern HR management.\nExisting research on employee turnover prediction has utilized various\napproaches. Contrary to relying solely on expert-based approaches [39], also\ntraditional statistical methods [40], and machine learning techniques [41]\nhave been used to develop predictive models for employee turnover.\nPredictive analytics seamlessly integrates into traditional business intel-\nligence (BI), often forming a part of Human Resource Information Systems\n(HRIS) [42]. By incorporating analytics components, the aim is to enhance\ndecision-making support within systems. HRIS have a rich history, where\nefforts to develop balanced scorecards are examples of elementary predic-\ntive systems [42]. As decision-makers observe and manage a larger number\nof subjects and incorporate ever-increasing amounts of data, the shift from\nqualitative expert-based decision-making to the use of quantitative predic-\ntive models is an important advancement.\nThis advancement, reflecting\nthe ongoing evolution of technology within organizational contexts [43], has\nspurred research in the scientific literature on employee turnover prediction.\nTo date, this research has explored the predictiveness of different types of\ndata and has investigated both different classification methods – ranging\nfrom logistic regression to neural networks – and performance metrics to\nevaluate predictive accuracy [40].\nComparing the results of previous studies is challenging due to several\nreasons, including the use of different datasets and classification methods,\ninconsistent data preprocessing, different approaches to hyperparameter tun-\ning, and selective reporting of performance metrics. This makes it difficult to\ndraw meaningful conclusions by meta-analyzing previous research findings.\nExisting research on employee turnover prediction is lacking a well-defined\ncommon ground with respect to the applicability of analytical methods, the\nrobustness of performance, and the procedure to evaluate performance. The\nstate-of-the-art is yet to be determined.\n30\n 3.1. Introduction\nThis paper presents an overview of recent studies in employee turnover\nprediction, involving two key components: a scoping review and a bench-\nmarking experiment. This is a widely adopted approach with studies pub-\nlished in research areas such as predictive process monitoring [44], energy\nquantification [45], and customer-centric decision support [46]. First, the\nscoping review aims to identify current research topics and limitations in ex-\nisting studies. By conducting a structured scoping of the existing literature,\nwe identify the limitations and inconsistencies in established methodologies\nand experimental setups. Second, the benchmarking experiment enables the\nevaluation of established methodologies for predicting employee turnover and\nan identification of the state-of-the-art. We follow a comprehensive and stan-\ndardized methodology for evaluating and comparing the performance of 14\ndifferent classification methods on 9 datasets, including synthetic and real\ndata, and report on a wide range of metrics. Statistical tests are carried\nout to assess the significance of performance differences between classifiers.\nAdditionally, we test the impact of various class rebalancing methods and\nfeature selection.\nWith this paper, we aim to target professionals and academics at the\nintersection of data science and human resource management. The scoping\nreview and benchmarking experiment, the main components of this study,\nare not standalone. Rather, they are complementary to each other. Our\ngoal is to establish a distinct focal point in predictive analytics for employee\nturnover. Our contributions to the literature on employee turnover predic-\ntion are the following:\n⋅We consolidate the scattered literature on employee turnover predic-\ntion and provide a structured scoping of previous research.\n⋅Based on established methodologies for employee turnover prediction,\nwe provide a structured original experimental setup that enables fair\ncomparison of methods along established and novel datasets, as well\nas a range of established performance metrics. Our code and public\ndatasets are available on Github to facilitate further extension of our\nresearch.\n⋅We guide both industry practitioners and academic researchers in iden-\ntifying targeted and effective methodologies for decision-making and\nfuture research, respectively.\nThe remainder of this paper is organized as follows. In Section 3.2 we\ndefine and summarize the research field of employee turnover prediction and\ndiscuss immediately related works. Section 3.3 presents our scoping review,\nspanning a total of 56 studies. The setup of our benchmarking experiment\nis presented in Section 3.4, of which results are presented and discussed in\nSection 3.5. We conclude in Section 3.6 by formulating suggestions and key\ntakeaways for academics and practitioners.\n31\n Chapter 3: Predicting employee turnover\n3.2\nRelated literatur",
  "8": "ructured original experimental setup that enables fair\ncomparison of methods along established and novel datasets, as well\nas a range of established performance metrics. Our code and public\ndatasets are available on Github to facilitate further extension of our\nresearch.\n⋅We guide both industry practitioners and academic researchers in iden-\ntifying targeted and effective methodologies for decision-making and\nfuture research, respectively.\nThe remainder of this paper is organized as follows. In Section 3.2 we\ndefine and summarize the research field of employee turnover prediction and\ndiscuss immediately related works. Section 3.3 presents our scoping review,\nspanning a total of 56 studies. The setup of our benchmarking experiment\nis presented in Section 3.4, of which results are presented and discussed in\nSection 3.5. We conclude in Section 3.6 by formulating suggestions and key\ntakeaways for academics and practitioners.\n31\n Chapter 3: Predicting employee turnover\n3.2\nRelated literature\nEmployee turnover in the traditional HR literature.\nEmployee turnover, defined as employees’ voluntary termination of their\nemployment relationships [38], has captivated the interest of scholars and\npractitioners alike for well over a century. It is a dynamic and ever-evolving\nfield [47]. To gain comprehensive insights into this phenomenon, researchers\nhave provided overviews in reference works such as [38], [47].\nThe literature has recognized a range of demographic and job-related\nfactors that impact employee turnover [48]. Nevertheless, there is frequent\ndisagreement regarding the direction and extent of these factors’ effects on\nturnover. Consider, for example, the impact of age on turnover. Direct ef-\nfects vary across studies, with some suggesting that turnover intention first\nincreases and then decreases with age [49], while others observe a negative\nrelationship between age and turnover intention [38], [40], [50]. For teachers,\nthe relationship between age and turnover has been found to be U-shaped\n[51].\nOn top of that, turnover can also be influenced by indirect effects\nlike job satisfaction and organizational commitment. These effects are com-\nplex, with some studies suggesting a positive linear relationship between job\nsatisfaction and age [52], [53], while others find a U-shaped pattern [54].\nIn traditional turnover theories, broad principles are often employed to\naddress entire populations. For instance, these theories commonly suggest\nthat job satisfaction reduces turnover, while embeddedness ties employees to\ntheir organizations [55]. However, a compelling question arises: do turnover\ntheories manifest differently across distinct segments of employee popula-\ntions? Consider the possibility that rational decision-making models may be\nmore relevant to long-term employees in stable industries than to short-term\nemployees in highly dynamic work environments [47]. Additionally, could we\ntake this a step further and apply these theories customized to the individual\nemployee level?\nRecent investigations have steered away from a one-size-fits-all perspec-\ntive on turnover, favoring instead theories that specify the conditions under\nwhich particular factors become more or less influential in employees’ deci-\nsions to quit [47]. These conditions can range from a focus on more specific\nsubgroups based on factors like sector (e.g., construction [56], pharmacists\n[57], hospitality [58]) to an individual level focus (e.g., the Proximal With-\ndrawal States Theory (PWST) [59] and the unfolding model [60]).\nWhile there is a growing trend to tackle the problem of employee turnover\nat a higher level of granularity, the above studies are all still explanatory in\nnature [61].\nThat is, they are all focused on theory building and rather\nrely on data and statistical modeling to test (multiple) causal hypotheses\nwithin the theoretical framework that is being developed or scrutinized.\n32\n 3.2. Related literature\nWith increasingly more and richer HR data becoming available, the adop-\ntion of data-driven models for predicting employee turnover is becoming\nmore widespread.\nThis alternative approach is predictive in nature, and\napplies statistical or machine learning methods to data for the purpose of\npredicting new or future observations [61]. Consequently, this approach nat-\nurally focuses on the individual employee level. This data-driven modeling\napproach to employee turnover prediction constitutes a key component of\nHR analytics.\nHR Analytics.\nWithin the value stream of an employee journey [39], [62], HR Analytics\nserves as an umbrella term encompassing various potential applications.\nThese include recruitment and selection [11], [63], [64], internal mobility\nmanagement [24], learning and development, performance management and\nprediction [65], skills, talent, and succession management [66]. Numerous dif-\nferent definitions of HR Analytics have been proposed and a good overview\ncan be found in reference work such as [3].\nHR Analytics can be broadly categorized into three levels: descriptive,\npredictive, and prescriptive [67]. In this context, these three levels can ad-\ndress the following questions:\n1. Descriptive analytics reveal and describe current and historical data\npatterns and relationships.\nE.g.: What is the turnover rate within\ndifferent departments, and what are the distinctive attributes of em-\nployees who have departed from the organization?\n2. Predictive analytics extend this by inferring predictions about the fu-\nture which are derived from the current and historical data. E.g.: What\nis the probability that a specific employee will leave the organization?\n3. Prescriptive analytics offer guidance on how to achieve desired out-\ncomes. E.g.: What actions should be taken to minimize the risk of an\nemployee leaving?\nPredictive analytics for employee turnover.\nIn the HR setting, we stress the importance of steering clear of decision\nautomation. Instead, our emphasis lies in outlining potential scenarios and\nevaluating the risk of employee turnover in a predictive framework, refrain-\ning from prescribing specific actions. In contrast, descriptive analytics depict\npast occurrences. Meanwhile, prescriptive analytics suggests optimal actions\nor decisions based on the outcomes of predictive analytics, to optimize results\nor attain specific goals under a particular policy [63]. In this sense, although\nprescriptive analytics has proven its worth in other similar domains, such\nas customer churn [68], predictive modeling serves as a prerequisite for pre-\nscriptive modeling. Therefore, in this study, we focus on predictive analytics\nfor employee turnover.\n33\n Chapter 3: Predicting employee turnover\nAs discussed above, the use of data-driven methods for predicting em-\nployee turnover at the individual employee level is becoming more widespread\nas increasingly more and richer HR data becomes available. As opposed to\nthe theory-driven explanatory modeling paradigm, this modeling approach is\npredictive in nature as it starts from available organization-specific historical\ndata (vs. a theoretical model) to predict employee turnover for unseen ob-\nservations. For a detailed discussion on the differences between explanatory\nand predictive modeling, one may refer to [61]. Hence, it offers valuable in-\nsights for effective turnover management within individual organizations as\nit can be used by HR practitioners to identify (and characterize) employees\nwho are at risk of leaving voluntarily and unexpectedly, so that they can try\nto prevent turnover in a targeted manner. Moreover, data-driven methods\noften provide more reliable information about the underlying (data) mecha-\nnisms [69].\nOur study focuses on the application of data-driven methods to pre-\ndict employee turnover at the individual employee level, moving away from\nexpert-driven and explanatory modeling approaches.\n3.3\nScoping predictive analytics for employee\nturnover\nTo understand the current state of predictive analytics solutions for employee\nturnover, we are adopting a scoping review methodology. This enables the\nidentification and mapping of the key concepts, sources, and types of evi-\ndence available in a given research area [70]. We will present the methodology\nto our review in Section 3.3.1, present findings in Section 3.3.2, and distill\ngaps in the research field in Section 3.3.3.\n3.3.1\nReview methodology\nOur approach to conducting a scoping review is primarily inspired by the\nframework outlined by Arksey and O’Malley [70] and Munn, Peters, Stern,\net al. [71]. As presented in Table 3.1, our search strategy involves six dis-\ntinct stages: database selection, topic selection, record selection, abstract\nreview, full-paper review, and data extraction. Each stage is designed to\nensure comprehensive coverage of the literature on predictive analytics for\nemployee turnover while restricting the scope to relevant studies.\nStep 1: Database selection.\nWe conduct an exhaustive search of the Scopus and Web of Science (WoS)\ndatabases.\nThese databases are renowned for their extensive coverage of\n34\n 3.3. Scoping predictive analytics for employee turnover\nTable 3.1: Search strategy and number of records. n.m.: Not meaningful\nFiltering Step\nDecision\nNumber of records\nScopus\nWeb of Science\nDatabase selection\n- Scopus\nn.m.\nn.m.\n- Web of Science\nTopic selection\n- “Prediction” ∥“Predicting” ∥“Forecasting” ∥“Prognosis”\n459\n220\n- “Employee” ∥“Worker” ∥“Laborer” ∥“Jobholder”\n- “Turnover” ∥“Attrition” ∥“Churn” ∥“Departure”\n- “Analytics” ∥“Data” ∥“Machine learning”\nTime frame\n- 2008 - 2022\n377\n191\nOutlets\n- Journals\n326\n183\n- Conference Proceedings\nFields\n- Business\n- Computer Sci.\n221\n126\n- Economics\n- Engineering\n- Mathematics\nLanguage\n- English\n217\n125\nDuplicates\n- Excl. double counting\n276\nAbstract review\n- Excl. irrelevant records\n97\nFull paper review\n- Excl. irrelevant records\n56\ndiverse scholarly publications and records, rendering them highly esteemed\nand commonly adopted for academic investigation",
  "9": "ence (WoS)\ndatabases.\nThese databases are renowned for their extensive coverage of\n34\n 3.3. Scoping predictive analytics for employee turnover\nTable 3.1: Search strategy and number of records. n.m.: Not meaningful\nFiltering Step\nDecision\nNumber of records\nScopus\nWeb of Science\nDatabase selection\n- Scopus\nn.m.\nn.m.\n- Web of Science\nTopic selection\n- “Prediction” ∥“Predicting” ∥“Forecasting” ∥“Prognosis”\n459\n220\n- “Employee” ∥“Worker” ∥“Laborer” ∥“Jobholder”\n- “Turnover” ∥“Attrition” ∥“Churn” ∥“Departure”\n- “Analytics” ∥“Data” ∥“Machine learning”\nTime frame\n- 2008 - 2022\n377\n191\nOutlets\n- Journals\n326\n183\n- Conference Proceedings\nFields\n- Business\n- Computer Sci.\n221\n126\n- Economics\n- Engineering\n- Mathematics\nLanguage\n- English\n217\n125\nDuplicates\n- Excl. double counting\n276\nAbstract review\n- Excl. irrelevant records\n97\nFull paper review\n- Excl. irrelevant records\n56\ndiverse scholarly publications and records, rendering them highly esteemed\nand commonly adopted for academic investigations [72].\nStep 2: Topic selection.\nThe field of predictive analytics for employee turnover lacks a standardized\nterminology (cf. Section 3.2). To streamline our search, we established four\nessential components that a study must possess to be considered a candidate:\n(i) predicting (ii) employee (iii) turnover via a (iv) data-driven approach.\nWe have included synonyms and related search terms for each of these ele-\nments in our search strategy, as outlined in Table 3.1. Our screening process\ninvolves analyzing titles, abstracts, and keywords from both databases con-\nsidered.\nThe complete search queries are presented in Table A.1 in the\nAppendix.\nStep 3: Record selection.\nThe third step of our methodology includes substeps aimed at refining our\nretrieved studies by imposing additional criteria. As analytical research has\nmade significant advancements in recent years, we will focus on publica-\ntions from 2008 onwards. Further, to filter any non-analytical studies, we\nwill focus on manuscripts published in scientific journals and conference pro-\nceedings, ensuring the quality of records and comparability in terms of the\nintended audience and writing style. Additionally, we will filter manuscripts\nby their research field to filter any non-analytical studies. The research fields\nencompass business, computer science, economics, engineering, and mathe-\nmatics.\nWe only consider studies written in English.\nAdhering to these\ncriteria reduced the number of eligible papers to 276.\n35\n Chapter 3: Predicting employee turnover\nStep 4: Abstract review.\nBefore analyzing the full-text manuscript, we conduct a feasibility check\nby screening the abstracts of the remaining manuscripts. To this end, we\nemploy the following exclusion criteria, resulting in 97 papers:\n⋅Papers that are not HR-related. This group can be divided into two\nsubcategories: Firstly, papers addressing closely related topics, such\nas customer churn prediction, that may have passed through previ-\nous filters. Secondly, papers that employ homonymous terms, such as\nturnover in a financial context, rather than an HR-related context.\n⋅Papers that adopt a macroeconomic approach. Examples include pa-\npers related to labor economics, those that examine the labor market\nor adopt a non-micro perspective.\n⋅Papers that are not specifically focused on predicting employee turnover\nas a concrete problem, but rather concentrate on a particular aspect\nof it or provide only a high-level overview of a framework. Examples\nof such papers include those on ‘the conceptual exploration of...’, ‘the\nethical implications of...’ and ‘the managerial implications of...’.\nStep 5: Full-text review.\nFinally, we performed a full-text screening on the remaining papers and ex-\ncluded any manuscript that: (i) contains clear technical mistakes, such as\nan improper train/test split or reporting results on training data; (ii) lacks\na clear selection of predictive methods; (iii) does not approach employee\nturnover prediction as a binary classification problem.\nThis includes pa-\npers that predict job satisfaction or turnover intention rather than employee\nturnover itself. Following these screening criteria, we identified a final 56\npapers that meet our inclusion criteria.\nStep 6: Data extraction.\nThe relevant data from the selected papers is extracted and organized. The\nresults are presented in the following section.\n3.3.2\nThe scope of predictive analytics for employee\nturnover\nThe results of our scoping review are summarized in Table 3.2. For each\narticle, we report the dataset(s) used, the methodology applied (including\nclass balancing, feature selection, and hyperparameter tuning), and the type\nof evaluation metrics used. We distilled five key insights from our scoping\nreview:\n1. The number of different datasets used across studies is limited. Most\nmanuscripts only consider a single dataset for assessing classifier perfor-\nmance and only three studies have considered more than one dataset.\n36\n 3.3. Scoping predictive analytics for employee turnover\nTable 3.2: Overview of established literature. n.a.: Not applicable\nData\nMethodology\nEvaluation\nNumber\nPublic\nNum.\nNum.\nNum.\nFeature\nPara.\nRef.\ndatasets\nOrigin\n(Name)\nObs.\nVar.\nClassifier\nBalanc.\nSelect.\ntuning\nThresh.\nAUC\n[64]\n1\nHigh-tech\n3,825\n8\n1\n[73]\n1\nSoftware\n150\n13\n5\n✓\n[74]\n1\nManufacturing\n881\n44\n1\n✓\n✓\n[41]\n1\nn.a.\n1,575\n25\n3\n✓\n[75]\n1\nIT\n130\n19\n3\n✓\n[76]\n1\nCall center\n1,037\n10\n2\n✓\n✓\n[77]\n1\nCall center\n3,543\n6\n2\n✓\n✓\n[78]\n1\nAutomotive\nn.a.\n14\n7\n✓\n[79]\n1\nSynthetic\n✓(Kaggle1)\n15,000\n10\n5\n✓\n[80]\n1\nSynthetic\n✓(IBM)\n1,470\n30\n6\n✓\n✓\n[81]\n1\nCall center\n479\n11\n3\n✓\n✓\n✓\n[82]\n1\nKaggle1\n✓(Kaggle1)\n15,000\n10\n6\n✓\n✓\n✓\n[83]\n1\nTelecom, Indonesia\n16,649\n11\n3\n✓\n[84]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n3\n✓\n✓\n✓\n✓\n[85]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n1\n✓\n[86]\n1\nIT, India\n1,650\n22\n1\n✓\n✓\n[48]\n1\nHR services, Belgium\n13,484\n13\n1\n✓\n✓\n[87]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n5\n✓\n[88]\n1\nSynthetic\n✓(Kaggle1)\n15,000\n10\n5\n✓\n✓\n[89]\n1\nSynthetic\n✓(Kaggle1)\n15,000\n10\n5\n✓\n✓\n✓\n✓\n✓\n[90]\n1\nCommunications, China\n2,000\n30\n5\n✓\n✓\n✓\n✓\n[91]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n5\n✓\n✓\n✓\n✓\n[92]\n1\nSynthetic\n✓(Kaggle1)\n15,000\n10\n4\n✓\n✓\n[93]\n2\nSynthetic\n✓(IBM)\n1,470\n31\n10\n✓\n✓\n✓\nBanking, USA\n9,089\n19\n[94]\n1\nHigh-tech, China\n66,911\nn.a.\n8\n✓\n✓\n✓\n✓\n[95]\n1\nSocial network, China\n47,257\n9\n6\n✓\n✓\n✓\n✓\n[96]\n1\nGlassdoor\n5,550\n45\n5\n✓\n✓\n✓\n[97]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n7\n✓\n[98]\n1\nSynthetic\n✓(Kaggle1)\n15,000\n10\n3\n✓\n[99]\n1\nSocial network, China\n287,229\n22\n16\n✓\n✓\n✓\n[100]\n1\nMetallurgy\n1,866\n22\n5\n✓\n✓\n✓\n[101]\n1\nHealthcare\n12,000\n24\n6\n✓\n✓\n✓\n[102]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n9\n✓\n✓\n[103]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n1\n✓\n✓\n✓\n[104]\n3\nSynthetic\n✓(IBM)\n1,470\n32\n9\n✓\n✓\n✓\nSynthetic\n✓(Kaggle1)\n15,000\n10\nn.a.\n450\n16\n[105]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n6\n✓\n✓\n[106]\n1\nPackaging\n6,552\nn.a.\n4\n✓\n✓\n✓\n✓\n[107]\n1\nSynthetic\n✓(Kaggle1)\n15,000\n10\n6\n✓\n✓\n[108]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n3\n✓\n✓\n✓\n✓\n✓\n[109]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n6\n✓\n✓\n[110]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n5\n✓\n✓\n[111]\n1\nConsumer goods\n365\n7\n3\n✓\n✓\n[112]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n6\n✓\n✓\n✓\n✓\n✓\n[113]\n2\nSynthetic\n✓(IBM)\n1,470\n32\n22\n✓\n✓\n✓\nSynthetic\n✓(Kaggle1)\n15,000\n9\n[114]\n1\nn.a.\n330\n8\n6\n✓\n✓\n✓\n[115]\n1\nSocial network, China\n287,229\n24\n7\n✓\n✓\n✓\n[116]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n3\n✓\n✓\n✓\n✓\n✓\n[117]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n9\n✓\n✓\n✓\n[118]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n6\n✓\n✓\n✓\n✓\n[119]\n1\nSynthetic\n✓(Kaggle1)\n15,000\n10\n5\n✓\n✓\n[120]\n1\nRetail\n1,186\n9\n6\n✓\n✓\n[121]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n3\n✓\n✓\n✓\n✓\n[122]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n4\n✓\n✓\n✓\n✓\n✓\n[123]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n4\n✓\n✓\n✓\n[124]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n8\n✓\n✓\n✓\n✓\n✓\n[125]\n1\nSynthetic\n✓(IBM)\n1,470\n32\n6\n✓\n✓\n✓\nThis\n9\ncf. Table 3.4\n14\n✓\n✓\n✓\n✓\n✓\npaper\n37\n Chapter 3: Predicting employee turnover\nIn addition, many studies have focused solely on synthetic data, with\nthe public IBM and Kaggle1 datasets being used 24 and 10 times, re-\nspectively. This has led to both a lack of research diversity and has\nconstrained the potential for generating broadly applicable insights\nthat can be used in real-world applications.\n2. The majority of studies use a limited set of classification methods, with\nmost studies utilizing five or fewer methods. Six studies consider only\none classification method. This lack of comparison limits the potential\nto draw robust conclusions.\n3. The modeling pipelines in the studies are inconsistent. We see a vari-\nety of different approaches to, for example, preprocessing (in terms of\ntraining data balancing), feature selection, or hyperparameter tuning.\nThese steps are essential to the performance of methods [126]. Incon-\nsistency limits comparability between studies. For example, more than\nhalf of the selected studies do not perform hyperparameter tuning, and\nonly about one-fourth of studies test the effect of rebalancing training\ndata [127].\n4. Studies do not report comparable performance metrics. Metrics can\nbe categorized into different types based on the notion of classifier per-\nformance that they embody. Most studies report on threshold metrics\nwhere predictions are rounded to 1 if their propensities exceed a certain\nthreshold and to 0 otherwise. Consequently, the stated performance\nstrongly depends on this threshold, which is often not specified (and\npresumably equal to the default threshold of 0.5). Additionally, the\nmetrics are not consistent across studies (for an overview of metrics,\nsee Section 3.4). Threshold-independent area under the curve (AUC)\nmetrics, such as area under the receiver operating characteristic curve\n(AUC-ROC) and Gini, which are popular in related fields like customer\nchurn prediction [128], fraud detection [129] and credit risk modeling\n[130], are used in only about half of the studies.\nThe difference in\nperformance metrics does not allow for a coherent comparison across\nstudies.\n5. A notable observation with regard to the literature is the lack of atten-\ntion that is given to statistical hypothesis testing. Specifically, a mere\n5% of studies examine the statistical significance of performance differ-\nences among various classification methods. M",
  "10": "ensities exceed a certain\nthreshold and to 0 otherwise. Consequently, the stated performance\nstrongly depends on this threshold, which is often not specified (and\npresumably equal to the default threshold of 0.5). Additionally, the\nmetrics are not consistent across studies (for an overview of metrics,\nsee Section 3.4). Threshold-independent area under the curve (AUC)\nmetrics, such as area under the receiver operating characteristic curve\n(AUC-ROC) and Gini, which are popular in related fields like customer\nchurn prediction [128], fraud detection [129] and credit risk modeling\n[130], are used in only about half of the studies.\nThe difference in\nperformance metrics does not allow for a coherent comparison across\nstudies.\n5. A notable observation with regard to the literature is the lack of atten-\ntion that is given to statistical hypothesis testing. Specifically, a mere\n5% of studies examine the statistical significance of performance differ-\nences among various classification methods. Moreover, when evaluat-\ning classifier performance across multiple datasets, relying on multiple\npairwise t-tests or ANOVA tests with their parametric assumptions\nmay not be the most suitable approach. These tests assume normal-\nity and homogeneity of variances, which cannot be guaranteed when\nassessing the performance of analytical models over diverse datasets\n[131]. Additionally, conducting paired t-tests for all possible classifier\n38\n 3.4. Experimental design\npairs is suboptimal because when so many tests are made, a certain\nproportion of the null hypotheses is rejected due to random chance. In\nsuch situations, non-parametric tests, such as the Friedman test along\nwith appropriate post-hoc tests, are a more fitting choice (cf. Section\n3.4.5). They make limited assumptions and are safer for comparing\nclassifier accuracy since they do not rely on normality or homogeneity\nassumptions.\nIn addition to these five key findings, we will provide further insight\ninto the different methods and datasets used in the selected studies in the\ndescription of our experimental setup (cf. Section 3.4).\n3.3.3\nResearch gaps in predictive analytics for employee\nturnover\nBased on the above analysis, we define three gaps in the literature on pre-\ndictive analytics for employee turnover.\nData: Most studies in the literature are using one dataset. Moreover, mostly\nsynthetic datasets are used with Kaggle1 and IBM being most popular. Con-\nsequently, the literature on predictive analytics for employee turnover lacks\na common set of benchmark datasets to be used for method performance\nevaluation. As case-specific conditions might apply, testing a method on a\nsingle dataset is likely not sufficient. This hypothesis also remains to be\ntested.\nModeling pipeline: To compare the performance of methods on a dataset\nor use case, preprocessing, processing, and postprocessing techniques must\nbe consistent. The research on predictive analytics for employee turnover\nsuffers from different approaches chosen in established manuscripts. Notably,\nwe find that most papers are not explicit about their experimental setup.\nPerformance metrics: We find no common ground in metrics used for\nevaluation in predictive analytics for employee turnover. While most stud-\nies use at least one threshold-based metric, the use of more comprehensive\nthreshold-independent metrics, such as Area Under the Receiver Operating\nCharacteristic curve (AUC-ROC) and Area Under the Precision-Recall curve\n(AUC-PR) metrics remains limited. As different metrics might suggest dif-\nferent rankings of methods, agreement is needed on a set of metrics that\nshould be reported (next to possible use-case-specific reporting).\n3.4\nExperimental design\nIn this section, we present an experimental design for evaluating multiple\nmethods across multiple data sets for turnover prediction. Our experiments\ncomprehensively address five identified limitations in the scoping review. We\n39\n Chapter 3: Predicting employee turnover\nensure generalizability by testing our methods on 9 diverse datasets and as-\nsess the effectiveness of 14 binary classification methods. We employ proper\nhyperparameter tuning, along with testing the effects of class rebalancing\nand feature selection methods.\nA comprehensive evaluation is conducted\nusing a wide set of performance metrics. Additionally, we conduct proper\nstatistical tests to analyze and compare the performance of different classi-\nfication methods. By overcoming these five limitations, our study provides\na more thorough evaluation of the methods and can inform future research\nin this field.\n3.4.1\nClassification methods\nFormally, turnover prediction can be described as a binary classification task.\nEach dataset D consists of N observed predictor-response pairs {(xi, yi)}N\ni=1\nwhere xi ∈Rm is an m-dimensional vector describing employee character-\nistics and yi ∈{0, 1} is a binary feature that distinguishes between stay\n(y = 0) and leave (y = 1). D is used to train a binary classification model\ns(.) ∶Rm →[0, 1]. This binary classification model predicts a propensity\nscore si ∈[0, 1] for each instance i based on the features xi. Depending on\nthe classification threshold t∗\ni , si is converted to a predicted class ̂yi ∈{0, 1}.\nIn our experiments, the threshold is set to 0.5.\nThis study compares 14\ndifferent classifiers, both individual and ensemble methods, as summarized\nin Table 3.3. For a complete overview of which classifiers are used in which\nstudy, we refer to Table A.2 in the Appendix.\nIn Section 3.5.2, this study conducts two ablation studies: one assessing\nthe effects of class rebalancing methods and another examining the impact\nof feature selection techniques. Since some classifiers may handle class im-\nbalance better than others, we will explore how combining specific classifiers\nwith class rebalancing methods or feature selection techniques might im-\nprove performance. These investigations aim to shed light on the sensitivity\nof various classifiers to class imbalance and feature selection, thereby guiding\npractitioners and researchers toward optimal model selection strategies for\nturnover prediction tasks.\n3.4.2\nDatasets\nTable 3.4 describes the 9 datasets used in the experiments. The Real1, Real2,\nand Real3 datasets were obtained from three large Belgian organizations and\nare not publicly available. The remaining six datasets are publicly accessible.\nThe IBM and Kaggle1 datasets are used in 24 and 10 of the studies included\nin the scoping review presented in Table 3.2, respectively.\nAll the datasets exhibit some degree of class imbalance (see Perc. Turnover),\nwith the positive class representing the minority. The extent of class imbal-\n40\n 3.4. Experimental design\nTable 3.3: Overview of established classifiers\nClassifier\nAbbr.\nNum.\nStudies\nIndiv. classifiers\nArtificial Neural Networks\nann\n12\nDecision Tree\ndt\n41\nK-Nearest Neighbors\nknn\n22\nLinear Discriminant Analysis\nlda\n4\nLogistic Regression\nlr\n33\nNaïve Bayes (Bayesian/Gaussian)\nbnb /gnb\n25\nQuadratic Discriminant Analysis\nqda\n1\nSupport Vector Machine\nsvm\n30\nEnsembles\nAdaBoost\nab\n7\nGradient Boosting\ngb\n9\nLightGBM\nlgbm\n3\nRandom Forest\nrf\n36\nExtreme Gradient Boosting\nxgb\n12\nTable 3.4: Dataset overview. S: synthetic, R: real, pub: publicly available,\npriv: private\nName\nAvail-\nOrigin\nType\nNum.\nNum.\nPerc.\nNum.\nability\nObs.\nVar.\nTurnover\nStudies\nReal1\npriv\nHR services\nR\n16,394\n25\n7.16%\n0\nReal2\npriv\nIT services\nR\n19,992\n46\n8.38%\n0\nReal3\npriv\nHigh-tech\nR\n19,413\n64\n6.11%\n0\nDS\npub\nAnalytics Vidhya\nR\n19,158\n12\n24.93%\n0\nIBM\npub\nIBM\nS\n1,470\n32\n16.12%\n24\nKaggle1\npub\nKaggle\nS\n14,249\n10\n23.81%\n10\nKaggle2\npub\nKaggle\nS\n6,284\n8\n23.63%\n0\nKaggle3\npub\nKaggle\nS\n19,104\n14\n8.46%\n0\nKaggle4\npub\nKaggle\nS\n3,310\n22\n32.90%\n0\n41\n Chapter 3: Predicting employee turnover\nance varies across the datasets, ranging from 6.11% to 32.90%. Each dataset\ncomprises a range of features covering demographics, employment details,\norganizational information, education, and talent. While certain features,\nsuch as age, gender, and organizational tenure, are common across datasets,\nthe specific features included can vary. For further information on dataset\ncontents, please refer to the GitHub repository where the public datasets are\navailable.\nThe time-flattening of the data, i.e., the handling of the temporal aspect\nof turnover, slightly varies across the datasets, depending on the source. In\nthe case of the Real1, Real2, and Real3 datasets, the continuous time aspect\nis divided into snapshots. In line with Rombaut and Guerry [48] and in line\nwith industry practice, we use yearly snapshots. For Kaggle2 and Kaggle3,\nwe adopted the snapshots as originally available in the publicly accessible\ndatasets, which are provided on a yearly and monthly basis, respectively.\nEach periodic snapshot captures information about every employee in the\norganization and indicates whether or not the employee left the organization.\nIn datasets with a time component, we have also included features that track\nchanges in feature values compared to the previous period. The remaining\nfour datasets do not include a timestamp, although some time-dependent\nfeatures are incorporated (e.g., years in the current role and seniority).\n3.4.3\nData preprocessing and partitioning\nFor each dataset, we employ a 5×2-fold cross-validation approach [131], re-\nsulting in ten distinct evaluations of out-of-sample classification performance\non which we report the performance average. In each iteration, the data is\ndivided into non-overlapping training (50%) and test (50%) sets. Addition-\nally, to fine-tune hyperparameters and select the best models, we conduct\nan internal five-fold cross-validation on each training set within the 5×2-fold\ncross-validation loop, i.e., we adopt a so-called nested cross-validation setup.\nAfter splitting the data, the numeric features in the training set are\nnormalized to have a mean of zero and a variance of one. The normalization\nprocess is then extended to the corresponding features in the vali",
  "11": "ture values compared to the previous period. The remaining\nfour datasets do not include a timestamp, although some time-dependent\nfeatures are incorporated (e.g., years in the current role and seniority).\n3.4.3\nData preprocessing and partitioning\nFor each dataset, we employ a 5×2-fold cross-validation approach [131], re-\nsulting in ten distinct evaluations of out-of-sample classification performance\non which we report the performance average. In each iteration, the data is\ndivided into non-overlapping training (50%) and test (50%) sets. Addition-\nally, to fine-tune hyperparameters and select the best models, we conduct\nan internal five-fold cross-validation on each training set within the 5×2-fold\ncross-validation loop, i.e., we adopt a so-called nested cross-validation setup.\nAfter splitting the data, the numeric features in the training set are\nnormalized to have a mean of zero and a variance of one. The normalization\nprocess is then extended to the corresponding features in the validation and\ntest sets, using the mean and variance estimates obtained from the training\nset. Categorical features are encoded using the weights-of-evidence approach\n[132]. We avoid any form of data leakage by performing these operations\nbased on the training data.\nWe use a full grid-search approach to optimize hyperparameters, system-\natically evaluating all possible combinations within predefined ranges on a\nseparate validation set.\nThis exhaustive exploration ensures no potential\nconfigurations are overlooked, providing a comprehensive view of the per-\nformance landscape. This approach is especially beneficial for configuring\nhyperparameters in benchmarking experiments, where a thorough assess-\n42\n 3.4. Experimental design\nment of model performance is crucial. The details of the hyperparameter\nsearch space can be found in Table E.2 in the Appendix.\n3.4.4\nPerformance metrics\nThe evaluation of binary classification algorithms typically involves labeling\none class as positive (i.e.\nturnover) and the other class as negative (i.e.\nstay), which enables the construction of a confusion matrix. In this context,\npositive classes typically refer to the minority class, while negative classes\ndescribe the majority class. The classification threshold is set to 0.5. Based\non the threshold-dependent confusion matrix, we calculate Accuracy, Sen-\nsitivity (also known as Recall), Specificity, Precision, and the F1-measure\n[126].\nThe Receiver Operating Characteristic (ROC) represents the trade-off\nbetween Sensitivity and the 1 - Specificity at various classification thresh-\nolds for the positive class. The area under the ROC (AUC-ROC) metric\nsummarizes the model’s ability to distinguish between positive and negative\ninstances across different classification thresholds. However, its effectiveness\nin comparing methods has limitations due to its dependence on the classi-\nfier’s score distribution, which can vary [133]. Given our study’s objective of\nmethod comparison, we have chosen also to include the H-measure. This is\nthe standardized measure of the expected minimum loss where Hand [133]\nproposes employing a symmetric beta(x; 2, 2) distribution to characterize the\ncosts of misclassification. A precise definition can be found in [133].\nAn alternative to AUC-ROC is AUC-PR, which calculates the area under\nthe precision-recall (PR) curve and hence quantifies the trade-off between\nprecision and recall at different classification thresholds.\nAUC-PR is the\npreferred metric in imbalanced learning settings like employee turnover pre-\ndiction since it takes into account the prevalence of turnover by being more\nsensitive to the absolute number of false alarms [134]. For both threshold-\nindependent metrics, AUC-ROC and AUC-PR, a higher value is better.\nThe Brier score assesses the calibration of a model’s predicted probabil-\nities by measuring the mean squared difference between the predicted prob-\nability (si) and the observed outcome (yi). A lower Brier score indicates\nbetter calibration.\n3.4.5\nStatistical tests\nTo evaluate the statistical significance of the benchmarking experiment re-\nsults, we employ a methodology described by Demšar [131] that involves\nutilizing a Friedman test in combination with a post-hoc test with Hochberg\np-adjustments for multiple comparisons [135].\n43\n Chapter 3: Predicting employee turnover\nIn this study, we compare the performance of 14 (k) different methods\nacross 9 (N) datasets. Let r\nj\ni represent the rank of algorithm j on dataset i\naccording to a specific metric. The Friedman test examines the average ranks\nof the algorithms, calculated as Rj = 1\nN ∑i r\nj\ni , across the datasets. Under the\nnull hypothesis, which assumes that all algorithms are equivalent and hence\ntheir average ranks Rj should be equal, the Friedman statistic follows a χ2\nF\ndistribution with k −1 degrees of freedom. If the null is rejected, indicat-\ning the presence of significant differences among the algorithms, we conduct\na post-hoc test with Hochberg p-adjustments, which carries out a pairwise\ncomparison where all classifiers are compared with the best-performing clas-\nsifier for the chosen metric.\n3.5\nEmpirical results\nThis section presents the empirical results. We divide our analysis of our\nresults into two distinct parts. Section 3.5.1 presents our core results, where\nwe report the results on all dataset-method combinations. In Section 3.5.2,\nwe investigate the effect of three class balancing methods and the effect of\nfeature selection.\n3.5.1\nResults\nThe results of the benchmarking study are presented in Table 3.5, which\nshows the average ranking (AR) of the classifiers across datasets per met-\nric. The rightmost column presents the average performance and standard\ndeviation across metrics for each classifier. The bottom row shows the Fried-\nman χ2 statistic to check whether at least two classifiers have significantly\ndifferent performances.\nThe p-values of the post-hoc test with Hochberg\np-adjustments that compare each classifier to the best-performing one are\nadded between brackets. Underscored values indicate a significance at the\n5% level that the null hypothesis of a classifier performing equally well as\nthe best classifier was not rejected.\nWe draw several insights from Table 3.5. Across all performance metrics,\nensemble classifiers largely exhibit the best performance among the evaluated\nmethods, except the artificial neural network (ann). Among the ensemble\nmethods, there are variations in rankings, but these differences are often\nnot statistically significant, as indicated by the underlined p-values. This\nsuggests that the various ensemble methods generally perform on par with\neach other. However, to provide a more comprehensive view, it is important\nto note a distinction within the group of ensemble methods. Specifically, ab,\none of the oldest ensemble methods [136], appears to differ from the rest of\nthe ensemble techniques, including the more recent xgb [137]. Two widely\n44\n 3.5. Empirical results\nrecognized industry standards, lr and dt, generally exhibit lower performance\nin terms of threshold-independent AUC-PR and AUC-ROC when compared\nto ensemble methods and ann. However, it is essential to highlight that their\nperformance is significantly influenced by the dataset’s characteristics. For\nexample, when considering the IBM dataset (see Table A.8 in the Appendix),\nlr emerges as the top performer, while dt demonstrates weak performance.\nThe complete results for each dataset, presented as relative rankings of\nclassifiers per metric, are provided in Tables A.4-A.12 in the Appendix. Next\nto the differences in characteristics between the datasets as summarized in\nTable 3.4, another important difference is the unknown generation process of\nthe synthetic datasets, resulting in considerable uncertainty regarding how\naccurately they reflect real-world scenarios. Consequently, caution is war-\nranted when attempting to generalize insights derived from these synthetic\ndatasets to real-world applications. Illustrating with the IBM dataset, lr out-\nperforms other classifiers in terms of AUC-PR, AUC-ROC, Accuracy, and\nBrier Score and ranks second for F1-Score and H-Measure after lda (another\nlinear method). These outcomes hint at a linear process in the synthetic\ndataset generation. Therefore, caution is crucial in generalizing these results\nto diverse scenarios despite the dataset’s common usage (Table 3.2). It is\nvital to recognize that these concerns extend beyond predictive performance,\nencompassing potential disparities in feature predictive power and correla-\ntions that may not mirror reality. These findings underscore the importance\nof selecting the right classifier for a specific task, taking into account not\nonly the classifier’s inherent properties but also a match with the unique\nproperties and complexity of the dataset under consideration.\n3.5.2\nEffect of class balancing and feature selection\nClass balancing\nClass imbalance is present in data when there is an unequal distribution\nof classes. This may present a significant challenge to learning algorithms\n[138]. An algorithm simply trained to minimize classification error might\noverfit the majority class, resulting in potentially high accuracy, yet low\nsensitivity. This is a significant threat as the minority class is typically of\ngreatest interest, as it is in predicting employee turnover.\nWe show in Table 3.4 that all datasets used in the literature exhibit lev-\nels of class imbalance. In this subsection, we will investigate the severeness\nof class imbalance in employee turnover datasets, and answer whether such\nimbalance has a detrimental effect on performance. We will deploy three dif-\nferent resampling methods, i.e., random over-sampling (ROS), the Synthetic\nMinority Oversampling Technique (SMOTE) [139], and the Adaptive Syn-\nthetic (ADASYN) sampling method [140], and evaluate whether they can\n45\n Chapter 3: Predicting employee turnover\n(a) AUC-ROC\n(b) AUC-PR\nFigure 3.1: The effect o",
  "12": "an unequal distribution\nof classes. This may present a significant challenge to learning algorithms\n[138]. An algorithm simply trained to minimize classification error might\noverfit the majority class, resulting in potentially high accuracy, yet low\nsensitivity. This is a significant threat as the minority class is typically of\ngreatest interest, as it is in predicting employee turnover.\nWe show in Table 3.4 that all datasets used in the literature exhibit lev-\nels of class imbalance. In this subsection, we will investigate the severeness\nof class imbalance in employee turnover datasets, and answer whether such\nimbalance has a detrimental effect on performance. We will deploy three dif-\nferent resampling methods, i.e., random over-sampling (ROS), the Synthetic\nMinority Oversampling Technique (SMOTE) [139], and the Adaptive Syn-\nthetic (ADASYN) sampling method [140], and evaluate whether they can\n45\n Chapter 3: Predicting employee turnover\n(a) AUC-ROC\n(b) AUC-PR\nFigure 3.1: The effect of various class balancing techniques per classifier\nin terms of area-under-the-curve metrics, relative to the case without class\nbalancing.\nimprove the predictive performance of each of the models in our main set of\nexperiments. ROS is the simplest resampling method. It tackles imbalance\nby creating new samples in the minority class through random duplication\nof existing minority samples. With SMOTE, the minority class is oversam-\npled by generating synthetic examples along line segments connecting the\nk nearest neighbors from the minority class. This technique aims to aug-\nment the representation of the minority class. ADASYN takes yet a different\napproach by adaptively generating minority data samples based on their dis-\ntributions. It produces more synthetic data for minority class samples that\nare more challenging to learn while generating fewer synthetic samples for\nthose that are easier to learn. ADASYN not only reduces the learning bias\narising from the original imbalanced data distribution but also aims to adjust\nthe decision boundary by focusing on the more difficult-to-learn samples.\nIn our analysis, we employed each resampling method on all nine em-\nployee turnover datasets and evaluated the resulting difference in AUC-PR\nand AUC-ROC for all learning algorithms. Each dataset was resampled to\na 1:1 distribution of majority to minority class samples. Full results can be\n46\n 3.5. Empirical results\nfound in Figure 3.1.\nThe effectiveness of resampling is both method-dependent and varying\nacross datasets. The performance of ensemble methods is seemingly unaf-\nfected by resampling, in terms of AUC-ROC, suggesting robustness against\nclass imbalance. Conversely, decision trees and k-nearest neighbors show an\nunexpected detrimental effect of resampling the training data. Only support\nvector machines, discriminant analyses, and naive Bayes approaches appear\nto benefit, although only slightly, from resampling. In terms of AUC-PR, we\nsee a generally negative effect of resampling, compared to using the training\ndata as-is. However, the impact is evidently limited with the most affected\nmethods losing less than 10% in AUC-PR when compared to training on\nunsampled data.\nWe note that we have only evaluated the impact of resampling to a 1:1\nratio of minority to majority class observations. This ratio could be tuned as\na hyperparameter in method training, potentially leading to different results.\nWe omit this step for the sake of brevity.\nWe do not claim holistic results, yet our experiments suggest the use\nof resampling when training learning algorithms might be limited.\nThis\nmight stem from several factors, such as the robustness of learning algorithms\nagainst class imbalance, or the level of imbalance in the data. This insight\nconfirms the findings of other studies on real-world classification problems\nwith class imbalance [141], [142].\nFeature selection\nIn this subsection, we assess the impact of feature selection on the results\nof the study. From a technical perspective, the objective of feature selec-\ntion is to identify and retain the most informative features from a given\ndataset, to mitigate the curse of dimensionality and the risk of overfitting,\nand to reduce computational complexity, while not compromising on predic-\ntive performance. From a business perspective, especially in the context of\nturnover prediction, fewer, highly relevant features are preferred to obtain\ndata-driven managerial insights, as it may be important and an objective of\nthe analysis to understand why employees leave a company [143].\nWe investigate the effect of feature selection by assessing performance\nwhile incrementally increasing the number of selected features, denoted as\nk. We select the k highest-ranking features based on the ANOVA F-statistic\nwhich tests the relationship between that feature and the target variable.\nHowever, it is important to note two key limitations to this approach. First,\nthis method is most effective with linear feature-target associations and may\nstruggle with complex, nonlinear relationships. Second, if features are not\nindependent, the F-scores can be influenced by confounding factors, poten-\ntially distorting feature importance.\n47\n Chapter 3: Predicting employee turnover\nFigure 3.2: The effect of feature selection per dataset in terms of AUC-PR.\nThe performance of single classifiers is indicated with a dashed line and of\nensemble methods with a full line.\nFigure 3.2 displays one dataset per panel, where performance in terms\nof AUC-PR is expressed in function of the number of features selected. Sin-\ngle classifiers are depicted using dashed lines, while ensemble methods are\nrepresented by solid lines. The plots summarize the results of a 5 × 2-fold\ncross-validation procedure where the AUC-PR is averaged over ten runs.\nFor the dataset Real2, performance in terms of the number of included\nfeatures tends to increase for most of the classifiers and reaches a plateau af-\nter incorporating more than 10 features. The height of this plateau depends\non the classification method. This largely aligns with prior research [128].\nFor the Real3, DS, IBM, and Kaggle2 datasets, we also predominantly ob-\nserve upward-trending curves, but with no clear signs of reaching plateaus.\nFor the Real3 and IBM datasets, for instance, for most of the classifiers, per-\n48\n 3.6. Conclusion\nformance continues to improve even after the last feature is added. Finally,\nfor the Real1, Kaggle1, Kaggle3, and Kaggle4 datasets, performance dete-\nriorates for some single classifiers when giving the models access to more\nfeatures. However, as for the other datasets, we do observe performance\nimprovements with an increase in the number of features for all ensemble\nmodels.\nThis last observation, in combination with the fact that the best-performing\nclassifiers are the ensemble methods (vs. the single classifiers) for all datasets,\nexcept the IBM dataset, allows us to conclude that feature selection is gen-\nerally not advisable from a predictive performance point of view. Even when\nconsidering the business need for explainability, the recommended approach\nis to prioritize predictive accuracy initially and subsequently address explain-\nability (when the goal is not to test a theory) since higher predictive accuracy\nsuggests a more profound understanding of the underlying structure in the\ndata [69]. To make (the predictions of) an ensemble model with a lot of fea-\ntures explainable, various strategies can be employed. Among these, options\ninclude building a single decision tree to explain ensemble predictions, see\ne.g., [144], [145], or using permutation-based feature importance estimates,\nsee e.g., [69], [146].\n3.6\nConclusion\nEmployee turnover significantly affects organizations, resulting in the loss of\nvaluable expertise and decreased productivity. Traditional turnover theories\nrely on a theoretical framework and often apply general principles to entire\npopulations.\nWith the abundance of available data, analytical tools can\npredict and explain employee turnover at a higher granularity, emphasizing\neither organization-specific, department-specific, or, in the most extreme\ncase, on an individual employee level.\nUtilizing data-driven insights improves decision-making processes and ul-\ntimately reduces costs through effective employee retention strategies. Nev-\nertheless, the current body of literature on predictive analytics for employee\nturnover is extensive and lacks cohesion. This paper aims to focus on the\nintersection of data science and human resource management, targeting both\nprofessionals and academics. It achieves this by presenting a comprehensive\nstudy consisting of several components focused on traditional HR literature,\nconducting an extensive survey on data-driven methodologies in turnover\nprediction, and performing a rigorous benchmarking experiment.\nThese\ncomponents synergize to establish a clear focal point in the landscape of\npredictive analytics for employee turnover, aiming to illuminate the current\nstate-of-the-art and provide a valuable reference for researchers and practi-\ntioners alike.\n49\n Chapter 3: Predicting employee turnover\nIn the structured scoping review, we reveal the following insights that\nunderscore the necessity for a standardized benchmarking experiment. No-\ntably, existing predictive analytics studies on employee turnover often (i) do\nnot disclose the datasets used or lack dataset variability and often focus on\nthe same publicly available datasets, (ii) are ambiguous regarding data pre-\nprocessing, processing, and postprocessing techniques, (iii) lack a common\nset of performance metrics.\nWe address these insights and limitations through a benchmarking ex-\nperiment. We evaluate 14 classification methods across 9 datasets, revealing\nensemble classifiers’ superior performance over single classifiers (except for\nann) across most metrics. However, specific classifiers’ effectiveness varies\ndepending on dataset size and complexity. ",
  "13": "r, aiming to illuminate the current\nstate-of-the-art and provide a valuable reference for researchers and practi-\ntioners alike.\n49\n Chapter 3: Predicting employee turnover\nIn the structured scoping review, we reveal the following insights that\nunderscore the necessity for a standardized benchmarking experiment. No-\ntably, existing predictive analytics studies on employee turnover often (i) do\nnot disclose the datasets used or lack dataset variability and often focus on\nthe same publicly available datasets, (ii) are ambiguous regarding data pre-\nprocessing, processing, and postprocessing techniques, (iii) lack a common\nset of performance metrics.\nWe address these insights and limitations through a benchmarking ex-\nperiment. We evaluate 14 classification methods across 9 datasets, revealing\nensemble classifiers’ superior performance over single classifiers (except for\nann) across most metrics. However, specific classifiers’ effectiveness varies\ndepending on dataset size and complexity. Additionally, our ablation study\ninvestigates class rebalancing and feature selection effects.\nResults show\ntheir impact varies with methodology and dataset, with ensemble methods\ndemonstrating no benefit from these techniques despite their superior pre-\ndictive performance.\nIn light of our findings, we offer key insights for industry practition-\ners striving for precise employee turnover prediction.\nIt is recommended\nto adopt a use-case-driven strategy, tailoring machine learning models to\norganizational dynamics. Beginning with off-the-shelf methods and simple\npipelines lays a foundational understanding, paving the way for refinement\nbased on specific needs, including class rebalancing. While our results cau-\ntion against prioritizing resampling, a mid-to-long-term evaluation of its\nnecessity remains vital, contingent upon factors such as the chosen method\nand data imbalance. Exercise caution when generalizing insights, especially\nwith the IBM dataset, where linear models excel but require validation for\nbroader applicability. The effectiveness of feature selection methods hinges\non various factors, including the number of features, classification method,\nand data structure. Some datasets show optimal predictive power with a\nsubset of features, while others benefit from a broader set. Additionally, the\nchosen feature selection method significantly impacts model performance,\nwith methods like ANOVA F-statistic tests assessing linear relationships\nbut potentially struggling with non-linear ones.\nThis study has some limitations. Firstly, it is crucial to acknowledge the\nabsence of data from external organizations, particularly in assessing per-\nceived job alternatives. This limitation may impact the comprehensiveness\nof the analysis, as external factors influencing turnover decisions remain\nunaccounted for. Additionally, there’s a trade-off between interpretability\nand performance in classification methods. While black box models enhance\npredictive power, they pose challenges in explaining outcomes. Although\nensemble methods perform best, the experiments lack a metric for explain-\nability, crucial for model deployment and user adoption to address skepticism\n50\n 3.6. Conclusion\ntowards these models [147], [148].\nFuture work could explore several interesting avenues to extend the cur-\nrent research. Firstly, integrating unstructured data like textual performance\nevaluations and CVs could enhance classification algorithm performance.\nSecondly, acknowledging the nuanced nature of turnover, researchers could\ninvestigate scenarios where employee departure is not necessarily undesir-\nable. A nuanced cost-sensitive approach to turnover involves understanding\nthe varying financial implications of departures and the diverse costs of re-\ntention programs per employee. This tailored perspective informs strategic\nworkforce management, facilitating informed decisions that account for the\nunique financial dynamics associated with both employee departures and\nretention initiatives, as the relationship between turnover and firm perfor-\nmance can be complex [149]. Finally, future studies could further explore\nthe why behind employee turnover, unraveling the underlying motivations\nfor departures so that HR professionals can respond to them.\nThis cor-\nresponds with a transition from predictive to prescriptive methods, where\ninsights are translated into actionable strategies [68], enabling the design of\neffective, personalized retention campaigns.\n51\n Chapter 3: Predicting employee turnover\nTable 3.5: Average ranking of classifiers across datasets per performance\nmeasure. The best-performing methods are emphasized in bold and second\nbest in italic. P-values are added between brackets.\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nAverage\nann\n5.6\n(0.297)\n5.7\n(0.207)\n7.1\n(0.075)\n5.9\n(0.090)\n5.8\n(0.380)\n6.0\n(0.419)\n8.1\n(0.029)\n5.9\n(0.524)\n8.1\n(0.087)\n6.5\n±1.0\nbnb\n13.4\n(0.000)\n12.7\n(0.000)\n11.6\n(0.000)\n10.6\n(0.000)\n10.6\n(0.004)\n10.3\n(0.014)\n12.4\n(0.000)\n9.6\n(0.022)\n10.8\n(0.003)\n11.3\n±1.3\ndt\n7.6\n(0.048)\n7.8\n(0.026)\n5.3\n(0.362)\n5.2\n(0.169)\n5.6\n(0.414)\n5.7\n(0.498)\n7.0\n(0.086)\n5.7\n(0.582)\n7.4\n(0.158)\n6.4\n±1.1\ngnb\n11.3\n(0.000)\n12.0\n(0.000)\n13.9\n(0.000)\n12.3\n(0.000)\n9.8\n(0.006)\n8.7\n(0.061)\n13.2\n(0.000)\n5.1\n(0.723)\n13.3\n(0.000)\n11.1\n±2.8\nknn\n10.4\n(0.001)\n12.4\n(0.000)\n9.2\n(0.007)\n8.3\n(0.004)\n10.6\n(0.004)\n10.8\n(0.014)\n7.7\n(0.044)\n10.6\n(0.010)\n5.8\n(0.474)\n9.5\n±2.0\nlda\n9.7\n(0.003)\n9.2\n(0.003)\n9.1\n(0.008)\n8.1\n(0.005)\n8.8\n(0.025)\n8.4\n(0.065)\n9.6\n(0.003)\n9.1\n(0.039)\n9.4\n(0.017)\n9.0\n±0.5\nlr\n7.9\n(0.038)\n7.6\n(0.031)\n8.1\n(0.031)\n6.4\n(0.048)\n9.9\n(0.006)\n10.0\n(0.015)\n8.0\n(0.030)\n10.9\n(0.007)\n7.1\n(0.193)\n8.4\n±1.5\nqda\n11.1\n(0.000)\n10.4\n(0.000)\n11.3\n(0.000)\n9.4\n(0.001)\n8.4\n(0.031)\n7.6\n(0.124)\n11.4\n(0.000)\n7.0\n(0.270)\n11.2\n(0.001)\n9.8\n±1.7\nsvm\n7.9\n(0.038)\n8.9\n(0.005)\n7.1\n(0.075)\n14.0\n(0.000)\n9.9\n(0.006)\n9.9\n(0.015)\n3.9\n(0.787)\n11.4\n(0.007)\n5.2\n(0.608)\n8.7\n±3.1\nab\n6.8\n(0.107)\n5.9\n(0.176)\n7.4\n(0.061)\n12.7\n(0.000)\n8.6\n(0.030)\n8.6\n(0.064)\n7.0\n(0.086)\n9.0\n(0.041)\n7.2\n(0.188)\n8.1\n±2.0\ngb\n3.4\n(0.933)\n3.2\n(0.900)\n3.3\n(0.943)\n3.1\n(0.660)\n3.7\n(0.980)\n3.8\n(1.000)\n4.7\n(0.547)\n4.1\n(1.000)\n6.0\n(0.429)\n3.9\n±0.9\nlgbm\n3.4\n(0.933)\n3.3\n(0.869)\n3.1\n(1.000)\n2.1\n(1.000)\n3.4\n(1.000)\n4.2\n(0.897)\n3.8\n(0.787)\n4.7\n(0.848)\n4.2\n(0.891)\n3.6\n±0.8\nrf\n3.2\n(0.975)\n3.0\n(0.920)\n4.2\n(0.665)\n2.9\n(0.734)\n6.0\n(0.322)\n6.1\n(0.395)\n3.1\n(1.000)\n6.8\n(0.312)\n3.9\n(1.000)\n4.4\n±1.5\nxgb\n3.1\n(1.000)\n2.8\n(1.000)\n3.6\n(0.884)\n3.8\n(0.464)\n3.4\n(1.000)\n4.3\n(0.888)\n4.4\n(0.621)\n4.6\n(0.863)\n4.9\n(0.700)\n3.9\n±0.7\nFried. χ2\n77.6\n(0.000)\n86.5\n(0.000)\n75.1\n(0.000)\n101.8\n(0.000)\n50.9\n(0.000)\n40.9\n(0.000)\n73.8\n(0.000)\n46.3\n(0.000)\n53.9\n(0.000)\n52\n 4\nData-driven internal mobility:\nSimilarity regularization gets the\njob done\nThis paper presents a novel approach to support career management by\nrecommending job-employee matches within an organization through data-\ndriven insights. We build a recommender system to propose matches. The\npresented approach extends upon a conventional collaborative filtering rec-\nommender system, which suggests matches based on the historic performance\nsimilarities of employees. To address the prevalent challenge of the cold start\nissue in internal placement, we incorporate personal employee data with a\nsimilarity-based regularization term. This regularization term finds latent\nrepresentations that are closer to each other when employees share similar\npersonal features. This approach is evaluated using three real-life datasets\nand demonstrates a highly competitive performance compared to state-of-\nthe-art benchmark methods. Overall, we make three contributions to the\nfield of HR analytics: (i) we present a comprehensive survey of job-employee\nrecommender systems in the context of internal mobility, (ii) we implement\na similarity regularization method, and (iii) we release a first-of-its-kind HR\ndataset on internal mobility. All code and data used in this study are pub-\nlicly available on GitHub.\n53\n Chapter 4: Data-driven internal mobility\n4.1\nIntroduction\nWith a vast amount of available data, companies in almost every sector\nare devoted to exploiting this data to gain a competitive advantage [150].\nRecently, many organizations have been relying on data-driven insights to\nsupport decision-making in their daily activities [151], [152]. The human\nresources (HR) field sees great potential in using big data, analytics, and\nmachine learning (ML) [153].\nAs a result, HR departments within orga-\nnizations are looking for ways to use analytical techniques to complement\ncurrent evidence-based techniques with objective and data-driven insights\n[30]. Correspondingly, HR analytics is becoming increasingly important as a\ndiscipline and research field. There has been a growing amount of literature\non the topic [154], but, despite the notable rising enthusiasm for HR ana-\nlytics, the ability to apply insights from this literature in business practices\nremains limited [155].\nAs organizations are moving away from traditional organizational struc-\ntures and delayering hierarchies, career ladders that guide the mobility of\nemployees within an organization become less clear [156], [157]. This chal-\nlenges traditional internal market organization theories, which mostly focus\non economic and institutional factors [158], and leads to the adoption of\ndata-driven approaches with the help of electronic human resource manage-\nment (e-HRM) regimes [159], [160]. Organizations are increasingly looking\ninto data-driven systems to support decisions on how to mobilize employees\nwithin the organization, either for career changes or for advancement in their\ncurrent careers [161]. This is crucial, as internal job transitions are bene-\nficial for employees’ employability [162] and can impact, amongst others,\nmotivation and retention behavior of employees [163]. Using a data-driven,\nalgorithmic approach to recommend career steps has several benefits, in-\ncluding facilitating the critical search for solutions ",
  "14": " from traditional organizational struc-\ntures and delayering hierarchies, career ladders that guide the mobility of\nemployees within an organization become less clear [156], [157]. This chal-\nlenges traditional internal market organization theories, which mostly focus\non economic and institutional factors [158], and leads to the adoption of\ndata-driven approaches with the help of electronic human resource manage-\nment (e-HRM) regimes [159], [160]. Organizations are increasingly looking\ninto data-driven systems to support decisions on how to mobilize employees\nwithin the organization, either for career changes or for advancement in their\ncurrent careers [161]. This is crucial, as internal job transitions are bene-\nficial for employees’ employability [162] and can impact, amongst others,\nmotivation and retention behavior of employees [163]. Using a data-driven,\nalgorithmic approach to recommend career steps has several benefits, in-\ncluding facilitating the critical search for solutions to hard-to-fill positions,\nguiding users to relevant opportunities, discovering passive internal candi-\ndates by better managing the internal talent pool, and streamlining manual\ncandidate searches [164]–[166].\nTo support the management of internal mobility with data-driven in-\nsights, we view an employee’s career within an organization from a longitu-\ndinal perspective as a sequence of activities, as is done in the field of process\nanalytics [167]. This means that HR data can be transformed into an event\nlog format, where events represent the execution of activities, resulting in\nstart, completion, and/or cancellation recordings. This dynamic process per-\nspective provides a comprehensive end-to-end view, which is suitable for the\ncomplex nature of internal mobility in modern organizations. B.1 shows an\nemployee journey map (EJM) in the form of a directly-follows graph derived\nfrom an event log.\n54\n 4.1. Introduction\nThe literature on data-driven decision support on internal mobility is, as\nfar as we are aware, limited to managerial concerns. No concrete algorithmic\napproach has been proposed yet, except for using simple distance-based or\nsimilarity-based insights for descriptive purposes [165]. In response to the\nlack of analytical approaches for supporting the management of internal mo-\nbility in the current literature, we propose an internal recommender system\nfor organization-specific employee mobility.\nOur proposed method focuses on predicting the performance of employ-\nees within an organization concerning specific job positions, where matches\nare rated with a score y ∈[0, 1] (see Section 4.4.1). Recommender systems\n(RSs) can assist in decision-making by suggesting job-employee matches and\nnarrowing down the large search space to a personalized subspace [168],\nwhich then aids in proposing fitting jobs to employees and vice versa. When\nconsidering internal mobility as a process, visualized by an EJM, the recom-\nmender system suggests the next activity, i.e., the next job, in the trace of an\nemployee. We start with a collaborative filtering (CF) approach that recom-\nmends job-employee matches based on the similarity with other employees.\nHowever, limited availability of data on new hires and employees with short\ntenure may affect the performance of RSs as a result of the cold start prob-\nlem [169]. In the case of new employees or employees with short tenure, the\nsystem does not have information about their past job performances in order\nto make new recommendations. Consequently, the cold start problem poses\na challenge for using recommender systems in internal mobility management.\nTherefore, we extend the loss function of this baseline method with a\nsimilarity regularization term to incorporate additional information captured\nby personal employee data such as education and field of study. The pro-\nposed similarity regularization technique effectively addresses the cold start\nproblem by incorporating additional personal information beyond past ex-\nperience, in order to decrease the distance between latent representations\nof employees when they share similar personal features. The performance\nof the proposed methods is assessed using three real-life datasets containing\nthousands of matches from the past ten years.\nOur contribution consists of three main parts. First, we examine the\nuse of recommender systems in the setting of data-driven internal mobility.\nSecond, we extend existing collaborative filtering methods by implementing\na similarity regularization term to include personal information on employees\nto address the prevalent cold start problem. Third, we also publish a first-\nof-its-kind HR-career dataset on internal mobility in the format of an event\nlog. This dataset considers internal mobility in event log format, spans a\nperiod of 10 years, and is made available on GitHub. As such, we ensure\nthe reproducibility of the reported results in this paper and facilitate further\nresearch on this topic.\n55\n Chapter 4: Data-driven internal mobility\nThe rest of this paper is organized as follows. In the next section, we\nprovide an overview of related work. In Section 4.3, we first motivate our\napproach with a running example, which we retake throughout this paper.\nNext, we explain the longitudinal approach with an event log as starting\npoint.\nThen, we introduce the collaborative filtering (CF) methodology\nbased on Matrix Factorization (MF) and the proposed similarity regular-\nization (SR) extension. In Section 4.4, we discuss the used datasets and\ndescribe the setup of our experiments. Next, Section 4.5 presents the re-\nsults and discusses the advantages and drawbacks of our approach. Finally,\nSection 4.6 concludes the paper.\n4.2\nRelated work\nHuman Resource Analytics (HRA) as a term is relatively new, first appearing\nin HR literature in 2003-2004 [154]. We define HR analytics in this paper as\nthe application of descriptive, predictive, and prescriptive analytics for data-\ndriven human resources management. [154] and [170] provide a taxonomy\nof research topics in this field.\nIn this research, we investigate data-driven decision support for internal\nemployee mobility. More specifically, we focus on predictive job-employee\nmatching in a post-hire setting, considering careers from a process perspec-\ntive. In our HR application, we emphasize the importance of avoiding deci-\nsion automation. Instead, we focus on describing potential scenarios without\nprescribing specific actions. HR professionals, in collaboration with employ-\nees, select the scenario, with a human always involved in assessing the op-\ntions. Thus, our emphasis lies on decision support rather than automation,\naligning with predictive analytics. In contrast, descriptive analytics depict\npast occurrences. However, our application focuses on unseen job-employee\nmatches rather than past events. Meanwhile, prescriptive analytics recom-\nmends optimal actions or decisions based on predictive analytics outcomes,\naiming to optimize results or achieve certain goals according to a specific\npolicy [63]. While a recommender system can directly influence decision-\nmaking, its primary function for our application is to predict the outcomes\nof certain job-employee matches rather than prescribe specific actions.\nTable 4.1 summarizes key related work in the field of Predictive HR An-\nalytics based on three criteria: (i) whether the dataset is publicly available\nor not, (ii) whether the problem statement is approached from a match-\ning perspective or not (as discussed in Subsection 4.2.2), and (iii) whether\nthe problem statement occurs in a post-hire setting (as discussed in Subsec-\ntion 4.2.3). The bottom row highlights the positioning of our work, which\naddresses the research gap of providing data-driven decision support on in-\nternal mobility, which is characterized as a matching problem in a post-hire\nsetting.\n56\n 4.2. Related work\n4.2.1\nPredictive HR analytics\nUnlike descriptive analytics, predictive analytics are forward-looking and\nenvision the future based on observed historical data. Within the field of\ndata-driven HRM, predictive analytics are frequently used for predicting\nemployee performance and employee turnover, for retention strategy design,\nand job-employee matching. We highlight some of the most important work\nfor each of these applications.\nPerformance prediction can be done for both current and future employ-\nees, i.e., candidates applying for a position. This helps HR professionals\nefficiently prioritize their resources by narrowing their focus to a shortlist\nof people [171]–[173]. Research on performance prediction in the context of\nrecruitment and selection has been conducted by [174]–[180].\nEmployee turnover is a pressing issue for many organizations. Depend-\ning on the position and the difficulty of finding a replacement, the costs of\nturnover can range from 1.5 to 5 times an employee’s annual salary [181].\nWhile prior HRM literature has heavily relied on qualitative survey data,\nthis approach may not always yield organization-specific insights. However,\nthe increasing availability of longitudinal data through Human Resources\nInformation Systems (HRISs) has enabled research on predictive analytics\nfor employee turnover. Several studies have proposed data mining and ML\ntechniques for turnover prediction [172], [182]–[185], and [178], [186] have\nexplored turnover prediction in specific domains. [63] combine the predic-\ntion of performance and turnover simultaneously. As an extension, research\non turnover can be taken to the next step by also developing data-driven\nretention policies [171].\nAnother field of predictive HR analytics is management-oriented and\nexamines how businesses can and should adopt predictive HRA in their daily\noperations. [29], [30], [187] describe the difficulties and barriers of adopting\nHR analytics on the most conceptual level. To tackle those barriers, [188]\npropose a framework to organize HR analytics. [189] look at HR analytics\nfrom a m",
  "15": "this approach may not always yield organization-specific insights. However,\nthe increasing availability of longitudinal data through Human Resources\nInformation Systems (HRISs) has enabled research on predictive analytics\nfor employee turnover. Several studies have proposed data mining and ML\ntechniques for turnover prediction [172], [182]–[185], and [178], [186] have\nexplored turnover prediction in specific domains. [63] combine the predic-\ntion of performance and turnover simultaneously. As an extension, research\non turnover can be taken to the next step by also developing data-driven\nretention policies [171].\nAnother field of predictive HR analytics is management-oriented and\nexamines how businesses can and should adopt predictive HRA in their daily\noperations. [29], [30], [187] describe the difficulties and barriers of adopting\nHR analytics on the most conceptual level. To tackle those barriers, [188]\npropose a framework to organize HR analytics. [189] look at HR analytics\nfrom a meta-perspective by assessing and comparing different frameworks.\n[190]–[192] focus on practical implementation tools to successfully adopt HR\nanalytics.\n4.2.2\nMatching\nMatching is the process of linking potential candidates to jobs based on\nsome criterion. There have been numerous approaches to matching, and a\ncomprehensive overview can be found in reference works such as [63], [200]–\n[202]. However, this literature predominantly focuses on pre-hire matching\nin the context of recruitment and selection or employment services, while\nthe focus of this research is on post-hire matching. Job recommender sys-\n57\n Chapter 4: Data-driven internal mobility\nTable 4.1: Predictive HR analytics overview.\nIn this overview, we\nlist various applications of predictive HR analytics related to matching or\na post-hire scenario, which are the two categories our research jointly en-\ncompasses. We provide details on the main problem it addresses (matching\nor other) and the dataset, including its availability (public or private). We\nalso specify whether the paper discusses a post-hire setting and the types of\ntechniques used.\nWe handle the following abbreviations. AR: association rules, CNN: convo-\nlutional neural networks, CS: cosine similarity, DT: decision trees, (X)GB:\n(extreme) gradient boosting, GD: graph databases, KNN: k-nearest neigh-\nbors, LDA: linear discriminant analysis, LR: logistic regression, (M)NB:\n(multinomial) naïve bayes, MF(SR): matrix factorization (with similarity\nregularization) NLP: natural language processing, NN: neural network,\nRF: random forest, SV M: support vector machines, V OBN: variable-\norder bayesian network.\nRef.\nMain focus\nSource\n#Obs.\nPublic\nMatching\nPost-hire\nTechniques\n[173]\nselection,\nHigh-tech\n3,825\n✓\nAR, DT\nretention\n[183]\nturnover\nunknown\n1,575\n✓\nDT, LR, NB, SVM, RF\n[76]\nperformance\nCall center\n1,037\n✓\nDT, NB\n[193]\nmatching\nLinkedIn\n11M\n✓\n✓\nProportional hazards model\n[194]\nmatching\nLinkedIn\n2,410\n✓\nCF, CS\n[165]\nmatching\nIBM\nn.a.\n✓\n✓\nKNN\n[182]\nturnover\nHR company\n13,484\n✓\nLR\n[195]\nmatching,\nrepr. learning\nHigh-tech\n>2M\n✓\nCNN, DT, GB, LDA, LR,\nNB, QDA, RF, SVM\n[184]\nturnover\n(1) USA bank\n14,322\n✓\nDT, GB, KNN, LDA, LR,\n(2) IBM\n1,470\nNB, NN, RF, SVM, XGB\n[196]\nmatching,\nrepr. learning\nCareerBuilder\n300K\n✓\nTripartite\ninformation graphs\n[63]\nrecruitment\nNonprofit\n±700K\n✓\nDT, GB, RF, LR,\nNB, SVM, VOBN\n[197]\nmatching\nBOSS Zhipin\n±150K\n✓\nGraph NN\n[198]\nrecruitment,\nmatching\nBaidu\n>1M\n✓\nNN\n[199]\nrecruitment,\nmatching\nOnline\nvacancies\n>2.5M\n✓\nGD, NLP\n[124]\nturnover\nIBM\n1,470\n✓\n✓\nANN, LR, RF, SVM, XGB\nThis\nwork\nmatching,\ninternal mobility\n(1) High-tech\n5,062\n✓\n✓\nKNN, MF, MFSR,\nSlopeOne, SVD\n(2) IT services\n11,327\n(3) HR services\n4,249\n✓\ntems, which use candidate profiles and preferences to suggest job-employee\nmatches, have seen a significant increase in popularity over the past two\ndecades [201]. Both content-based and collaborative filtering methods have\nbeen used in the design of such systems. Research on job-employee matching\nusing social network information and resumes can be found in the work of\n[179], [198], [203]–[205]. Studies on job recommender systems can be found\nin [166], [199]–[202], [206].\n58\n 4.3. Methodology\n4.2.3\nPost-hire setting\nIn this work, we approach job-employee matching from the perspective of\nmobilizing already existing employees within an organization rather than\nrecruiting new candidates. The employees may be in their starting position\nor have prior experience within the organization. We consider an employee’s\npast experience within an organization to be crucial for predicting the suc-\ncess of future job-employee matches. The field of HRA in the context of\nemployee performance has been extensively studied by [171], [172], [174]–\n[178], although their research has been limited to a static view of job po-\nsitions. In contrast, our research focuses on the dynamic modeling of jobs\nthroughout an employee’s career within the organization.\n4.3\nMethodology\nThis section elaborates on the motivation of this research, how an event log\nis taken as a starting point, and the functioning of the RS.\n4.3.1\nProblem definition\nFigure 4.1 shows a simplified employee journey map where three employees\ncan visit eight possible jobs. A more realistic employee journey map, based\non dataset 1, is presented in B.1. Each career path, expressed as a trace in\na process, is characterized by the jobs an employee has held and currently\nholds within an organization. This trace is supplemented with individual,\nemployee-specific data like degree and branch of study. The process begins\nwith employees joining an organization. Next, each employee can sequen-\ntially visit one or multiple jobs.\nFor each job-employee combination, we\nobserve a performance score y. The details of this performance score are\ndiscussed in Section 4.4. For example, employee 1 covers the trace of jobs\n1, 3, and 5 with respective observed performance scores of 0.4, 0.8, and 0.5.\nTo provide decision support, a recommender system can be used to suggest\nsensible next steps in this employee’s career path by predicting a score ˆy for\neach possible job-employee match and then ranking these potential matches\naccording to this predicted outcome score.\nBased on a comprehensive review of the relevant literature [29], [30] and\nby working closely with industry partners, we identify three key challenges\nthat hinder the design and adoption of data-driven decision support in HR\ncompared to other fields. These specific challenges are requirements to con-\nsider during the development of a data-driven method in the context of\ninternal mobility.\nA first challenge is the inferior quality and limited availability of data,\n59\n Chapter 4: Data-driven internal mobility\nFigure 4.1: a simplified example of three internal career paths from a pro-\ncess perspective. This figure shows three employees, each with their own\ncharacteristics, transitioning between eight possible jobs over time. Each\nobserved job-employee combination is characterized by a performance score\ny. To recommend potential next jobs, we infer future performance scores\nˆy for each possible job-employee match. The data for this example can be\nfound in Tables 4.2 and 4.3.\nmaking it often difficult to apply advanced methods, e.g., machine learning,\nthat require large amounts of data.\n[203] describe how the performance\nof their models strongly hit the limits because of lacking data availability.\nWhereas their method is capable of handling more data to further finetune\nand enhance their proposed models. High-standard data accessibility is often\nunfeasible for many organizations without entailing issues with privacy, data\nprotection, or depending on third-party providers.\nAlso in our case, the\navailable real-life data is limited. In response, in this research our objective\nis to develop a method that can maximally gain insights based on a limited,\nrealistically available amount of data.\nA second challenge relates to the authenticity of data that comes from ex-\nternal sources like social media to further enhance employee profiles. While\nsuch data sources can be used to supplement already available data, there\nis a risk that this might lead to misleading decisions. For example, there\nhave been cases where social media information negatively affects the rat-\nings of potential future employees regardless of their true qualifications [207],\n[208]. Therefore, in this paper, we restrict the scope to data that is typi-\ncally available within the HRIS of an organization, where we only have data\navailable on employees and historic job-employee matches without having\n60\n 4.3. Methodology\nany information on job properties.\nA third challenge concerns the accountability, fairness, and explainability\nof often-used opaque ML methods. An infamous example is the Amazon\ncase where a secret AI recruiting tool was removed after showing bias to the\ndetriment of women [209]. Because of the growing need for interpretable\nalgorithms [210]–[212], the objective is to develop a method that takes into\naccount the importance of explainability and transparency, which is further\ndiscussed in Section 4.5.2.\n4.3.2\nEvent log as starting point\nOur analysis represents job-employee matches and their corresponding out-\ncomes using an event log format [213]. An event log is a dataset containing\nevents that are recordings of the execution of an activity within a business\nprocess linked to a particular case [214]. The case or process instance is the\nentity being handled by the process that is analyzed. Events refer to process\ninstances. A trace is a sequence of events. E.g., typical examples in process\nanalytics include the traversal of a customer applying for a loan, or a patient\ngoing through a healthcare process where the customer and patient are the\ncase identifiers respectively. In this representation, each case corresponds\nto an employee, each activity represents a job, and each trace represents an\nemployee’s career within the organization. Formally, we have ",
  "16": "velop a method that takes into\naccount the importance of explainability and transparency, which is further\ndiscussed in Section 4.5.2.\n4.3.2\nEvent log as starting point\nOur analysis represents job-employee matches and their corresponding out-\ncomes using an event log format [213]. An event log is a dataset containing\nevents that are recordings of the execution of an activity within a business\nprocess linked to a particular case [214]. The case or process instance is the\nentity being handled by the process that is analyzed. Events refer to process\ninstances. A trace is a sequence of events. E.g., typical examples in process\nanalytics include the traversal of a customer applying for a loan, or a patient\ngoing through a healthcare process where the customer and patient are the\ncase identifiers respectively. In this representation, each case corresponds\nto an employee, each activity represents a job, and each trace represents an\nemployee’s career within the organization. Formally, we have access to a\ndataset D = {(uk, ts\nk, te\nk, vk, xk, yk, ) ∶k = 1, . . . , K}. An example of D can\nbe found in Table 4.2.\nEach tuple (uk, ts\nk, te\nk, vk, xk, yk) represents a job-employee match with\nemployee uk holding job vk from time ts\nk to time te\nk with an observed outcome\nyk. This outcome yk is defined by the end-user of the system and can be a\nfunction of any feature of interest or the combination of multiple features,\nbased on the preferences and the intent of the end user.\nThis is further\ndiscussed in Section 4.4.1. Employee uk has features xk, e.g., the branch of\nstudy, degree, age, and gender. In total, we observe K job-employee matches\nwhere multiple tuples can refer to the same employee.\nPersonalized RSs provide suggestions to a user based on their profile. In\nthe setting of this research, this means that an RS provides a relevant next\nstep in a career to an employee, based on their profile. An employee profile\nDi ⊆D of person ui consists of the combination of ∣Di∣tuples where ∣Di∣\nis the number of jobs this employee has occupied within this organization,\ni.e., the length of the trace. Hence, ∣Di∣tuples contribute to one employee\nprofile.\nThe employee identifier ui is unique for each employee and the\nsame for all tuples that belong to this employee. Additionally, we assume\nthat each unique job v can be executed at most once by each employee u.\nConsequently, an employee profile Di consists of (i) the visited jobs with\ntheir corresponding performance score y and (ii) personal information xi.\n61\n Chapter 4: Data-driven internal mobility\nTable 4.2: Synthetic example data D in the format of an event log. Figure\n4.1 in Section 4.3.1 visualizes this event log as a directly-follows graph.\nu\nts\nte\nv\nx1\nx2\nx3\nx4\nx5\ny\n1\n10/2014\n06/2016\nJob 1\nMSc\nPhysics\nF\n1975\n1\n0.4\n⎫⎪⎪⎪⎬⎪⎪⎪⎭\nD1\n1\n07/2016\n02/2019\nJob 3\nMSc\nPhysics\nF\n1975\n1\n0.8\n1\n03/2019\n07/2022\nJob 5\nMSc\nPhysics\nF\n1975\n1\n0.5\n2\n09/2009\n02/2016\nJob 1\nBSc\nFinance\nM\n1981\n0.8\n0.9\n}D2\n2\n03/2016\n07/2022\nJob 4\nBSc\nFinance\nM\n1981\n0.8\n0.3\n3\n06/2016\n03/2019\nJob 2\nPhD\nElectronics\nM\n1977\n1\n0.8\n}D3\n3\n04/2019\n07/2022\nJob 3\nPhD\nElectronics\nM\n1977\n1\n0.3\n4\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n4.3.3\nCollaborative filtering\nTwo types of collaborative filtering (CF) algorithms are commonly used:\nneighborhood-based and model-based. Neighborhood-based approaches fo-\ncus on the similarity between either users or items, while model-based ap-\nproaches leverage RS information to construct a model that generates the\nrecommendations [215]. The latter category encompasses the latent factor\napproach, a method we both apply and extend in our current work.\nExtensive literature exists within the RS domain, particularly in CF,\naiming to improve performance through CF enhancement [164]. Particu-\nlarly in sparse datasets, where users have provided ratings for only a few\nitems or items have been rated by only a few users, the quality of recom-\nmendations is significantly compromised [216]. To boost CF performance,\nmany approaches involve the use of customized similarity metrics to tackle\ndata sparsity issues. Here, only rating vectors are used as input, with the\nimpact of data sparsity on similarity measures significantly influencing RS\naccuracy. A detailed overview of diverse customized similarity metrics and\ntheir performance can be found in reference work such as [217], [218].\nCF starts from an observed outcome matrix R ∈Rm×n describing the\noutcome of m employees on n jobs which can be directly derived from the\ninformation on u, v, and y provided in D. Tables 4.2 and 4.3 provide ex-\namples on dataset D and matrix R ∈Rm×n. We observe K job-employee\nmatches, which results in matrix R with a sparsity of (1 −\nK\nm×n). The latent\nfactor approach that we adopt aims to factorize (MF) this matrix R by two\nmatrices U ∈Rl×m and V ∈Rl×n with l < min(m, n).\nEquation 4.1 represents the loss function L.\nFollowing the approach\nof [219], [220], we use gradient descent to obtain two matrices U and V .\nUltimately, with these two matrices, the matrix ˆR = U T V with a predicted\noutcome ˆyij for each combination of ui and vj is calculated.\n62\n 4.3. Methodology\nTable 4.3: Synthetic example data in the format of observed job-employee\nrating matrix Rm×n, derived from D.\nu\nv\nJob 1\nJob 2\nJob 3\nJob 4\nJob 5\nJob 6\n1\n0.4\n0.8\n0.5\n. . .\n2\n0.9\n0.3\n. . .\n3\n0.8\n0.3\n. . .\n4\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nmin\nU,V L(R, U, V ) = 1\n2\nm\n∑\ni=1\nn\n∑\nj=1\nIij (Rij −U T\ni Vj)\n2\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(i)\n+ λ1\n2 ∥U∥2\nF + λ2\n2 ∥V ∥2\nF\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(ii)\n(4.1)\nTerm (i) expresses the difference between the observed outcomes in ma-\ntrix R and the reconstructed outcomes U T V . Since matrix R is sparse, Iij is\nthe indicator function that takes value 1 if the combination of the employee\nui and job vj is observed in the training set and takes value 0 otherwise.\nTerm (ii) consists of two regularization terms with hyperparameters λ1 and\nλ2. ∥⋅∥2\nF denotes the Frobenius form, which is element-wise defined for\na matrix A by Equation 4.2. The complete algorithm of model-based CF\nthrough conventional MF and its implementation in this work can be found\nin B.2.\n∥A∥F ≡\n√\n√\n√\n√\n√\n√\n⎷\nm\n∑\ni=1\nn\n∑\nj=1\n∣aij∣\n2\n(4.2)\n4.3.4\nMatrix factorization with similarity regularization\nThe model-based conventional version of CF, however, is limited in that it\nonly considers the past job positions of employees with their corresponding\nperformance scores when determining similarity and generating latent repre-\nsentations of employees. Consequently, it does not take into account personal\ninformation xi. Employees with limited tenure or new hires may not possess\nenough historical data to calculate representative latent representations and\nconsequently provide reliable recommendations. Therefore, matching them\nwith appropriate jobs within the organization may be difficult.\nTo overcome this limitation, we enhance the traditional matrix factor-\nization approach with similarity regularization SR. This concept is related\nto the idea of social network regularization, introduced by [221]. This ap-\nproach implements the intuition that users of a recommender system value\n63\n Chapter 4: Data-driven internal mobility\nrecommendations from users close in their social network, e.g., good friends,\nmore than from other users [222]. In this work, we introduce a resembling\napproach by identifying similar employees based on personal information\nstored in x. Depending on the available data, personal information may in-\nclude the branch of study, degree, date of birth, full-time equivalent, type of\ncontract, location of employment, and marital status. Some features are nu-\nmerical, others are categorical. The selection of features on which similarity\nis calculated can be done either manually by the user of the system based\non domain knowledge, or can be done by testing the system’s performance\non a separate validation set.\nWe calculate the similarity between two employees using a metric Sim\n(Equation 4.3) to identify resembling peers. The more similar two employees\nui and up are, e.g., by having the same degree, the more similar their latent\nrepresentations Ui and Up should be. To enforce this, the SR term compares\nan employee ui to their m −1 peers up individually and adjusts the latent\nrepresentations accordingly. This is represented by term (iii) in Equation\n4.4, where β > 0.\nThe similarity metric Sim(x, y) handles a mix of numerical and cate-\ngorical components by combining and weighting a numerical and categorical\nmetric: Sim(x, y) = γ ⋅NumSim(xnum, ynum) + (1 −γ) ⋅CatSim(xcat, ycat)\nwhere xnum are the numerical and xcat the categorical variables of vector\nx. The parameter γ is set to the fraction of numerical features in the data.\nFor NumSim we use the cosine similarity. For CatSim we use the Jaccard\nindex to measure the overlap between two categorical vectors. This index\nwill be directly proportional to the number of attributes in which they have\nan equal value [223].\nThe precise formulation of this similarity metric is\ngiven by Equation (4.3) where n is the number of numerical features, k is\nthe number of categorical features, and w is set to 1\nk. For clarity in notation,\nsuperscripts num and cat are omitted.\nA large value of Sim(i, p) indicates that the distance between feature\nvectors Ui and Up should be low and vice versa. By integrating the regular-\nization term into the combined loss function, we introduce information about\nthe relationships or similarities between distinct entities such as employees.\nConsequently, when regularization is employed to incorporate supplementary\ninformation, it goes beyond its conventional role of preventing overfitting.\nInstead, it becomes a tool for injecting employee characteristics into the\nlearning process.\n64\n 4.4. Experimental evaluation\nSim(x, y) = γ ⋅\n∑n\ni=1 xiyi\n√\n∑n\ni=1 x2\ni\n√\n∑n\ni=1 y2\ni\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒ",
  "17": " proportional to the number of attributes in which they have\nan equal value [223].\nThe precise formulation of this similarity metric is\ngiven by Equation (4.3) where n is the number of numerical features, k is\nthe number of categorical features, and w is set to 1\nk. For clarity in notation,\nsuperscripts num and cat are omitted.\nA large value of Sim(i, p) indicates that the distance between feature\nvectors Ui and Up should be low and vice versa. By integrating the regular-\nization term into the combined loss function, we introduce information about\nthe relationships or similarities between distinct entities such as employees.\nConsequently, when regularization is employed to incorporate supplementary\ninformation, it goes beyond its conventional role of preventing overfitting.\nInstead, it becomes a tool for injecting employee characteristics into the\nlearning process.\n64\n 4.4. Experimental evaluation\nSim(x, y) = γ ⋅\n∑n\ni=1 xiyi\n√\n∑n\ni=1 x2\ni\n√\n∑n\ni=1 y2\ni\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\nNumSim\n+(1 −γ) ⋅\nk\n∑\ni=1\n(w ⋅s (xi, yi))\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\nCatSim\nwith s (xi, yi) = { 1\nif xi = yi\n0\notherwise\n(4.3)\nThe complete loss function of MF with SR consists of three terms (Equa-\ntion 4.4). The SR term is labeled as (iii).\nmin\nU,V L(R, U, V ) = 1\n2\nm\n∑\ni=1\nn\n∑\nj=1\nIij (Rij −U T\ni Vj)\n2\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(i)\n+ λ1\n2 ∥U∥2\nF + λ2\n2 ∥V ∥2\nF\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(ii)\n+ β\n2\nm\n∑\ni=1\nm\n∑\np=1\nSim(i, p) ∥Ui −Up∥\n2\nF\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(iii)\n(4.4)\nThe full algorithm for MF with SR is represented by Algorithm 1. Specif-\nically, the extension to regular MF (i.e., as presented in B.2) and MFSR\nare step 6 where γi is calculated, step 7 where βγi is added, and step 10 were\n(iii) is added to the composite loss function.\n4.4\nExperimental evaluation\nIn this section, the implementation of our proposed method is tested on three\nreal-life datasets of which two are private and one is made publicly available.\nFirst, we describe the datasets and performance score y. Next, we outline\nthe setup of the experiment.\n4.4.1\nData\nDescription of observed y\nWe use a performance score y ∈[0, 1] to as-\nsess the quality of each job-employee match (u, ts, te, v, x, y) in the dataset\nD. Taking into account the trade-off between costs and benefits of onboard-\ning people, the measure y that is defined exhibits a sigmoidal relationship\nwith lead time. Specifically, lower lead times correspond to low values for y.\nAs lead time increases, y rises until it saturates for longer lead times. This\n65\n Chapter 4: Data-driven internal mobility\nAlgorithm 1: Matrix Factorization with Similarity Regularization\nInput\n: observed ratings R, initial matrices U and V , learning\nrate α, regularization parameters λ1 and λ2, learning\nsteps n, stopping threshold t, similarity regularization\nparameter β, similarity metric Sim\nOutput : matrix ˆR = U T V with estimated ratings\nfor steps = 1, 2, . . . , n do\nfor each element Ri,j do\n;\n> i ∈{u1, u2, . . . , um}, j ∈{v1, v2, . . . , vn}\nif Ri,j > 0 then\nei,j ∶= Ri,j −ˆRi,j ;\n> calculate error\nγi = ∑m\np=1 Sim(i, p) (Ui −Up) ;\n> sim.\nreg.\nUi ∶= Ui + α(ei,jVj\nÍÒÒÒÒÒÒÑÒÒÒÒÒÒÏ\n(i)\n+ λ1Ui\nÍÒÒÒÒÑÒÒÒÒÏ\n(ii )\n+ βγi\nÍÒÒÑÒÒÏ\n(iii )\n) ;\n> update vector Ui\nVj ∶= Vj + α(ei,jUi\nÍÒÒÒÒÒÒÑÒÒÒÒÒÒÏ\n(i)\n+ λ2Vj)\nÍÒÒÒÒÒÒÑÒÒÒÒÒÒÒÏ\n(ii)\n;\n> update vector Vj\ne ∶= 1\n2 ∑m\ni=1 ∑n\nj=1 Iij (Rij −U T\ni Vj)\n2\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(i)\n+ λ1\n2 ∥U∥2\nF + λ2\n2 ∥V ∥2\nF\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(ii)\n+ β\n2 ∑m\ni=1 ∑m\np=1 Sim(i, p) ∥Ui −Up∥\n2\nF\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(iii)\n;\n> sim.\nreg.\nif error e < t then\nbreak ;\n> stop if error falls below threshold t\nReturn : ˆR = U T V\n66\n 4.4. Experimental evaluation\nrelationship is formally given by Equation 4.5, where t is expressed in years,\nso that y(1) = 0.12, y(2) = 0.5, y(3) = 0.88, and y(4) = 0.98.\ny(t) =\n1\n1 + e−2(t−2)\n(4.5)\nThis approach is based on discussions with the industry partner that de-\nlivered the data and perceives this to be a viable approach for using the RS\nin the setting of filling hard-to-fill positions. By combining insights from the\nliterature [224] and our own experience, we have identified various factors\nthat make it difficult to accurately and appropriately measure job perfor-\nmance and define the metric y. These factors include a lack of consistent\nmonitoring of job specifications and work outcomes, constraints posed by\nprivacy and ethical concerns, the potential influence of biases on individual\nperformance assessments, discrepancies between intended and actual mea-\nsurements, the interdependence of complex jobs in which outcomes result\nfrom the collective efforts of multiple employees, and the untraceable nature\nof certain employee behaviors.\nGiven these challenging factors, we have opted to use a performance\nmetric based on lead times, which is always observed and a commonly-used\napproach in the field of labor economics [225]. Specifically, we treat a job\nmatch as a pure experience good, where employees with satisfactory matches\nare more inclined to stay, while those with lower-quality matches tend to\nleave early [226], [227].\nHowever, we recognize that our definition of y serves as a proxy for true\njob-employee success. Other metrics and definitions may exist, depending\non available data and the precise objectives of the recommender system.\nThese variables to determine y could include salary increase, performance\ngain, skill enhancement, education, or training. Nonetheless, exploring these\nalternatives would warrant a separate research study and is beyond the scope\nof this manuscript.\ny and ongoing cases\nIn the context of HR and job-employee matching\nin specific, a match may be censored if the employee leaves the company or\nchanges roles within the company after the end of the data capturing period\n[228]. This type of censoring can impact the outcome variable y, as it takes\nlead times into consideration. To ensure the validity and accuracy of our\nresults, we choose to exclude ongoing observations, and withhold a random\nsubsample to reduce the risk of bias and increase the validity of the results.\nThe downside of leaving out these observations is the increased sparsity of\nmatrix R, which could negatively affect the accuracy of our method [229],\n[230].\n67\n Chapter 4: Data-driven internal mobility\nDataset description\nWe have access to three real-life event logs, one\nof which is fully anonymized and made publicly available along with the\npublication of this paper. A summary of the characteristics of these datasets\nis presented in Table 4.4. All three datasets are extracted from an HRIS and\nrepresent three different companies. Dataset 1 originates from a high-tech\nR&D company with ±3,000 employees. It contains twelve years of HR data,\nresulting in over 5,062 observed job-employee matches. Dataset 2 is provided\nby a company active in IT services with ±4,500 employees. It spans a period\nof ten years, resulting in 11,327 observed matches. Dataset 3 is provided by\na company active in HR services with ±1,500 employees. The data spans a\nperiod of 10 years, resulting in 3,792 observed matches.\nPersonal features stored in x\nThe personal employee data x that is\navailable for analysis varies among the datasets. Depending on the dataset, x\nmay include information on degree, the field of study, contract type, location\nof employment, and percentage employed. The values of these features may\nchange over time for a small number of observations. For these observations,\nthe features are updated to the first-observed values, respecting the out-of-\ntime aspect for validation and testing. Other features that are available in\neach dataset but not considered in this analysis include nationality, marital\nstatus, zip code of residence, and gender.\nAnonymous dataset\nDataset 3 is publicly available on GitHub. It is ob-\ntained from an HR services provider and fully anonymized. Due to confiden-\ntiality concerns, the original features and additional background information\nabout the data cannot be disclosed. Personal information x consists of eight\ncategorical variables (V 01−V 08) and three numerical variables (V 09−V 11)\nwhich are normalized between 0 and 1. When an employee leaves the organi-\nzation, this is indicated with the leave activity. The data covers the period\nfrom 2012 to 2021.\nTable 4.4: Overview of datasets\nDataset\nSource\nPublic\n#Empl.\n#Jobs\n#Matches\nTimeframe\nRm×n sparsity\n1\nHigh-tech\n±3000\n±250\n5,062\n2009-2021\n99.3%\n2\nIT Services\n±4500\n±200\n11,327\n2012-2022\n98.7%\n3\nHR Services\n✓\n±1500\n±250\n3,792\n2012-2021\n99.0%\n4.4.2\nExperimental setup\nEach dataset is split into a training, validation, and test set with proportions\nof 0.5/0.25/0.25 in an out-of-time fashion. A job-employee rating matrix\n68\n 4.4. Experimental evaluation\nRm×n is created based on the performance scores of the observed matches.\nAn example is shown in Table 4.3.\nTo be able to calculate latent representations of employees, at least an\nemployee’s first job is required in the training set. Therefore, if a certain\nemployee ui has no observed first job in the training set, we add one from\nthe validation or test set. This approach is consistent with the focus of our\nresearch, which is on matching current employees with new jobs within the\ncompany rather than placing unemployed individuals in open vacancies.\nTo evaluate the performance of our proposed methods, we use both met-\nrics for predicted score accuracy and metrics for assessing the relative ranking\nof proposed matches. The latter type of metric is used because it allows us\nto compare the expected performance of matches rel",
  "18": "it into a training, validation, and test set with proportions\nof 0.5/0.25/0.25 in an out-of-time fashion. A job-employee rating matrix\n68\n 4.4. Experimental evaluation\nRm×n is created based on the performance scores of the observed matches.\nAn example is shown in Table 4.3.\nTo be able to calculate latent representations of employees, at least an\nemployee’s first job is required in the training set. Therefore, if a certain\nemployee ui has no observed first job in the training set, we add one from\nthe validation or test set. This approach is consistent with the focus of our\nresearch, which is on matching current employees with new jobs within the\ncompany rather than placing unemployed individuals in open vacancies.\nTo evaluate the performance of our proposed methods, we use both met-\nrics for predicted score accuracy and metrics for assessing the relative ranking\nof proposed matches. The latter type of metric is used because it allows us\nto compare the expected performance of matches relative to other proposed\nmatches, rather than just looking at the predicted scores themselves.\nThe Mean Absolute Error (MAE) and Root Mean Square Error (RMSE),\nas shown in Equations 4.6 and 4.7, are used to measure the prediction accu-\nracy of the actual performance score of a match. Dtest denotes the test set\nof matches that are used for evaluation. Since the set of observed ground-\ntruth matches is extremely sparse, we tweak the metrics’ definition by only\nsumming matches present in Dtest.\nMAE =\n∑(i,j)∈Dtest\n»»»»»Rij −̂\nRij»»»»»\n∣Dtest∣\n(4.6)\nRMSE =\n√\n√\n√\n√\n√\n√\n√\n⎷∑(i,j)∈Dtest (Rij −̂\nRij)\n2\n∣Dtest∣\n(4.7)\nTo assess the relative ranking of matches, we use the Spearman and\nKendall rank correlation coefficients, which measure the extent to which the\norder of the match scores is predicted correctly. These metrics are deployed\nfrom a job point of view, i.e, we evaluate the proposed ranking of employees\nper job with the true ranking in the test set. Moreover, only jobs with more\nthan two matches are considered, as a ranking is only possible with at least\ntwo matches. A higher score indicates better performance.\nWe evaluate the performance of matrix factorization with (MFSR) and\nwithout (MF) similarity regularization on three datasets.\nWe compare\nthe performance with six other methods: Singular Value Decomposition\n(SV D), SlopeOne, and various nearest neighbor approaches utilizing cosine\nsimilarity (KNNc), Pearson similarity (KNNp), difference-based similarity\n(KNNsmd), and triangle-based cosine measure (KNNta) [217].\nThe selection of these six methods was motivated as follows: SV D closely\nresembles our MFSR approach and is a well-established model-based CF\ntechnique. SlopeOne was chosen for its simplicity, efficiency, and widespread\n69\n Chapter 4: Data-driven internal mobility\nadoption. KNN was included due to its extensive usage in the literature\n[164] and its versatility in employing different similarity measures [215].\nKNNc and KNNp were added for their simplicity and common application.\nConversely, KNNsmd and KNNta represent more advanced techniques that\nare state-of-the-art in neighborhood-based CF [217].\nSV D decomposes the job-employee matrix R into UΣV T where U and\nV are left and right singular matrices and Σ is a rectangular diagonal matrix\nwith non-negative real numbers on the diagonal, to approximate R and make\npredictions.\nThe SlopeOne algorithm predicts item ratings based on the average de-\nviation of ratings for similar items and users [231]. It calculates pairwise\ndifferences between all items from matrix R and generates a deviation ma-\ntrix storing average deviation values for each item pair to make predictions.\nIn recommender systems, KNN predicts item ratings based on the rat-\nings of k nearest items or users. It constructs a user-item matrix, computes\nsimilarities using a chosen distance metric, identifies the nearest items or\nusers, and predicts the rating of the target item by the user based on their\nratings. Many similarity metrics are available, ranging from basic (such as\ncosine and Pearson) to advanced methods tailored to address typical RS chal-\nlenges like data sparsity, demonstrating state-of-the-art performance [217].\nSV D, SlopeOne, KNNc, and KNNp were implemented using the Sur-\nprise package with the Python implementation provided by [232]. KNNsmd\nand KNNta are based on the implementation of [217].\nWe perform a rigorous grid search on a separate out-of-time validation set\nfor optimizing each method’s hyperparameters. A full overview of the con-\nsidered grid and the optimal hyperparameters for each method and dataset\nare summarized in B.3.\nThese hyperparameters are tuned separately for\neach method and dataset.\n4.5\nResults and discussion\nThis section first presents the experimental results. Then, the discussion\nhighlights the advantages and covers the potential drawbacks of our proposed\nmethod and of using data-driven methods in HRM in a more general context.\n4.5.1\nResults\nTable 4.5 summarizes the experimental results. The MFSR method consis-\ntently performs best across all datasets in terms of the RMSE and Kendall\nrank correlation. Moreover, MFSR always outperforms MF in all metrics\nover all datasets, highlighting the benefit of including the similarity reg-\nularization term. The state-of-the-art benchmark methods KNNsmd and\n70\n 4.5. Results and discussion\nKNNta display similar performance and often perform best.\nThe other\nmethods have varying performances across the datasets and metrics.\nTable 4.5: Summary of results. The table presents the results of evaluating\nthree model-based CF methods (MF, MFSR, and SV D) and five memory-\nbased CF methods (SlopeOne, KNNc, KNNp, KNNsmd, and KNNta) on\nthree different datasets using four metrics. The Mean Absolute Error (MAE)\nand Root Mean Squared Error (RMSE) are both measures of prediction er-\nror, with lower values indicating better performance. The Spearman and\nKendall coefficients are both measures of rank correlation, with values closer\nto 1 indicating a stronger positive correlation. The best results are high-\nlighted in bold.\nDataset\nMetric\nMFSR\nMF\nSV D\nSlopeOne\nKNNc\nKNNp\nKNNsmd\nKNNta\n1\nMAE\n0.171\n0.176\n0.179\n0.166\n0.184\n0.189\n0.157\n0.159\nRMSE\n0.204\n0.234\n0.222\n0.210\n0.225\n0.224\n0.217\n0.216\nSpearman\n0.265\n0.172\n0.166\n0.317\n0.063\n-0.018\n0.339\n0.322\nKendall\n0.409\n0.334\n0.358\n0.340\n0.285\n0.250\n0.358\n0.374\n2\nMAE\n0.188\n0.215\n0.195\n0.220\n0.263\n0.206\n0.185\n0.188\nRMSE\n0.237\n0.287\n0.259\n0.285\n0.317\n0.257\n0.282\n0.281\nSpearman\n0.048\n0.028\n0.114\n0.105\n0.066\n-0.029\n0.092\n0.076\nKendall\n0.267\n0.219\n0.244\n0.236\n0.220\n0.165\n0.240\n0.247\n3\nMAE\n0.164\n0.174\n0.168\n0.173\n0.178\n0.174\n0.177\n0.170\nRMSE\n0.201\n0.219\n0.211\n0.217\n0.217\n0.208\n0.221\n0.214\nSpearman\n0.194\n0.216\n0.172\n0.226\n-0.028\n0.050\n0.243\n0.290\nKendall\n0.386\n0.346\n0.345\n0.325\n0.253\n0.253\n0.372\n0.363\nThe experiment’s findings, consolidated across the three datasets are\nsummarized in Table 4.6, showcasing the average ranking (AR) of methods\nover datasets per metric.\nTo test the experimental results for statistical\nsignificance, we employ a methodology described by Demšar [131], leveraging\na combination of the Friedman test and a post-hoc Dunn test with Holm p-\nadjustments for multiple comparisons [233].\nThe bottom of Table 4.6 presents the Friedman χ2 statistic, assessing\nwhether there are statistically significant differences in performance among\nthe methods. Enclosed in brackets are the p-values derived from the post-\nhoc test. The underscored p-values in the table denote significance at the\n5% level, rejecting the null hypothesis of equal performance compared to the\nbest method per metric.\nFrom this table, we conclude the following. First, although table C.1\nindicates that MFSR only scores best once in terms of MAE, on aggregate\nover the three datasets this method still is ranked first on average. Second,\nin terms of RMSE, the performances of MF and MFSR are significantly dif-\nferent, highlighting the benefit of adding the similarity regularization term.\nThird, in terms of Spearman, the Friedman χ2 test statistic indicates some\n71\n Chapter 4: Data-driven internal mobility\nsignificant difference but the post-hoc test fails to establish the difference\nto be significant.\nThis may be a result of the lower power of the latter\nas described by [131]. In general, the results suggest a good performance\nof MFSR, but for establishing the difference to be statistically significant,\ntypically more data sets are needed to increase the power of the tests.\nTable 4.6: Average ranking of methods across datasets per performance mea-\nsure. The best-performing methods are emphasized in bold. P-values are\nadded between brackets. The P-values of methods that perform statistically\nsignificantly differently at the 5% level from the best-performing one are un-\nderlined.\nMethod\nMAE\nRMSE\nSpearman\nKendall\nMFSR\n2.3\n(1.000)\n1.0\n(1.000)\n5.0\n(0.915)\n1.0\n(1.000)\nMF\n5.7\n(0.857)\n7.3\n(0.025)\n5.3\n(0.844)\n5.7\n(0.307)\nSVD\n4.0\n(0.998)\n3.7\n(0.959)\n4.3\n(0.976)\n3.7\n(0.941)\nSlopeOne\n4.7\n(0.986)\n4.7\n(0.742)\n2.7\n(0.998)\n5.3\n(0.419)\nKNNc\n7.7\n(0.137)\n6.7\n(0.078)\n6.7\n(0.307)\n7.0\n(0.044)\nKNNp\n6.0\n(0.757)\n3.3\n(0.976)\n7.7\n(0.081)\n7.7\n(0.013)\nKNNsmd\n3.0\n(0.998)\n5.7\n(0.317)\n2.0\n(1.000)\n3.3\n(0.976)\nKNNta\n2.7\n(0.998)\n3.7\n(0.959)\n2.3\n(0.998)\n2.3\n(0.990)\nFried. χ2\n12.1\n(0.096)\n14.6\n(0.042)\n15.0\n(0.036)\n18.7\n(0.009)\n4.5.2\nDiscussion\nThis study explores the potential advantages of data mining and machine\nlearning techniques in HRM. Specifically, the use of data-driven recom-\nmender systems for job matching and career path management is examined.\nWhile these methods have the potential to improve decision support in HRM,\nthere are also risks and drawbacks that must be considered. In the following\ndiscussion, the advantages, drawbacks, and limitations of the current study\nare analyzed in greater depth.\nTo mitigate the risk of making bad decisions that is associated with the\nuse of these data-driven methods, it is to use them as a tool for decision\nsupport, rather than for automating ",
  "19": ".7\n(0.986)\n4.7\n(0.742)\n2.7\n(0.998)\n5.3\n(0.419)\nKNNc\n7.7\n(0.137)\n6.7\n(0.078)\n6.7\n(0.307)\n7.0\n(0.044)\nKNNp\n6.0\n(0.757)\n3.3\n(0.976)\n7.7\n(0.081)\n7.7\n(0.013)\nKNNsmd\n3.0\n(0.998)\n5.7\n(0.317)\n2.0\n(1.000)\n3.3\n(0.976)\nKNNta\n2.7\n(0.998)\n3.7\n(0.959)\n2.3\n(0.998)\n2.3\n(0.990)\nFried. χ2\n12.1\n(0.096)\n14.6\n(0.042)\n15.0\n(0.036)\n18.7\n(0.009)\n4.5.2\nDiscussion\nThis study explores the potential advantages of data mining and machine\nlearning techniques in HRM. Specifically, the use of data-driven recom-\nmender systems for job matching and career path management is examined.\nWhile these methods have the potential to improve decision support in HRM,\nthere are also risks and drawbacks that must be considered. In the following\ndiscussion, the advantages, drawbacks, and limitations of the current study\nare analyzed in greater depth.\nTo mitigate the risk of making bad decisions that is associated with the\nuse of these data-driven methods, it is to use them as a tool for decision\nsupport, rather than for automating decision-making. HR professionals and\nemployees obviously should have the final say in any proposed job-employee\nmatch, but recommendations that are made by such a system would also\nhave to be examined and interpreted before acting on them.\nAdvantages\nOne advantage of the proposed method is its ability to ad-\ndress the cold start problem, where little information is available about past\n72\n 4.5. Results and discussion\nexperience of employees. By utilizing similarity regularization, these meth-\nods can create meaningful embeddings based on the similarity with other\nemployees even in the absence of extensive information about an employee\nin question’s past experience.\nAdditionally, collaborative filtering, which utilizes matrix factorization\ntechniques, maps both employees and jobs to a latent factor space in such a\nway that job-employee matches are the result of inner products within this\nspace [234]. For employees, the latent factors could potentially correspond\nto essential dimensions such as possessing specific technical proficiency or\neffective management competency. Even if this were not the case, despite\nthe fact that the latent factors themselves may not seem transparent, they\nstill function as indicators of the closeness and resemblances between jobs\nand among employees. This addresses the specific challenges listed in Section\n4.3.1.\nFigure 4.2 displays a two-dimensional t-SNE plot of the higher-dimensional\nlatent representation of employees [235]. The effect of similarity regulariza-\ntion on the latent representation can be observed by comparing the two\npanels, as clusters between similar employees are distinctly visible. For the\npurpose of simplicity, the similarity metric for similarity regularization in\nthis example is based on education level only. When multiple variables are\ntaken into account for calculating similarity, or when considering a higher-\ndimensional representation, the clusters become visually less distinguishable\nin a two-dimensional plot.\nDrawbacks\nOne of the main drawbacks of RSs is their tendency towards\nover-specialization, which can lead to a lack of exploration and novelty in\ndecision-making [200]–[202]. This phenomenon, referred to as the serendipity\nproblem, has been addressed in previous research through the incorporation\nof randomness to explore more novel options [236]–[238].\nHowever, it is\nworth noting that this issue may be less significant in cases where routine\ndecisions are the primary focus or where the user of the system is aware of\nthis issue.\nAdditionally, RSs may prioritize short-term considerations, such as the\nnext job an employee should take, rather than considering longer-term ca-\nreer development. Another limitation of RSs is their domain-specific nature.\nBecause of current or historic HR strategies and policies, observed matches\nin the data do not happen at random. In other words, some of the observa-\ntions on job-employee matches and transitions are missing in a non-random\nway (MNAR) and are related to some characteristics of the data that are not\nobserved [239]. This can make it difficult to accurately extrapolate insights\nto a new domain, i.e. transitions between jobs that have never been observed\nbefore, and may also impact the validity of evaluation measures. Special-\n73\n Chapter 4: Data-driven internal mobility\n(a) Two-dimensional latent representa-\ntion for MF without similarity regular-\nization for dataset 1.\n(b) Two-dimensional latent representa-\ntion for MF with similarity regulariza-\ntion on education for dataset 1.\nFigure 4.2: The effect of similarity regularization.\nThe two panels dis-\nplay the m two-dimensional latent representation of employees (Ui with i =\n1, 2, . . . , m).\nPanel (a) displays a much more scattered representation,\nwhereas panel (b) clearly shows a more clustered representation.\nized methods such as multiple imputation, inverse probability weighting, or\ncausal methods could mitigate or even eliminate this bias [240].\nIt is also worth noting that historical data, which is often used to train\nthese systems, may not accurately reflect current HR policies or job transi-\ntion patterns as a result of concept drift. To evaluate job-employee matches\nbased on the metric y as defined in this manuscript is a strong simplifica-\ntion. In reality, the appointment and evaluation of employees are influenced\nby many factors, including more nuanced soft HR-based evaluation crite-\nria. Furthermore, this method does not consider the capacity of positions\nor any other constraints or HR policies that may impact the feasibility of\nrecommended job-employee matches. It is also highly sensitive to proper\nhyperparameter tuning, requiring careful validation in order to achieve reli-\nable results. Finally, while this method has been evaluated for its ability to\npredict employee performance, it does not consider factors such as diversity,\nserendipity, novelty, and robustness in its evaluation metrics.\n4.6\nConclusion\nWith the increasing trend of companies utilizing big data, analytics, and\nmachine learning to gain a competitive advantage, also the HR field sees\nsignificant potential in using such tools to support decision-making. As a re-\n74\n 4.6. Conclusion\nsult, HR departments are seeking to complement traditional evidence-based\ntechniques with data-driven insights to recommend career steps, streamline\nmanual candidate searches, and manage the internal talent pool.\nWe present a data-driven approach for internally matching employees\nwith job opportunities and guiding career path decisions using a recom-\nmender system with collaborative filtering as a baseline. This method’s per-\nformance is further improved by incorporating personal information about\nemployees with a similarity-based regularization term. Doing so addresses\nthe cold start problem which is present due to limited data on new hires\nand employees with short tenure. This data limitation makes it difficult to\ngenerate meaningful, similarity-based latent employee representations. By\nconsidering both the perspective of the employee and the organization, our\napproach is able to provide more targeted and effective recommendations for\ninternal mobility.\nThe efficacy of our approach was rigorously tested on three real-life\ndatasets comprising thousands of job-employee matches. The results of this\nevaluation showed that our method outperformed the default collaborative\nfiltering approach in terms of ranking performance. Python code of the pre-\nsented method and experiments is provided, along with an anonymized HR\ndataset on internal mobility in the format of an event log, so as to facilitate\nreplication of the presented results and to spur further research.\nOverall, the presented results highlight the potential of using data-driven\napproaches to support internal mobility management in organizations and\nprovide objective and data-driven insights that can complement the blend\nof HR professionals’ expertise and intuition on the one hand and evidence-\nbased techniques from more traditional scientific HR literature on the other\nhand. The adoption of such approaches has the potential to transform the\nway organizations manage the careers of their employees and make informed\ndecisions about internal mobility.\nFuture work will focus on mitigating the effect of selection bias, i.e., con-\ntrolling for the current HR policies that result in the non-random selection\nof the next jobs in a career path. We believe this issue could be addressed\nwith causal methods.\n75\n  Part II\nMethodological advances\n77\n  5\nRobust instance-dependent\ncost-sensitive classification\nInstance-dependent cost-sensitive (IDCS) learning methods have proven use-\nful for binary classification tasks where individual instances are associated\nwith variable misclassification costs. However, we demonstrate in this pa-\nper by means of a series of experiments that IDCS methods are sensitive to\nnoise and outliers in relation to instance-dependent misclassification costs\nand their performance strongly depends on the cost distribution of the data\nsample. Therefore, we propose a generic three-step framework to make IDCS\nmethods more robust: (i) detect outliers automatically, (ii) correct outlying\ncost information in a data-driven way, and (iii) construct an IDCS learning\nmethod using the adjusted cost information. We apply this framework to\ncslogit, a logistic regression-based IDCS method, to obtain its robust ver-\nsion, which we name r-cslogit. The robustness of this approach is introduced\nin steps (i) and (ii), where we make use of robust estimators to detect and\nimpute outlying costs of individual instances. The newly proposed r-cslogit\nmethod is tested on synthetic and semi-synthetic data and proven to be\nsuperior in terms of savings compared to its non-robust counterpart for vari-\nable levels of noise and outliers. All our code is made available online at\nhttps://github.com/SimonDeVos/Robust-IDCS.\n79\n Chapter 5: Robust instance-dependent cost-sensitive classification\n5.1",
  "20": "strongly depends on the cost distribution of the data\nsample. Therefore, we propose a generic three-step framework to make IDCS\nmethods more robust: (i) detect outliers automatically, (ii) correct outlying\ncost information in a data-driven way, and (iii) construct an IDCS learning\nmethod using the adjusted cost information. We apply this framework to\ncslogit, a logistic regression-based IDCS method, to obtain its robust ver-\nsion, which we name r-cslogit. The robustness of this approach is introduced\nin steps (i) and (ii), where we make use of robust estimators to detect and\nimpute outlying costs of individual instances. The newly proposed r-cslogit\nmethod is tested on synthetic and semi-synthetic data and proven to be\nsuperior in terms of savings compared to its non-robust counterpart for vari-\nable levels of noise and outliers. All our code is made available online at\nhttps://github.com/SimonDeVos/Robust-IDCS.\n79\n Chapter 5: Robust instance-dependent cost-sensitive classification\n5.1\nIntroduction\nClassification is a well-studied machine learning task that involves the as-\nsignment of instances to a predefined set of outcome classes. Cost-sensitive\nclassification methods take into account asymmetric costs related to incor-\nrectly classifying instances across various classes [9], [241]. Such misclassifi-\ncation costs may either be class-dependent, i.e., equal for all instances of a\nclass, or instance-dependent, i.e., vary across instances.\nClassification methods are adopted to support or automate business decision-\nmaking, e.g., for credit scoring [242] or customer churn prediction [243]. Note\nthat in both applications, misclassified instances involve variable costs. For\ninstance, the cost of a misclassified churner equals the future customer life-\ntime value, whereas a misclassified non-churner typically involves a much\nsmaller cost, i.e., the cost of targeting the customer with the retention cam-\npaign. Either or both may be instance-dependent or class-dependent de-\npending on the characteristics of the particular application setting.\nA broad variety of cost-sensitive (CS) and instance-dependent cost-sensitive\n(IDCS) classification methods have been proposed in the literature as re-\nviewed and experimentally evaluated by [244] and [245]. A prominent ap-\nproach that is adopted by both CS and IDCS methods for taking misclas-\nsification costs into account is to weigh instances proportionally with the\nmisclassification cost involved when learning a classification model.\nIn this article, we raise the question of whether IDCS classification meth-\nods are sensitive to outliers and noise in the data. No prior work seems to\nhave addressed this question, which nonetheless is of significant practical im-\nportance given the broad adoption and potential monetary impact of using\nbiased classification models for decision-making.\nTo address these shortcomings, we present the results of a series of ex-\nperiments to evaluate the robustness of IDCS classification methods with\nrespect to outlying costs in the data, which highlight the potential bias and\nvulnerability of IDCS classification methods. We propose a robust approach\nto IDCS classification by extending the existing cslogit approach [14]. An\nimportant benefit is the automatic and reliable detection of outliers in the\ndata. These outliers may not only spoil the resulting analysis (as illustrated\nin this article) but can also contain valuable information. A robust analysis\ncan thus provide better insight into the structure of the data.\nThe following section outlines related work on IDCS learning and dis-\ncusses both cslogit and robustness.\nNext, in Section 5.3, simulations on\nsynthetic data are presented that motivate the need for robust IDCS learn-\ning, which we develop in Section 5.4.\nSection 5.5 presents the results of\nexperiments illustrating the excellent performance of the proposed robust\nIDCS learning method, denoted r-cslogit, in comparison with both logit and\ncslogit. We conclude and present directions for future research in Section 5.6.\n80\n 5.2. Related work\n5.2\nRelated work\nElkan [9] introduces a learning paradigm where different misclassification\nerrors incur different penalties depending on the predicted and actual class,\nwith applications to, for example, detecting transaction fraud and credit\nscoring. The benefits and costs of different predictions can be summarized\nin a two-dimensional instance-dependent cost matrix with one dimension for\nthe predicted value and another dimension for the ground truth. Given these\nbenefits and costs, each new instance should be assigned to the class that\nleads to the lowest expected cost, which is calculated by means of conditional\nprobabilities.\n5.2.1\nIDCS learning, cslogit, and robustness\nFor certain applications, benefits and costs depend not only on the class\nbut also on the instance itself. Therefore, instance-dependent cost-sensitive\nlearning considers a more detailed, lower level of granularity than class-\ndependent costs. For these applications, using instance-dependent costs in-\nstead of class-dependent costs leads to a decreased total misclassification\ncost [245], [246].\nSeveral instance-dependent cost-sensitive methodologies have been pro-\nposed in the literature, with recent overviews given by [244] and [245]. Es-\npecially relevant to our work are methodologies that adjust the learning al-\ngorithm to incorporate instance-dependent costs. Instance-dependent cost-\nsensitive variants have been proposed for several common machine learning\nclassifiers, such as boosting [14], [247], [248], support vector machines [246],\ndecision trees [249], [250], and logistic regression [14], [251].\nIn this work, we will build upon an instance-dependent cost-sensitive\nversion of logistic regression. Following [14], we will refer to this method\nas cslogit. Logistic regression is a widely used method for binary classifica-\ntion tasks. To extend logistic regression to its IDCS counterpart, Bahnsen,\nAouada, Stojanovic, et al. [252] and Höppner, Baesens, Verbeke, et al.\n[14] propose an objective function that combines both cost-sensitivity and\ninstance-dependent learning, resulting in instance-dependent costs for opti-\nmization. The application of this objective function yields significant im-\nprovements in terms of higher savings compared to cost-insensitive or class-\ndependent cost-sensitive models in the context of, for example, credit scoring\nand transaction fraud detection.\nClassical nonrobust methods for regression, such as least squares or max-\nimum likelihood techniques, try to fit the model optimally to all the data.\nAs a result, these methods are heavily influenced by data outliers. This im-\nplies that outliers may bias the parameter estimates and confidence intervals,\n81\n Chapter 5: Robust instance-dependent cost-sensitive classification\nand thus, hypothesis tests may become unreliable and/or uninformative. In\ncontrast, robust methods can resist the effect of outliers to avoid distorted\nresults and false conclusions. As an important benefit, they allow the au-\ntomatic detection of outliers as observations that deviate substantially from\nthe robust fit. It is important to note that the detected outliers are not\nnecessarily errors in the data.\nThe presence of outliers may reveal that\nthe data are more heterogeneous than has been assumed and that it can\nbe handled by the original statistical model.\nOutliers can be isolated or\nmay come in clusters, indicating that there are subgroups in the population\nthat behave differently. Many different approaches to robust regression have\nbeen proposed, and a good overview can be found in reference works such\nas [253]–[255]. In the context of generalized linear models (GLMs), various\nrobust alternatives have been presented, such as [256]–[260]. Robust logistic\nregression has been studied by [261]–[269].\n5.2.2\nPreliminaries\nThe dataset D consists of N observed predictor-response pairs {(xi, yi)}N\ni=1\nand is used to train a binary classification model s(.). The costs Ci corre-\nspond to the cost matrix defined in Table 5.1.\nThis binary classification model predicts a probability score si ∈[0, 1]\nfor each instance i based on the features xi. Depending on the classification\nthreshold t∗\ni , si is converted to a predicted class ̂yi ∈{0, 1}.\nFor models trained with AEC (Equation (5.3)), savings remain relatively\nstable across different thresholding strategies [245].\nTherefore, we use a\ndefault threshold of 0.5.\nA binary logistic regression predicts a probability score that an obser-\nvation belongs to the positive class. This probability score is calculated by\nEquation (5.1), where β0 is the bias term, β1 . . . βd the learned weights and\nxi are the features of a particular observation i:\nsi = s(β0,β)(xi) =\n1\n1 + e−z where z = β0 + β1xi1 + β2xi2 + . . . + βdxid. (5.1)\nThis probability score is then compared to a threshold to categorize each\nof these observations into classes. The objective function of a logistic re-\ngression is the likelihood that is maximized or the cross-entropy loss that is\nminimized. For a single sample with true label yi ∈{0, 1} and a probability\nscore si = P(Y = 1), the cross-entropy loss is presented by Equation (5.2):\nLlog(yi, si) = −(yi log(si) + (1 −yi) log(1 −si)).\n(5.2)\nNote that this equation does not take into account any costs. Because\nthis objective function assigns equal weights to each misclassification, it does\n82\n 5.2. Related work\nnot necessarily correspond to the underlying business problem where costs\nare to be minimized. The reason for this is twofold: misclassification costs\nare different per class and per instance. The real business objective is to\nminimize the average expected total cost of the binary classifier.\nWe build upon the instance-dependent cost-sensitive logistic (cslogit)\nmodel as proposed by [252] and [14]. Cslogit minimizes an instance-dependent\ncost-sensitive objective function corresponding to the real business objective\nof minimizing costs ",
  "21": "ession is the likelihood that is maximized or the cross-entropy loss that is\nminimized. For a single sample with true label yi ∈{0, 1} and a probability\nscore si = P(Y = 1), the cross-entropy loss is presented by Equation (5.2):\nLlog(yi, si) = −(yi log(si) + (1 −yi) log(1 −si)).\n(5.2)\nNote that this equation does not take into account any costs. Because\nthis objective function assigns equal weights to each misclassification, it does\n82\n 5.2. Related work\nnot necessarily correspond to the underlying business problem where costs\nare to be minimized. The reason for this is twofold: misclassification costs\nare different per class and per instance. The real business objective is to\nminimize the average expected total cost of the binary classifier.\nWe build upon the instance-dependent cost-sensitive logistic (cslogit)\nmodel as proposed by [252] and [14]. Cslogit minimizes an instance-dependent\ncost-sensitive objective function corresponding to the real business objective\nof minimizing costs in domains such as customer churn prediction, credit\nscoring, and direct marketing [270], [271]. Dependent on this business ob-\njective, also other cost matrices can be considered.\nFor example, Höpp-\nner, Baesens, Verbeke, et al. [14] propose the cost matrix Ci(0 ∣0) = 0,\nCi(0 ∣1) = Ai, Ci(1 ∣0) = cf, and Ci(1 ∣1) = cf for the detection of transfer\nfraud where cf is a fixed administrative fee. Alternatively, Bahnsen, Aouada,\nand Ottersten [251] propose the cost matrix Ci(0 ∣0) = 0, Ci(0 ∣1) = Lgd,\nCi(1 ∣0) = ri + Ca\nF P , and Ci(1 ∣1) = 0 for credit scoring where Lgd is\nthe loss given default, ri is the loss in profit by rejecting what could have\nbeen a good customer, and Ca\nF P is the cost related to the assumption that\nthe financial institution will not keep the amount of the declined applicant\nunused. However, the reason for this work is to address the need for robust-\nness and to propose a solution to solve this potential issue in a generic way,\nregardless of its application. Therefore, to present an application-agnostic\nmethodology and preferring the most simple cost matrix, this work utilizes\na symmetric cost matrix.\nEquation (5.3) shows the average expected cost (AEC), the cost-sensitive\nobjective function that is used by cslogit, given a symmetric cost matrix, as\nshown in Table 5.1:\nAEC(s(D)) = 1\nN E[Cost(s(D)) ∣X]\n= 1\nN\nN\n∑\ni=1\n(yi[siCi(1 ∣1) + (1 −si)Ci(0 ∣1)]\n+ (1 −yi)[siCi(1 ∣0) + (1 −si)Ci(0 ∣0)])\n= 1\nN\nN\n∑\ni=1\n(Ai(yi(1 −si) + (1 −yi)si)).\n(5.3)\nIn Equation (5.3), each observation i is a pair of d features xi = (xi1, ..., xid)\nand a binary response label yi ∈{0, 1}.\nAcross multiple models, the total cost as a metric is not unambiguously\ninterpretable, as datasets with high instance-dependent costs might have a\nhigher total misclassification cost but still have a better relative score. Pro-\nceeding with the idea of normalizing the total classification costs of a model\n83\n Chapter 5: Robust instance-dependent cost-sensitive classification\nTable 5.1: Symmetric cost matrix for cslogit\nActual 0\nActual 1\nPredicted as 0\nCi(0 ∣0) = 0\nCi(0 ∣1) = Ai\nPredicted as 1\nCi(1 ∣0) = Ai\nCi(1 ∣1) = 0\npresented in [272], [251] introduce a more interpretable metric: Savings.\nThis metric represents the relative improvement of the cost of a newly pro-\nposed model, Cost(s(D)), compared to the cost of using an empty model\nthat assigns all instances to a single class, Costempty(D). Costempty(D) is\ncalculated by taking the minimum of the costs incurred when classifying all\ninstances as either belonging to the negative or positive class:\nCostempty(D) = min {Cost (s0(D)) , Cost (s1(D))} .\n(5.4)\nUsing the Costempty(D) of an empty model as a factor to normalize total\ncosts, Savings of the model s(D) are calculated by Equation (5.5):\nSavings(s(D)) = 1 −\nCost(s(D))\nCostempty(D).\n(5.5)\n5.3\nSensitivity analysis\nData can contain outliers in terms of misclassification costs due to various\nreasons, such as missing data, invalid observations, or typos. By incorpo-\nrating instance-dependent costs in the learning algorithm, outliers in these\nmisclassification costs could potentially have a large impact on instance-\ndependent cost-sensitive learning methodologies such as cslogit. Therefore,\nwe test the sensitivity of cslogit to these outliers and examine to what extent\nthis is a shortcoming of this method.\n5.3.1\nSimulation setup\nWe analyze the sensitivity to outlying costs through a series of simulations\non synthetic data. The different synthetic datasets all share the following\nproperties. Each observation is visualized by a dot, with the size of the dot\ncorresponding to its misclassification cost. The positive class is presented\nin red and the negative class in blue. Each observation has, other than its\nmisclassification cost and label, two features: X1 and X2. X1 is the feature\nfor the misclassification cost A. For the positive class, this cost is positively\nrelated to X1. Cases of the negative class have a negative relation between\nX1 and their cost. The underlying function is given by Equation (5.6):\nAi = { 20 + 2x1i\nfor the positive class,\n20 −2x1i\nfor the negative class.\n(5.6)\n84\n 5.3. Sensitivity analysis\nPanel (b) and (c) in Figure 5.1 visualize this equation.\n(a) Class\ndistribution\n(b) Generated costs:\nnegative class\n(c) Generated costs:\npositive class\nFigure 5.1: The setup for synthetic data. Panel (a) displays the distribution\nof the negative and positive class, dependent on X2.\nPanels (b) and (c)\nrepresent the misclassification costs of observations from the negative and\npositive classes as a linear function of X1, generated by Equation (5.6). The\nthree panels all show a sample of size 50 per class.\nX2 is the feature that determines the two distributions of classes 0 and\n1. The two class distributions are a 2-dimensional Gaussian, sharing the\nsame standard deviations. Observations from the negative class are sam-\npled from N(µ0, σ2\n0, ν0, τ 2\n0 , ρ) and observations from the positive class from\nN(µ1, σ2\n1, ν1, τ 2\n1 , ρ).\nµ0 and µ1 are both equal to 0, while ν0 = −5 and ν1 = 5. The variances\nσ2\n0, τ 2\n0 , σ2\n1 and τ 2\n1 are equal to 4. As there is no correlation between the two\ndimensions X1 and X2, ρ is equal to 0. The cases of the positive class have\na higher X2 value than the cases of the negative class. Panel (a) in Figure\n5.1 displays these class distributions by which data are generated.\nTo generate outliers, both in the synthetic setup (Sections 5.3 and 5.5.1)\nand in the sensitivity analysis on real data (Section 5.5.2), the multivariate\ndistribution of X remains unchanged since we only focus on outliers in the\nobserved costs of instances. We use the Tukey-Huber contamination model\nto generate the cost distribution with outliers where the Dirac function is\napplied to generate costs of any size [254].\nGiven these settings for instance-dependent costs and class distribution,\nobservations of the negative class with a high associated cost are expected to\nbe located in the third quadrant and observations of the positive class with\na high associated cost in the first quadrant. The symmetric cost matrix used\nfor the examples on synthetic data is presented in Table 5.1 as introduced\nin Section 5.2.2.\n85\n Chapter 5: Robust instance-dependent cost-sensitive classification\n5.3.2\nResults\nWithin each setting, two classifiers are compared: logit and cslogit. They are\nboth linear classifiers and propose a distinctly different decision boundary\nbased on the training data. Since the data are only two-dimensional, these\ndecision boundaries can be visually represented by lines.\nThe logit and\ncslogit models’ proposed boundaries are respectively colored in red and blue.\nThe normal behavior of both models in the default settings of examples on\nsynthetic data is visualized in Panel a of Figure 5.2. This figure motivates the\nneed for a robust version of cslogit. Three examples of synthetic data where\nlogit and cslogit are tested are shown. Panel a shows the normal behavior of\ncslogit and logit in the default case. Panel b displays the case where a large\noutlier is added. The blue decision boundary of cslogit shifts, while the red\ndecision boundary of logit remains stable. Note that the effect of the outlier\ncan be subtle, i.e. it pulls on the decision boundary, resulting in a slight\nrotation, without actually being classified correctly. Panel c displays the\ncase where random noise is added to the misclassification costs. The decision\nboundary of cslogit shifts even further, resulting in an almost perpendicular\nboundary in comparison with Panel a. We further elaborate on the exact\nsetting of the examples on synthetic data in Section 5.5.1.\n5.4\nRobust IDCS\nTo overcome the sensitivity of instance-dependent cost-sensitive classifiers to\noutlying costs, we introduce a three-step framework to make IDCS methods\nrobust by detecting outliers and adapting their cost matrix.\nHence, the\nfinal model will be trained using a less volatile and more rigid set of costs.\nThe resulting robust classification model will also yield automatic outlier\ndetection. The concrete implementation of this framework is represented by\nAlgorithm 2.\nTo estimate the misclassification costs of observations in a robust manner\nin step 1, a regression with Huber loss is applied. Concretely, we estimate\nthe cost Ai of observation i as a function of its features xi and label yi with a\nlinear regression with a Huber loss function: ˆAi = f(xi, yi). A formalization\nof robustness in statistics started with the work of [273]. Interestingly, his\nground-breaking results and well-known loss function are still widely used\ntoday in the field of statistics and machine learning. The Huber loss function\nis defined by Equation (5.7). This results in a regression that is less sensitive\nto outliers than traditional regression methods, which often use a squared\nerror loss.\n86\n 5.4. Robust IDCS\nAlgorithm 2: Robust IDCS\nInput\n: D = {(xi, Ai, yi) ∶i = 1, . . . , N} where xi is a feature\nvector, Ai is the associated misclassi",
  "22": "cation model will also yield automatic outlier\ndetection. The concrete implementation of this framework is represented by\nAlgorithm 2.\nTo estimate the misclassification costs of observations in a robust manner\nin step 1, a regression with Huber loss is applied. Concretely, we estimate\nthe cost Ai of observation i as a function of its features xi and label yi with a\nlinear regression with a Huber loss function: ˆAi = f(xi, yi). A formalization\nof robustness in statistics started with the work of [273]. Interestingly, his\nground-breaking results and well-known loss function are still widely used\ntoday in the field of statistics and machine learning. The Huber loss function\nis defined by Equation (5.7). This results in a regression that is less sensitive\nto outliers than traditional regression methods, which often use a squared\nerror loss.\n86\n 5.4. Robust IDCS\nAlgorithm 2: Robust IDCS\nInput\n: D = {(xi, Ai, yi) ∶i = 1, . . . , N} where xi is a feature\nvector, Ai is the associated misclassification cost and\nyi ∈{0, 1} is the response label of an observation i.\nOutput : Robust IDCS predictions ˆy of label y\nStep 1: Detect outliers.\nTrain a linear regression model with Huber loss so that:\nˆA = f(X, Y )\nInitialize set Soutlier ∶= ∅\nfor each observation i do\nif absolute value of the standardized residuals > 2.5 then\nadd observation (xi, Ai, yi) to set Soutlier\nremove observation (xi, Ai, yi) from D\nStep 2: Impute instance-dependent misclassification cost.\nfor each observation i ∈Soutlier do\nreplace Ai with ˆAi\nD′ ∶= D ∪Soutlier\nStep 3: Apply the IDCS method.\nApply cslogit to the new set D′\nReturn : ˆy\n87\n Chapter 5: Robust instance-dependent cost-sensitive classification\n(a) Example on synthetic data:\nOptimal behavior of logit and cslogit\n(b) Example on synthetic data:\nInfluence of outliers on cslogit\n(c) Example on synthetic data:\nInfluence of noise on cslogit\nFigure 5.2: The instability of cslogit’s decision boundary.\nLδ(a) = {\n1\n2a2\nfor ∣a∣≤δ,\nδ (∣a∣−1\n2δ)\notherwise.\n(5.7)\nNext, to detect outliers, we compare the absolute value of the standard-\nized residuals with a cutoff value of a normal distribution [274]. If this value\nexceeds 2.5, we consider it an outlier and add it to the initially empty set\nSoutlier.\nThe observed cost Ai of observation i is operationally defined as an outlier\nif, for an estimator ˆA = f(X, Y ), the absolute value of the standardized\nresidual ϵi is larger than 2.5.\nFigure 5.3 further clarifies the concept of a conditional outlier on the\nsetting of synthetic data where noise is added to the observed costs, as is\ndisplayed in Figure 5.2c. In this figure, the black line displays the estimated\n88\n 5.4. Robust IDCS\nFigure 5.3: Cost estimation in function of X1.\ncosts, as predicted by the linear regression model with Huber loss (Algorithm\n2, line 2). The red dots represent the costs of observations in function of X1.\nConsider the two observations A and B. To check whether their observed\ncosts are outliers, we look at the standardized residuals, represented by the\ngrey vertical lines for those two observations. Both observations A and B\nhave an observed cost of 50. The standardized residual of A exceeds 2.5,\nwhereas for B it is smaller than 2.5. Consequently, although both observa-\ntions have the same cost, the cost of A is considered an outlier, whereas the\ncost of B is not.\nBy doing so, the costs A that are outliers, conditional on their features\nX and label Y , are detected. In step 2, the observed outlying costs A of\nall observations in Soutlier are imputed with their estimated counterpart ˆA.\nThis results in a robust cost matrix (Table 5.2).\nTable 5.2: Symmetric cost matrix for r-cslogit\ny = 0\ny = 1\nˆy = 0\nCi(0 ∣0) = 0\nCi(0 ∣1) = {\nˆAi = f(xi, yi)\nif outlier,\nAi\notherwise\nˆy = 1\nCi(1 ∣0) = {\nˆAi = f(xi, yi)\nif outlier,\nAi\notherwise\nCi(1 ∣1) = 0\nEquation (5.8) retakes the cost-sensitive objective function AEC, given\nby Equation (5.3), but adapts it to the new robust cost-matrix given by\nTable 5.2. Indicator function 1o takes value 1 if the cost Ai of observation i\nis classified as an outlier.\n89\n Chapter 5: Robust instance-dependent cost-sensitive classification\nAEC(s(D)) = 1\nN E[Cost(s(D)) ∣X]\n= 1\nN\nN\n∑\ni=1\n[1o( ˆAi (yi (1 −si) + (1 −yi) si) )\n+ (1 −1o)(Ai (yi (1 −si) + (1 −yi) si) )]\n(5.8)\n5.5\nResults\nThis section discusses the performance of logit, cslogit, and the novel r-cslogit\non synthetic data and tests their sensitivity on real data with additional\noutliers. In the reported experiments, symmetric cost matrices are taken\ninto account. However, the use of alternative cost matrices as presented in\nSection 5.2.2 yields similar results concerning robustness.\nThe performance of binary classification algorithms is typically measured\nby labeling one class as positive and the other class as negative and con-\nstructing a confusion matrix. Positive classes are typically used to describe\nthe minority class, and negative classes are used to describe the majority\nclass. From the confusion matrix, we count the following numbers. True\nNegatives (TN) are the number of correctly classified negative cases. False\nPositives (FP) are the number of negative cases incorrectly classified as pos-\nitive.\nFalse Negatives (FN) are the number of positive cases incorrectly\nclassified as negative. True Positives (TP) are the number of correctly clas-\nsified positive cases. With these numbers, we define Sensitivity or Recall,\nSpecificity, and Precision by Equations 5.9, 5.10, and 5.11:\nSensitivity = Recall =\nTP\nTP + FN\n(5.9)\nSpecificity =\nTN\nTN + FP\n(5.10)\nPrecision =\nTP\nTP + FP\n(5.11)\nEquation 5.12 defines F1-measure, which is the Harmonic mean of pre-\ncision and recall.\nF1 = 2 ⋅precision ⋅recall\nprecision + recall\n(5.12)\nThe area under the receiver operating curve (AUC) of a classifier can be\ninterpreted as a measure of the probability that a randomly chosen minority\n90\n 5.5. Results\ncase is predicted to have a higher score than a randomly chosen majority case.\nTherefore, a higher AUC indicates better classification performance [14].\nClass distributions and misclassification costs are not taken into account in\ncalculating the AUC.\nEquation 5.13 defines the Brier score, where si is the predicted probability\nand yi is the observed outcome. This metric measures the mean squared\ndifference between the predicted probability and the actual outcome and is\nused to assess whether the model’s predictions are calibrated probabilities.\nA lower score is better.\nBrier = 1\nN\nN\n∑\ni=1\n(si −yi)2\n(5.13)\n5.5.1\nSynthetic data\nIn this subsection, we reuse the examples on synthetic data introduced in\nSection 5.3.1 to demonstrate how the possible shortcomings of cslogit can\nbe countered by deploying the more robust r-cslogit. Figure 5.4 displays the\ndecision boundaries of logit, cslogit, and r-cslogit in red, blue, and green,\nrespectively.\nSynthetic data: Three settings\nThe basic setting of the examples on synthetic data in Panel a is the same\nas explained in Subsection 5.3.1. In Panel b, an additional outlier of the\npositive class is added in the third quadrant with a cost equal to 400. The\nrobust method first estimates its cost with a linear Huber regression to be\n13.75 and flags it as an outlier. Next, the cost for this instance of 400 is\nchanged to its estimated cost of 13.75. In Panel c, we add noise to the costs.\nHence, the misclassification costs are generated by Equation (5.14), where\nthe noise ϵi is sampled from a lognormal distribution with parameters µ =\n2 and σ = 1.5.\nAi = { 20 + 2x1i + ϵi\nfor the positive class,\n20 −2x1i + ϵi\nfor the negative class.\n(5.14)\nDescription of results\nFigure 5.4 visualizes the decision boundaries of the three models. In Panel\na, the decision boundaries of cslogit and r-cslogit overlap as regression with\nHuber Loss can perfectly predict the underlying function of associated mis-\nclassification costs as a function of X1 in the absence of noise or outliers.\n91\n Chapter 5: Robust instance-dependent cost-sensitive classification\n(a) Example on synthetic data:\nDefault case\n(b) Example on synthetic data:\nOne outlier\n(c) Example on synthetic data:\nAdditional noise\nFigure 5.4: Superiority of r-cslogit.\nPanel b displays the case where one outlier is added. The decision bound-\nary of the logit model is not affected by the size of misclassification costs.\nHence, it is not influenced by the outlier and remains unchanged, demon-\nstrating normal behavior as defined before. The blue decision boundary of\nthe cslogit model is strongly influenced by outliers. The objective function\ntakes into account the full misclassification costs of the observations in the\ntraining set, including the excessive outliers. As a consequence, the behav-\nior of the cslogit model has been completely disrupted. This is strongly in\nconflict with its normal behavior, as the decision boundary is almost tilted\nby a quarter turn.\nThis tilted decision boundary results in poor predic-\ntive classification power, making the cslogit model to be of inferior quality.\nThe green decision boundary of r-cslogit remains largely unchanged, as it is\nrobust against the single added outlier.\nPerformance metrics are summarized in Table 5.3. We consider the cost-\n92\n 5.5. Results\nTable 5.3: Results on synthetic data (i) in the default setting, (ii) with an\noutlier, and (iii) with additional noise added to the amounts. The sample size\nis set to 300, i.e., 150 per class. The data is generated according to the setting\nas explained in Section 5.3.1. We apply a 2×5-fold cross-validation procedure\nwith a train/test split ratio of 0.8/0.2. The best-performing methods are\nindicated in bold. A full analysis on synthetic data with different settings\nfor class imbalance and outlier size can be found in Appendix C.1. We report\nthe average together with the standard deviation over these 10 runs.\nSetting\nMethod\nSavings\nF1\nAUC\nSensitivity\nSpecificity\nBrier\ndefault setting\nlogit\n0.68 ± 0.08\n0.84±0.04\n0.85±0.04\n0.83±0.04\n0.88±0.05\n0.14±0.04\ncslogit\n0.80±0.05\n0.77 ± 0.05\n0.77 ± ",
  "23": "e green decision boundary of r-cslogit remains largely unchanged, as it is\nrobust against the single added outlier.\nPerformance metrics are summarized in Table 5.3. We consider the cost-\n92\n 5.5. Results\nTable 5.3: Results on synthetic data (i) in the default setting, (ii) with an\noutlier, and (iii) with additional noise added to the amounts. The sample size\nis set to 300, i.e., 150 per class. The data is generated according to the setting\nas explained in Section 5.3.1. We apply a 2×5-fold cross-validation procedure\nwith a train/test split ratio of 0.8/0.2. The best-performing methods are\nindicated in bold. A full analysis on synthetic data with different settings\nfor class imbalance and outlier size can be found in Appendix C.1. We report\nthe average together with the standard deviation over these 10 runs.\nSetting\nMethod\nSavings\nF1\nAUC\nSensitivity\nSpecificity\nBrier\ndefault setting\nlogit\n0.68 ± 0.08\n0.84±0.04\n0.85±0.04\n0.83±0.04\n0.88±0.05\n0.14±0.04\ncslogit\n0.80±0.05\n0.77 ± 0.05\n0.77 ± 0.06\n0.78 ± 0.03\n0.77 ± 0.11\n0.23 ± 0.06\nr-cslogit\n0.80±0.05\n0.77 ± 0.05\n0.77 ± 0.06\n0.78 ± 0.03\n0.77 ± 0.11\n0.23 ± 0.06\nwith outlier\nlogit\n0.68 ± 0.08\n0.84±0.04\n0.86±0.04\n0.83±0.04\n0.88±0.05\n0.14±0.04\ncslogit\n0.65 ± 0.15\n0.82 ± 0.05\n0.83 ± 0.05\n0.81 ± 0.05\n0.8219 ± 0.07\n0.17 ± 0.05\nr-cslogit\n0.80±0.05\n0.77 ± 0.05\n0.77 ± 0.06\n0.78 ± 0.03\n0.77 ± 0.11\n0.23 ± 0.06\nwith noise\nlogit\n0.68 ± 0.08\n0.84±0.04\n0.86±0.04\n0.83±0.04\n0.88±0.05\n0.14±0.04\ncslogit\n0.57 ± 0.09\n0.76 ± 0.06\n0.77 ± 0.05\n0.77 ± 0.06\n0.83 ± 0.04\n0.17 ± 0.05\nr-cslogit\n0.78±0.06\n0.82 ± 0.03\n0.83 ± 0.04\n0.81 ± 0.05\n0.86 ± 0.05\n0.15 ± 0.03\nsensitive metric Savings introduced in Section 5.2.2 and cost-independent\nmetrics Sensitivity, Specificity, F1, AUC, and Brier score.\nr-cslogit outperforms logit and cslogit in terms of Savings when we add\nan outlier and noise. Moreover, the performance in terms of Savings remains\nunchanged after adding an outlier. In the default case of setting one, r-cslogit\nand cslogit are equivalent, as they make the exact same predictions. When\nconsidering cost-insensitive metrics, logit performs best. A full analysis on\nsynthetic data where we experiment with different settings of class imbalance\nand outlier size can be found in Appendix C.1.\n5.5.2\nSensitivity analysis on real data\nIn this subsection, we analyze the sensitivity of the three methods in an\nexperiment with real data where we add an additional outlier, gradually\nincreasing in size. To add outliers, we randomly select an observation and\nchange its class label and instance-dependent misclassification cost.\nThis setup is similar to the second setup with synthetic data as pre-\nsented in the previous subsection. The performance is measured by the cost-\nsensitive metric Savings as described before as well as the cost-independent\nmetrics Sensitivity, Specificity, F1, AUC, and Brier score. The measure-\nment of performance makes use of five-fold cross-validation with a stratified\nsplit on class distribution that is repeated twice with a different random\ninitialization.\n93\n Chapter 5: Robust instance-dependent cost-sensitive classification\nDescription of the dataset\nThe dataset on which the three methods are tested is the Kaggle Credit Card\nFraud Detection dataset [275].\nThe dataset dates from September 2013\nand contains transactions made by European credit cardholders. A total\nof 492 out of 284,807 transactions are fraudulent, resulting in a high class\nimbalance. The numerical input features V 1, V 2, . . . , V 28 are the results of\na PCA transformation to anonymize the dataset. Time and Amount have\nnot been transformed. The feature Time is not taken into consideration in\nthis experiment and is therefore dropped in the preprocessing phase. The\nfeature Amount is the transaction amount, which is of high importance in\ncost-sensitive instance-dependent learning and translates into our setting as\nthe instance-dependent misclassification cost. The feature Class ∈{0, 1}\nindicates whether a transaction is fraudulent or not.\nResults\nTable 5.4 contains the results of a 2 × 5-fold cross-validation procedure for\nthe Kaggle Credit Card Fraud Detection dataset. We measure each clas-\nsifier’s performance averaged over the ten (2 × 5) test sets with the met-\nrics Savings, F1, AUC, Sensitivity, Specificity, and Brier score, where\ninstance-independent thresholds are applied.\nFigure 5.5: Sensitivity analysis on real data.\nIn terms of Savings, logit is always outperformed by cslogit and r-cslogit.\nWhen adding an outlier, r-cslogit outperforms cslogit. Note that the perfor-\nmance of r-cslogit remains stable for all considered metrics when increasing\nthe size of the outlier. In terms of cost-insensitive metrics AUC, Specificity,\n94\n 5.5. Results\nTable 5.4: Sensitivity analysis on real data resulting from a two times five-\nfold cross validation procedure on the Kaggle Credit Card Fraud Detection\ndataset. The size of the outlier is gradually increased. Each metric is based\non 10 (2 × 5) out-of-sample performance estimates over the 10 test sets. We\nreport the average together with the standard deviation over these 10 runs.\nOutlier size\nMethod\nSavings\nF1\nAUC\nSensitivity\nSpecificity\nBrier\n0\nlogit\n0.13 ± 0.14\n0.72 ± 0.01\n0.97±0.01\n0.62 ± 0.04\n0.99±0.00\n0.00±0.00\ncslogit\n0.61±0.06\n0.81 ± 0.01\n0.93 ± 0.02\n0.78 ± 0.04\n0.99 ± 0.00\n0.00 ± 0.00\nr-cslogit\n0.61±0.06\n0.81±0.02\n0.93 ± 0.02\n0.78±0.04\n0.99 ± 0.00\n0.00 ± 0.00\n10K\nlogit\n0.13 ± 0.14\n0.72 ± 0.01\n0.97±0.01\n0.62 ± 0.04\n0.99±0.00\n0.00±0.00\ncslogit\n0.61±0.06\n0.81±0.01\n0.93 ± 0.02\n0.78±0.04\n0.99 ± 0.00\n0.00 ± 0.00\nr-cslogit\n0.59 ± 0.07\n0.81 ± 0.01\n0.93 ± 0.02\n0.78 ± 0.04\n0.99 ± 0.00\n0.00 ± 0.00\n100K\nlogit\n0.13 ± 0.14\n0.72 ± 0.01\n0.97±0.01\n0.62 ± 0.04\n0.99±0.00\n0.00±0.00\ncslogit\n0.61 ± 0.06\n0.81 ± 0.01\n0.93 ± 0.02\n0.78 ± 0.04\n0.99 ± 0.00\n0.00 ± 0.00\nr-cslogit\n0.61±0.06\n0.81 ± 0.01\n0.93 ± 0.02\n0.78±0.04\n0.99 ± 0.00\n0.00 ± 0.00\n1M\nlogit\n0.13 ± 0.14\n0.72 ± 0.01\n0.97±0.01\n0.62 ± 0.04\n0.99±0.00\n0.00±0.00\ncslogit\n0.54 ± 0.06\n0.72 ± 0.03\n0.89 ± 0.04\n0.78 ± 0.04\n0.99 ± 0.00\n0.00 ± 0.00\nr-cslogit\n0.60±0.60\n0.81±0.01\n0.93 ± 0.02\n0.78±0.03\n0.99 ± 0.00\n0.00 ± 0.00\n10M\nlogit\n0.13 ± 0.14\n0.72 ± 0.01\n0.97±0.01\n0.62 ± 0.04\n0.99±0.00\n0.00±0.00\ncslogit\n0.41 ± 0.13\n0.66 ± 0.04\n0.87 ± 0.05\n0.76 ± 0.06\n0.99 ± 0.00\n0.00 ± 0.00\nr-cslogit\n0.61±0.06\n0.81±0.01\n0.93 ± 0.02\n0.78±0.04\n0.99 ± 0.00\n0.00 ± 0.00\n100M\nlogit\n0.13 ± 0.14\n0.72 ± 0.01\n0.97±0.01\n0.62 ± 0.04\n0.99±0.00\n0.00±0.00\ncslogit\n0.32 ± 0.24\n0.62 ± 0.04\n0.85 ± 0.05\n0.73 ± 0.09\n0.99 ± 0.00\n0.00 ± 0.00\nr-cslogit\n0.60±0.06\n0.81±0.01\n0.93 ± 0.02\n0.78±0.04\n0.99 ± 0.00\n0.00 ± 0.00\n1B\nlogit\n0.13 ± 0.14\n0.72 ± 0.01\n0.97±0.01\n0.62 ± 0.04\n0.99±0.00\n0.00±0.00\ncslogit\n0.29 ± 0.27\n0.63 ± 0.05\n0.87 ± 0.04\n0.76 ± 0.05\n0.99 ± 0.00\n0.00 ± 0.00\nr-cslogit\n0.61±0.06\n0.81±0.01\n0.93 ± 0.02\n0.78±0.04\n0.99 ± 0.00\n0.00 ± 0.00\nand Brier score, logit performs best. In terms of F1 and Sensitivity, logit\nis outperformed by either cslogit or r-cslogit. This could be due to the effect\nof class imbalance and is in line with previous findings of Höppner, Baesens,\nVerbeke, et al. [14]. The results in terms of Savings are visualized in Fig-\nure 5.5. Since the logit model is not cost-sensitive, its performance remains\nconstant after adding an outlier.\nThe performance of cslogit is strongly\ndisrupted after the cost of the outlier is set to 1 M or larger. This corre-\nsponds with the shift of the two-dimensional linear decision boundary, as\nshown by the findings of the examples on synthetic data. Even though the\ndataset contains over 280,000 instances, a single outlier, albeit a large out-\nlier, can unhinge the cslogit method. When increasing the misclassification\ncost of a single outlier, the performance of r-cslogit remains stable. It is\ncertainly more robust to this additional noise than its non-robust counter-\npart, as the individual outlier is detected and its cost is imputed with an\nestimated, expected cost. The shaded areas in Figure 5.5 represent the vari-\nability of performance over different folds in cross-validation. In contrast to\nthe variability of cslogit, which increases drastically, the variability in the\nperformance of r-cslogit remains stable.\n95\n Chapter 5: Robust instance-dependent cost-sensitive classification\n5.6\nConclusion\nInstance-dependent cost-sensitive (IDCS) learning methods take into ac-\ncount variable misclassification costs across instances in the training data\nin learning a classification model.\nThis allows for optimizing the perfor-\nmance of the resulting classification model in terms of the misclassification\ncosts rather than the classification accuracy.\nIn this article, we present the results of a series of experiments on syn-\nthetic data to demonstrate the sensitivity of IDCS methods to outliers and\nnoise in the data. We show that the resulting classification model may be\nhighly sensitive to outlying instance-dependent costs, in learning an instance-\ndependent cost-sensitive classification model. Consequently, using existing\ncost-sensitive models in the presence of noise or outliers can result in large\nmisclassification costs.\nTo address this potential vulnerability, we propose a generic, IDCS-\nmethod-independent, three-step framework to develop robust IDCS methods\nwith respect to the effects of random variability and noise. In the first step,\ninstances with outlying misclassification costs are detected. In the second\nstep, outlying costs are corrected in a data-driven way. In the third step,\nan IDCS learning method is applied using the adjusted instance-dependent\ncost information.\nThis generic framework is subsequently applied in combination with\ncslogit, which is a logistic regression-based IDCS method, to obtain its ro-\nbust version named r-cslogit. The robustness of this approach is introduced\nin the first two steps of the generic framework by making use of robust es-\ntimators to detect and impute outlying costs of individual instances. The\nnewly proposed r-cslogit method is tested on synthetic and semi-synthetic\ndata. The results show that t",
  "24": "noise or outliers can result in large\nmisclassification costs.\nTo address this potential vulnerability, we propose a generic, IDCS-\nmethod-independent, three-step framework to develop robust IDCS methods\nwith respect to the effects of random variability and noise. In the first step,\ninstances with outlying misclassification costs are detected. In the second\nstep, outlying costs are corrected in a data-driven way. In the third step,\nan IDCS learning method is applied using the adjusted instance-dependent\ncost information.\nThis generic framework is subsequently applied in combination with\ncslogit, which is a logistic regression-based IDCS method, to obtain its ro-\nbust version named r-cslogit. The robustness of this approach is introduced\nin the first two steps of the generic framework by making use of robust es-\ntimators to detect and impute outlying costs of individual instances. The\nnewly proposed r-cslogit method is tested on synthetic and semi-synthetic\ndata. The results show that the proposed method is superior in terms of\ncost savings when compared to its non-robust counterpart for variable levels\nof noise and outliers.\n96\n 6\nDecision-centric fairness:\nEvaluation and optimization for\nresource allocation problems\nData-driven decision support tools play an increasingly central role in decision-\nmaking across various domains. In this work, we focus on binary classifica-\ntion models for predicting positive-outcome scores and deciding on resource\nallocation, e.g., credit scores for granting loans or churn propensity scores\nfor targeting customers with a retention campaign. Such models may ex-\nhibit discriminatory behavior toward specific demographic groups through\ntheir predicted scores, potentially leading to unfair resource allocation. We\nfocus on demographic parity as a fairness metric to compare the proportions\nof instances that are selected based on their positive outcome scores across\ngroups. In this work, we propose a decision-centric fairness methodology\nthat induces fairness only within the decision-making region—the range of\nrelevant decision thresholds on the score that may be used to decide on re-\nsource allocation—as an alternative to a global fairness approach that seeks\nto enforce parity across the entire score distribution.\nBy restricting the\ninduction of fairness to the decision-making region, the proposed decision-\ncentric approach avoids imposing overly restrictive constraints on the model,\nwhich may unnecessarily degrade the quality of the predicted scores. We\nempirically compare our approach to a global fairness approach on multiple\n(semi-synthetic) datasets to identify scenarios in which focusing on fairness\nwhere it truly matters, i.e., decision-centric fairness, proves beneficial.\n97\n Chapter 6: Decision-centric fairness\n6.1\nIntroduction\nIn an increasingly data-driven world, algorithms and machine learning mod-\nels play a crucial role in business decision-making. In this paper, we focus on\npredictive models that are used to optimize resource allocation, particularly\non binary classification models that predict positive-outcome scores for this\npurpose.\nSuch models are widely used across various domains, including\nmarketing, where retention incentives are offered based on churn propensity\nscores [276], [277]; credit risk management, where loans are granted based\non default risk assessments [278]; and fraud detection, where investigative\nresources are allocated based on predicted fraud risk [279]. These models can\nexhibit discriminatory behavior toward specific demographic groups through\ntheir predicted scores—that is, the predicted scores may follow different dis-\ntributions across demographic groups—potentially leading to unfair resource\nallocations. Such discriminatory behavior can originate from biases in the\nhistorical data that is used to train the models or from inherent differences\nbetween groups in their tendency to belong to the positive class, which—\nalthough statistically justified—may be considered unacceptable discrimina-\ntion when acted upon.\nFairness is central to the acceptability of algorithm-informed decisions,\nparticularly in domains where these decisions can significantly affect individ-\nuals’ access to resources or opportunities [280]. While much of the existing\nresearch has focused on preventing gender-based discrimination in pricing\nto comply with regulatory standards [281]–[283], the relevance of fairness\nextends well beyond pricing models to key business functions such as credit\nrisk assessment, targeted marketing, and fraud detection. Fairness is closely\ntied to principles embedded in non-discrimination laws, particularly in the\nEU and US, which emphasize equitable outcomes across demographic groups\nand provide a foundation for fairness criteria like demographic parity [284].\nSimilar to the pricing context, discrimination in credit risk management\n[285], [286] and fraud detection is legally prohibited (e.g., to prevent gender-\nbased discrimination and ethnic profiling). Additionally, in marketing, fair-\nness considerations are seen by some companies as crucial for building and\nmaintaining both a diverse customer base and a positive reputation.\nTraditional approaches to evaluating algorithmic fairness typically rely\non output-based metrics that assess disparities in average predictions or er-\nror rates between demographic groups [21]. These approaches are built on\nthe idea that protected attributes, such as gender or race, should not im-\npact a predicted score (e.g., a customer’s default risk score). Although such\nmetrics provide a general view, they often overlook more subtle nuances in\nalgorithmic behavior [287]. Recent work has expanded fairness analyses by\nincorporating higher-order moments of the output distribution, such as the\n98\n 6.1. Introduction\nvariance [288], and by comparing entire output distributions [289]. More\nspecifically, Han, Jiang, Jin, et al. [289] propose distribution-level variants\nof demographic parity, a fairness metric that compares the proportions of\npositive class predictions—the predictions on which we would potentially act\nin a resource allocation setting—across groups. While Han, Jiang, Jin, et al.\n[289] focus on model evaluation from a fairness perspective, Peeperkorn and\nDe Vos [290] show that these distribution-level fairness notions can be used to\ndevelop predictive models that are intrinsically more fair. Building on these\nworks, we propose a pragmatic decision-centric fairness approach to classifi-\ncation for resource allocation optimization. Specifically, rather than focusing\non inducing demographic parity across the entire output distribution (i.e.,\nensuring a proportionally equal number of positive outcomes at all possible\ndecision thresholds on the predicted scores), which we term a global fairness\napproach, we propose to concentrate on the \"decision-making region\" by in-\nducing parity only within the range of relevant thresholds used for resource\nallocation, as visualized in Figure 6.1. For example, in a customer reten-\ntion campaign, interventions are typically targeted at customers with a high\npredicted churn propensity score. Since resource allocation decisions affect\nonly instances within this high-risk-score region, fairness should be enforced\nwithin this region, across all relevant application-dependent thresholds. The\nproposed decision-centric approach aims to ensure fairness where it matters,\nwhile achieving better predictions compared to a global fairness approach.\nThe latter imposes overly strict constraints on the model to enforce fairness\nalso outside the decision-making region, where it will not affect real-world\ndecisions, potentially degrading the predictive quality of the generated scores\nmore than necessary.\nWhile achieving fairness in terms of demographic parity is straightfor-\nward using a group-dependent decision threshold post-hoc when decisions\nare made in batch, optimizing classification models to be inherently more\nfair without focusing on a single decision threshold is useful in common on-\nline decision-making settings (i.e., on a continuous basis). In such settings,\nresource constraints—such as marketing budgets, loan-granting capacity, or\ninvestigative resources—may change over time [291], causing the decision\nthreshold to vary within a certain decision-making region. In these cases,\nsetting group-dependent decision thresholds post hoc to achieve parity is\nnot possible, and retraining the model each time a new threshold (within\nthe decision-making region) is adopted due to changing resource constraints\ncan be (too) costly.\nOur main contributions are as follows: (i) we introduce and formalize\nthe concept of decision-centric fairness for resource allocation optimization;\n(ii) we propose a decision-centric fairness approach to optimize classification\nmodels that are used for resource allocation; (iii) we propose a decision-\n99\n Chapter 6: Decision-centric fairness\n0\n1\n2\nDensity\n0\n0.2\n0.4\n0.6\nτ\n0.8\n1\nPredicted score\n0.0\n0.2\n0.4\nDP\n(a) No fairness induction\n0\n1\n2\nDensity\n0\n0.2\n0.4\n0.6\nτ\n0.8\n1\nPredicted score\n0.0\n0.2\n0.4\nDP\n(b) Induced decision-centric fairness\nFigure 6.1: Densities of predicted scores ˜y for two demographic groups (with\nprotected attributes s = 0 and s = 1, in blue and red, respectively), along\nwith the corresponding demographic parity (DP) across all possible thresh-\nolds. By inducing decision-centric fairness, we aim to achieve demographic\nparity in the decision-making region, i.e., where ˜y > τ, to ensure a pro-\nportionally equal number of positive outcomes across the two groups at all\nthresholds within this region.\ncentric predictive performance metric for classification models; and (iv) we\nempirically compare our proposed decision-centric fairness methodology to\na global fairness approach on multiple (semi-synthetic) datasets to identify\nscenarios where, from a decision-centric evaluation perspective, focusing on\nfairness only where it truly matters, outperforms imposing fairne",
  "25": ".0\n0.2\n0.4\nDP\n(a) No fairness induction\n0\n1\n2\nDensity\n0\n0.2\n0.4\n0.6\nτ\n0.8\n1\nPredicted score\n0.0\n0.2\n0.4\nDP\n(b) Induced decision-centric fairness\nFigure 6.1: Densities of predicted scores ˜y for two demographic groups (with\nprotected attributes s = 0 and s = 1, in blue and red, respectively), along\nwith the corresponding demographic parity (DP) across all possible thresh-\nolds. By inducing decision-centric fairness, we aim to achieve demographic\nparity in the decision-making region, i.e., where ˜y > τ, to ensure a pro-\nportionally equal number of positive outcomes across the two groups at all\nthresholds within this region.\ncentric predictive performance metric for classification models; and (iv) we\nempirically compare our proposed decision-centric fairness methodology to\na global fairness approach on multiple (semi-synthetic) datasets to identify\nscenarios where, from a decision-centric evaluation perspective, focusing on\nfairness only where it truly matters, outperforms imposing fairness globally\nacross the output domain.\nThe remainder of this paper is structured as follows.\nIn Section 6.2,\nwe formalize the problem setting and discuss common fairness metrics and\nrelated work. Our proposed decision-centric fairness optimization methodol-\nogy is presented in Section 6.3. Section 6.4 outlines the experimental design,\nwith results presented and discussed in Section 6.5.\nFinally, Section 6.6\npresents our conclusions and outlines possible future research directions.\n6.2\nBackground and related work\nIn this section, we formalize the problem setting, provide a discussion of\ncommon fairness metrics, and motivate the use of demographic parity in\nbusiness decisions.\n100\n 6.2. Background and related work\n6.2.1\nClassification and resource allocation\nSuppose that we have a data set D = {(xi, yi, si)}N\ni=1 with N the num-\nber of instances with each a feature vector xi ∈Rk, a binary response\nvariable yi ∈{0, 1}, and a binary protected attribute si ∈{0, 1}.\nLet\nf ∶Rk+1 →[0, 1] be a model that maps instances to a score ˜y ∈[0, 1]\nwhich allows ranking instances from low to high score for belonging to the\npositive class. A predicted class ˆy ∈{0, 1} is obtained by setting a decision\nthreshold τ ∈[0, 1]. Instances with a predicted score below the threshold,\n˜y < τ, are assigned to the negative class, while those with a score above the\nthreshold are assigned to the positive class. In resource allocation applica-\ntions, resources are allocated to instances that are classified in the positive\nclass based on the predicted score and the adopted threshold. In practice, the\ndecision threshold is often determined post-training, as resource constraints\nthat affect the threshold may only be known at runtime (e.g., the available\ninvestigative capacity for fraud detection), and/or because the threshold is\nto be optimized at runtime (e.g., to maximize the profitability of a retention\ncampaign by taking into account the customer lifetime value of customers\nthat are classified as churners [277]).\n6.2.2\nFairness notions\nWe focus on situations in which algorithmic fairness issues may arise as a\nresult of using model scores in combination with a (variable) decision thresh-\nold τ to decide whether or not resources are allocated [292]. Specifically, we\nconsider settings where an action (e.g., offering a retention incentive) is taken\nfor instances with a model score ˜y ≥τ [293]. A standard approach to deter-\nmine whether a classification model conforms to a given notion of fairness is\nto evaluate its outcome distribution with respect to a number of protected\nattributes, such as gender or race [294]. However, the current literature on\nalgorithmic fairness offers a wide range of fairness notions and metrics [16],\n[21].\nDirect vs. indirect discrimination\nAlgorithmic discrimination comes\nin two types [16]: direct discrimination, where decisions are based on pro-\ntected attributes, and indirect discrimination, where decisions hurt protected\ngroups even without explicitly using protected attributes [295]. Direct dis-\ncrimination is typically avoided by excluding protected attributes or per-\nfectly correlated variables [296]. Therefore, this paper focuses on indirect\ndiscrimination. Even without using sensitive data directly, discriminatory\nbehavior can occur through proxy features [297]. For example, a churn model\nmight offer a retention incentive based on customer usage, but if that us-\n101\n Chapter 6: Decision-centric fairness\nage pattern is linked to a protected characteristic, the incentive may end up\nfavoring one group over another [298].\nIndividual vs. group fairness\nFairness metrics are typically categorized\ninto two primary types [294]: individual fairness and group fairness. Indi-\nvidual fairness is based on the principle that similarly situated individuals\nshould receive comparable outcomes [299], yet its practical application is\nhindered by the challenge of defining a robust, context-independent met-\nric for similarity [300]. Alternative formulations of individual fairness are\nprovided by causal and counterfactual fairness measures, which rely on ex-\nplicitly modeling hypothetical scenarios at the individual level [301].\nIn\ncontrast, group fairness evaluates statistical differences in outcomes across\nvarious demographic segments, facing the difficulty of selecting an appropri-\nate metric—since even popular measures like demographic parity and equal\nopportunity are often mutually incompatible [302], [303]. In this paper, we\nfocus exclusively on group fairness, as it is more prevalent in practice and\nits enforcement and measurement are comparatively more straightforward\n[304].\nGroup fairness metrics\nGroup fairness evaluation methods typically\ntranslate philosophical or political fairness ideals into statistical parity met-\nrics that apply to model outputs [305].\nFor instance, demographic par-\nity in classification compares the proportions of positive class predictions\n(ˆy = 1) across groups. Alternatively, by incorporating ground truth labels\n(y), one can assess disparities in error rates—equality of opportunity, for\nexample, compares true positive rates between groups [306]. These conven-\ntional output-based metrics focus on first-order statistics, evaluating average\noutcomes or error rates across groups [280]. Although these group fairness\nnotions are widely used, many alternative definitions exist [21]. However,\nthese output-based evaluations mainly rely on threshold-dependent first-\norder statistics and lack interpretability, potentially overlooking discrimina-\ntory behavior captured by higher-order moments or the broader prediction\ndistribution [307], [308].\nDistribution-level fairness metrics\nRecent work expands on traditional\ngroup fairness metric—which typically focus solely on first moments—by\nincorporating higher-order statistics, such as the variance [287], [288], or by\ncomparing entire output distributions [289]. More specifically, Han, Jiang,\nJin, et al. [289] argue that the standard demographic parity metric fails to\ndetect unfairness due to its threshold dependency, i.e., a slight variation in\nthe decision threshold may potentially re-introduce unfairness. Therefore,\nthey propose two distribution-level variants of demographic parity:\n102\n 6.2. Background and related work\n⋅Area Between Probability density function Curves (ABPC):\nABPC = ∫\n1\n0\n∣f0(x) −f1(x)∣dx,\n(6.1)\nwhere f0(x) and f1(x) are the probability density functions (PDFs) of\nthe predicted scores for two demographic groups, characterized by a\nprotected attribute s, with s = 0 and s = 1, respectively.\n⋅Area Between Cumulative density function Curves (ABCC):\nABCC = ∫\n1\n0\n∣F0(x) −F1(x)∣dx,\n(6.2)\nwhere F0(x) and F1(x) are the cumulative distribution functions (CDFs)\nof the predicted scores for two demographic groups, characterized by\na protected attribute s, with s = 0 and s = 1, respectively.\nWhile we agree that threshold-sensitivity is a challenging problem when im-\nplementing demographic parity in practice, we argue in Section 6.3 that the\nproposed ABPC and ABCC metrics are too rigid as they enforce demo-\ngraphic parity across the entire output distribution. This allows the use of\ndecision thresholds across the entire output distribution, including regions\nwhere, in practice, the decision threshold would never be set due to resource\nconstraints that limit the number of instances that can be acted upon (e.g.,\nthose that can be targeted with a retention campaign).\n6.2.3\nDemographic parity in resource allocation\nFairness in business operations is increasingly recognized as a fundamen-\ntal concern [309]. When allocating resources based on predictive models,\nbusinesses must ensure that their decision-making processes adhere to well-\ndefined fairness criteria [310]. However, as fairness in algorithmic decision-\nmaking is inherently multifaceted, choosing an appropriate fairness notion\nis non-trivial [19], [311].\nMany widely-used fairness metrics, such as de-\nmographic parity and equal opportunity, are mutually incompatible except\nunder highly constrained conditions [303]. This incompatibility is further\nexacerbated by deep-rooted philosophical disagreements on which notions\nare most appropriate in different contexts [312]. The choice between demo-\ngraphic parity and equal opportunity ultimately hinges on the evaluator’s\nunderlying assumptions and worldview [313], making it imperative to ground\nfairness considerations in regulatory and ethical frameworks that guide real-\nworld business decisions.\nIn the European Union, non-discrimination legislation and fairness re-\nquirements stemming from hard and soft law sources are highly influential\n103\n Chapter 6: Decision-centric fairness\nin this context.\nFor instance, the EU guidelines on discrimination in in-\nsurance explicitly require fairness in the access to and supply of goods and\nservices [284]. Similarly, ensuring fairness in insurance pricing and prevent-\ning discriminatory effects in customer engagement strategies are also legal\nobli",
  "26": "utually incompatible except\nunder highly constrained conditions [303]. This incompatibility is further\nexacerbated by deep-rooted philosophical disagreements on which notions\nare most appropriate in different contexts [312]. The choice between demo-\ngraphic parity and equal opportunity ultimately hinges on the evaluator’s\nunderlying assumptions and worldview [313], making it imperative to ground\nfairness considerations in regulatory and ethical frameworks that guide real-\nworld business decisions.\nIn the European Union, non-discrimination legislation and fairness re-\nquirements stemming from hard and soft law sources are highly influential\n103\n Chapter 6: Decision-centric fairness\nin this context.\nFor instance, the EU guidelines on discrimination in in-\nsurance explicitly require fairness in the access to and supply of goods and\nservices [284]. Similarly, ensuring fairness in insurance pricing and prevent-\ning discriminatory effects in customer engagement strategies are also legal\nobligations [282]. This broader interpretation of fairness aligns closely with\nthe principle of demographic parity, which seeks to equalize the likelihood of\nfavorable outcomes across demographic groups, independent of underlying\ndifferences in target variable distributions and potential historical injustices.\nBeyond EU law, the concept of demographic parity also intersects with US\nanti-discrimination regulations, particularly in the context of disparate im-\npact analysis [314]–[316]. Despite this apparent close connection between\nwhat is legally required and demographic parity, EU law does not foresee a\nspecific implementation where fairness must be enforced across all potential\nmodel outputs [317]. In fact, if individuals fall outside the actionable range\nof scores regardless of whether a model has been globally or locally con-\nstrained, the legal impact remains unchanged when fairness is implemented\nlocally. This suggests that constraints focused solely on the decision-making\nregion could potentially fulfill legal fairness or non-discrimination objectives\nequally well as methods pursuing global demographic parity.\nAt the same time, pursuing global demographic parity often comes at\nthe cost of predictive performance and calibration. Enforcing demographic\nparity usually involves imposing constraints on the model that can reduce\npredictive performance [318], [319]. Furthermore, demographic parity and\ncalibration are fundamentally incompatible unless the base rates for the\npositive class are identical between the demographic groups [306]. These\ntrade-offs are critical considerations for businesses that must balance fair-\nness objectives with operational effectiveness and the reliability of predicted\nscores.\n6.3\nDecision-centric demographic parity\nIn this section, we first propose decision-centric variants of the distribution-\nlevel demographic parity metrics, ABPC and ABCC. We then outline how\nthese metrics can be leveraged to induce decision-centric fairness in classifi-\ncation models used for resource allocation.\n6.3.1\nEvaluating decision-centric fairness\nThe two distribution-level variants of demographic parity proposed by Han,\nJiang, Jin, et al. [289], introduced in Section 6.2.2, are threshold-independent.\nAs discussed in Section 6.1, this is an important property, as predictive mod-\nels are often deployed in online decision-making settings, in which resource\n104\n 6.3. Decision-centric demographic parity\nconstraints may change over time [291], causing the decision threshold to\nvary. Additionally, the decision threshold may depend on instance-dependent\ncosts and benefits [277]. In a resource allocation context, however, we argue\nthat these distribution-level variants of demographic parity, which allow for\nthe use of decision thresholds across the entire output distribution, are overly\nstrict. Specifically, they penalize deviations from demographic parity even in\nregions with predictions that will never be acted upon in practice due to the\naforementioned resource constraints. To address this, we propose decision-\ncentric variants of the two distribution-level demographic parity metrics that\nonly penalize unfairness within a relevant decision-making region:\nABPCτ = ∫\n1\nτ\n∣f0(x) −f1(x)∣dx,\n(6.3)\nABCCτ = ∫\n1\nτ\n∣F0(x) −F1(x)∣dx.\n(6.4)\nThat is, we adapt the previously defined ABPC (Equation (6.1)) and ABCC\n(Equation (6.2)) metrics by restricting their integration domains to the\ndecision-making region [τ, 1], making ABPC and ABCC special cases of\nthe more general proposed ABPCτ and ABCCτ metrics.\n6.3.2\nInducing decision-centric fairness\nThe ABPC and ABCC distribution-level demographic parity metrics [289] al-\nlow us to evaluate algorithmic fairness across full score distributions. Build-\ning on this work, Peeperkorn and De Vos [290] operationalize a global fair-\nness approach, which allows inducing algorithmic fairness across full score\ndistributions, by training a neural classifier using a composite loss function\nthat combines standard binary cross-entropy loss LBCE with an unfairness\npenalty Lunfairness for deviations from demographic parity between the dis-\ntributions of predicted scores for two groups defined by a protected attribute\ns:\nL = (1 −λ) ⋅LBCE + λ ⋅Lunfairness,\n(6.5)\nwhere the hyperparameter λ controls the trade-off between predictive per-\nformance and fairness.\nThey propose to use Integral Probability Metrics\n(IPMs) [320], notably the 1-Wasserstein distance1, to quantify differences in\npredicted score distributions between demographic groups over the entire do-\nmain [0, 1]. However, in resource allocation applications, fairness constraints\napplied globally (i.e., over the entire output domain) can impose unnecessary\n1Efficient methods are available to approximate the distance and its gradients, enabling\nits use in a neural network objective function, as it can be directly minimized using\ncommon frameworks for neural network training.\n105\n Chapter 6: Decision-centric fairness\nrigidity, aiming to enforce fairness also outside the decision-making region\nwhere it will not affect real-world decisions, potentially degrading the pre-\ndictive quality of the generated scores more than necessary. Therefore, we\npropose a decision-centric fairness approach specifically tailored to induce\nfairness only within the decision-making region (i.e., over the domain [τ, 1]),\nthereby focusing only on the predictions that are relevant for resource allo-\ncation.\nOur approach operationalizes this concept by applying the unfairness\npenalty exclusively to the top-k% of the predicted scores of each protected\ngroup:\nLunfairness = IPM(˜y\n(k%)\n0\n, ˜y\n(k%)\n1\n),\n(6.6)\nwhere ˜y\n(k%)\n0\nand ˜y\n(k%)\n1\ndenote the distributions of the top-k% predicted scores\nfor the protected groups with s = 0 and s = 1, respectively, and the IPM\ncorresponds to the 1-Wasserstein distance 2. To determine the percentile\nthreshold k%, we first train a baseline unconstrained model (with λ = 0) and\nset k% to match the proportion of instances with scores above the decision\nthreshold τ on a validation set, irrespective of the protected attribute s. In\nthis way, the unfairness penalty specifically focuses on actionable instances.\nConceptually, this means we alter the composition of the set of instances\nwith ˜y ≥τ, as produced by a model without any penalty for unfairness, in\norder to induce greater fairness in the decision-centric demographic parity\nsense.\nThis effect is illustrated in Figure 6.2, which shows how different\nvalues of the hyperparameter λ impact the predicted score distributions for\ntwo demographic groups (s = 0 and s = 1) on a test set after training\na model for 30 epochs. Panel 6.2a shows the obtained score distributions\nfor λ = 0, i.e., a model optimized solely for classification error.\nIf only\ninstances in the decision-making region (i.e., those with a predicted score\n˜y ≥τ = 0.7) are acted upon, this model would result in clearly unfair resource\nallocation. Panels 6.2b and 6.2c, in contrast, induce decision-centric fairness\nwith increasing strength, resulting in increasingly fair score distributions in\nthe decision-making region (as reflected by the increasing overlap between\nthe top-k% score distributions across demographic groups), potentially at\nthe cost of predictive performance.\n2An alternative, more directly aligned with the goal of inducing fairness in the decision-\nmaking region [τ, 1], is a quantile-based approach that characterizes the differences be-\ntween the score distributions in a discretized manner using a histogram-based [321] unfair-\nness penalty. Although this allows for the direct use of the decision threshold τ—which\ndefines the relevant decision-making region—in optimization, training proved unstable in\npreliminary experiments, as even very small values of λ led to all predicted scores ˜y falling\nbelow τ.\nThe percentile-based approach prevents this downward biasing of predicted\nscores.\n106\n 6.4. Experimental design\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nProbability density (global)\n0\n10\n20\n30\n40\n50\nProbability density (top−k%)\nτ\n(a) λ = 0.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.0\n0.5\n1.0\n1.5\n2.0\nProbability density (global)\n0\n5\n10\n15\n20\nProbability density (top−k%)\nτ\n(b) λ = 0.3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.0\n0.5\n1.0\n1.5\n2.0\nProbability density (global)\ns=0\ns=1\n0\n2\n4\n6\n8\n10\n12\nProbability density (top−k%)\nτ\n(c) λ = 0.6\nFigure 6.2: The decision-centric fairness approach for different values of λ.\nScore distributions (PDFs) on a test set after training each model for 30\nepochs are shown, split by the protected attribute (s = 0 and s = 1). Solid\nlines represent the distributions of all predicted scores, while dotted lines\nrepresent the top-k% score distributions. The decision threshold τ is shown\nas a vertical line.\nAn animated version of these plots is available in the\nGitHub repository.\n6.4\nExperimental design\nThis section provides a detailed overview of the experimental design. We first\ndescribe the da",
  "27": "1.0\n1.5\n2.0\n2.5\nProbability density (global)\n0\n10\n20\n30\n40\n50\nProbability density (top−k%)\nτ\n(a) λ = 0.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.0\n0.5\n1.0\n1.5\n2.0\nProbability density (global)\n0\n5\n10\n15\n20\nProbability density (top−k%)\nτ\n(b) λ = 0.3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.0\n0.5\n1.0\n1.5\n2.0\nProbability density (global)\ns=0\ns=1\n0\n2\n4\n6\n8\n10\n12\nProbability density (top−k%)\nτ\n(c) λ = 0.6\nFigure 6.2: The decision-centric fairness approach for different values of λ.\nScore distributions (PDFs) on a test set after training each model for 30\nepochs are shown, split by the protected attribute (s = 0 and s = 1). Solid\nlines represent the distributions of all predicted scores, while dotted lines\nrepresent the top-k% score distributions. The decision threshold τ is shown\nas a vertical line.\nAn animated version of these plots is available in the\nGitHub repository.\n6.4\nExperimental design\nThis section provides a detailed overview of the experimental design. We first\ndescribe the datasets used and the process of creating semi-synthetic data to\nintroduce (additional) bias into the historical data, providing a way to con-\ntrol the level of discriminatory behavior when left unmitigated. This allows\nus to test the sensitivity of the different fairness induction methods to vary-\ning levels of bias in the data used to train classification models for resource\nallocation. Next, we introduce and motivate the use of a decision-centric\nand threshold-independent measure based on the precision-recall curve to\nassess predictive performance, complementing the fairness evaluation using\nthe decision-centric fairness metrics introduced in Section 6.3.1. Finally, we\nprovide details on the problem configuration setups and the hyperparameter\ncombinations tested. The code for reproducing the experiments is publicly\navailable at https://github.com/SimonDeVos/DCF.\n6.4.1\nData\nWe use three datasets for our experiments: TelecomKaggle (public), Churn\n(proprietary), and Adult (public). The first two datasets are directly rele-\nvant in the context of resource allocation, as they involve predicting churn\npropensity scores, whereas the last one is included because it is a standard\n107\n Chapter 6: Decision-centric fairness\ndataset in the algorithmic fairness literature3. An overview of dataset char-\nacteristics is provided in Table 6.1.\nThe need for fairness induction arises when biases are present in the\nhistorical data used to train classification models, or when there are inher-\nent differences between groups in their tendency to belong to the positive\nclass, which—although statistically justified—are considered unacceptable\ndiscrimination when acted upon. Specifically, if the data itself is unbiased\nand there are no inherent differences between groups, a classification model\noptimized solely for classification error will naturally produce fair predicted\nscores, and consequently, a fair resource allocation. To test the sensitivity of\nthe fairness induction methods discussed in Section 6.3.2, we control for the\nfirst issue mentioned above by varying the levels of discriminatory bias in the\nhistorical data for the TelecomKaggle dataset. Based on the raw TelecomK-\naggle dataset, we create three semi-synthetic datasets to introduce (addi-\ntional) bias into the historical data. This bias is systematically introduced\nthrough informed label flipping.\nSpecifically, within one protected group,\nwe selectively flip ground-truth outcome labels from 0 to 1. The details of\nthis procedure are outlined in Algorithm 4 in Appendix D.1. Figures D.1–\nD.3 display the score distributions obtained using no fairness induction, i.e.,\nreflecting the baseline discriminatory behavior present in the datasets. Fig-\nure D.1 illustrates the effect of introducing additional bias through informed\nlabel flipping. As the bias increases, the violation of demographic parity be-\ncomes more pronounced—both globally across the entire domain [0, 1] and\nwithin the decision-making region [τ, 1].\nAfter introducing bias through informed label flipping (where applica-\nble), each dataset is split into training, validation, and test sets using a fixed\nratio of 0.34/0.33/0.33. The training set is used to learn model parameters,\nthe validation set is used for hyperparameter tuning (i.e., selecting the con-\nfiguration with the lowest validation loss for λ = 0), and the test set is used\nfor final model evaluation. A relatively large portion of the data is allocated\nto the validation and test sets to ensure sufficient resolution and stability\nin assessing performance and fairness metrics, particularly when analyzing\ndecision-centric results across subgroups and thresholds.\n6.4.2\nEvaluation metrics\nOur evaluation examines both predictive performance and fairness through\na decision-centric lens, enabling the assessment of how the deployment of\nthe models would impact resource allocation.\n3Note that it is difficult to find public datasets in the fields of credit risk management\nand fraud detection, as in these domains fairness through unawareness is often used to\ncomply with existing regulations; hence, no information on potential protected attributes\nis (publicly) available in those datasets [322], [323].\n108\n 6.4. Experimental design\nTable 6.1: Overview of dataset characteristics. By introducing additional\nbias in TelecomKaggle, we increase the ground-truth class imbalance within\nthe protected group s = 1. For an overview of the score distributions ob-\ntained without fairness induction, i.e., reflecting the baseline discrimina-\ntory behavior present in the datasets, we refer to Figures D.1–D.3 in Ap-\npendix D.1.2.\nDataset\nProtected\n# Vars.\n# Obs.\nBias\ns = 0\ns = 1\nClass balance\nattribute s\nrate\ny = 0\ny = 1\ny = 0\ny = 1\ny = 0\ny = 1\nTelecomKaggle\n‘Sex’\n39\n7,032\n0.25\n0.27\n0.22\n0.37\n0.13\n0.64\n0.36\n0.50\n0.18\n0.31\n0.37\n0.13\n0.55\n0.45\n0.75\n0.09\n0.40\n0.37\n0.13\n0.46\n0.54\nChurn\n‘Sex’\n10\n44,942\n-\n0.38\n0.27\n0.18\n0.17\n0.56\n0.44\nAdult\n‘Sex’\n14\n32,561\n-\n0.46\n0.20\n0.29\n0.04\n0.76\n0.24\nTo evaluate the predictive performance of a classification model in the\ncontext of online resource allocation with a dynamic decision threshold τ,\ndriven by dynamic resource constraints (and/or instance-dependent costs\nand benefits), we need a threshold-independent evaluation metric. More-\nover, this metric should focus on predictions within the decision-making\nregion, as these are the predictions we will potentially act upon. Specifi-\ncally, for predictions across the relevant decision thresholds [τ, 1], we aim to\nmaximize both precision and recall. Focusing on precision alone can be mis-\nleading, as a model with fewer predictions for which ˜y ≥τ may appear more\nprecise simply because it predicts ˜y ≥τ only for the most confident cases,\ndisregarding recall. Conversely, focusing solely on recall may reward models\nthat capture more true positives at the cost of a higher false positive rate,\nleading to wasted resources. Precision-recall (PR) curves address this trade-\noff by considering both metrics simultaneously for all possible thresholds. To\nrestrict the thresholds of interest to those within the decision-making region,\nwe introduce the decision-centric performance metric AUC-PRτ. This metric\nsummarizes precision and recall by calculating the area under a partial PR\ncurve, which only considers thresholds above τ. This ensures that the perfor-\nmance assessment incorporates the deployment of the classification model in\ncombination with all decision thresholds within the decision-making region.\nFigure 6.3 illustrates how AUC-PRτ is obtained.\nIn addition to evaluating decision-centric predictive performance via AUC-PRτ,\nwe also evaluate the decision-centric fairness of the classification models. To\nthis end, we use the ABPCτ (Equation (6.3)) and ABCCτ (Equation (6.4))\nmetrics introduced in Section 6.3.1.\n109\n Chapter 6: Decision-centric fairness\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nPartial PR curve for τ = 0.7\nPR curve (AUC-PR = 0.88)\nPartial PR curve (AUC-PRτ = 0.62)\nDecision area cutoﬀ\nFigure 6.3: An example partial precision-recall curve and its corresponding\nAUC-PRτ, which summarizes precision and recall for all decision thresholds\nwithin the decision-making region [τ, 1]. This region corresponds to the top-\nleft segment of the PR curve starting at τ.\n6.4.3\nProblem and hyperparameter configurations\nThe most important hyperparameter in our experimental study is λ, as it\ncontrols the importance assigned to the unfairness penalty during training.\nDue to the predictive performance-fairness trade-off (see Section 6.2.3), how-\never, objectively tuning this hyperparameter is often not possible. Therefore,\nwe train neural classification models for λ ∈{0, 0.05, 0.10, . . . , 0.95} for both\nthe global and decision-centric fairness approaches. These models will be\nassessed using the concept of Pareto-optimality, as explained in more detail\nin Section 6.5. To determine all other hyperparameters, tuning is performed\nfor the model with λ = 0 on each dataset. The hyperparameter configu-\nration that results in the lowest validation loss is selected, and the same\nconfiguration is subsequently used for all values of λ. To quantify dissimi-\nlarities between score distributions for different demographic groups, i.e., for\nLunfairness, we use the implementation by Shalit, Johansson, and Sontag [320]\nto approximate the 1-Wasserstein distance IPM [324] (and its gradients) us-\ning Sinkhorn distances [325], [326]. For further details, we refer to Appendix\nB.1 of Shalit, Johansson, and Sontag [320]. Additional information on the\nmodel architecture, implementation, training, and hyperparameter tuning is\nprovided in Appendix D.2.\nIn addition to testing the sensitivity of the different fairness induction\nmethods to varying levels of discriminatory bias in the data (via the creation\nof semi-synthetic datasets with additional bias introduced; see Section 6.4.1),\nwe also assess their sensitivity to the size of the decision-making ",
  "28": "with λ = 0 on each dataset. The hyperparameter configu-\nration that results in the lowest validation loss is selected, and the same\nconfiguration is subsequently used for all values of λ. To quantify dissimi-\nlarities between score distributions for different demographic groups, i.e., for\nLunfairness, we use the implementation by Shalit, Johansson, and Sontag [320]\nto approximate the 1-Wasserstein distance IPM [324] (and its gradients) us-\ning Sinkhorn distances [325], [326]. For further details, we refer to Appendix\nB.1 of Shalit, Johansson, and Sontag [320]. Additional information on the\nmodel architecture, implementation, training, and hyperparameter tuning is\nprovided in Appendix D.2.\nIn addition to testing the sensitivity of the different fairness induction\nmethods to varying levels of discriminatory bias in the data (via the creation\nof semi-synthetic datasets with additional bias introduced; see Section 6.4.1),\nwe also assess their sensitivity to the size of the decision-making region by\nvarying the decision threshold τ.\n110\n 6.5. Results and discussion\n6.5\nResults and discussion\nIn this section, we present our results by comparing the Pareto fronts of the\nglobal and decision-centric fairness induction approaches. Specifically, we vi-\nsualize the trade-offs achieved between decision-centric predictive performance—\nmeasured by AUC-PRτ (higher is better)—and decision-centric fairness—\nmeasured by ABPCτ or ABCCτ (lower is better)—for the different values\nof λ; and we construct Pareto fronts for both the global and decision-centric\napproaches by identifying all models that represent optimal trade-offs under\nPareto-optimality (i.e., models for which no objective can be improved with-\nout worsening the other). The model without fairness induction (i.e., λ = 0)\nis also visualized and marked with a red star.\nWe structure our presentation and discussion of the results around the\nfollowing three key questions:\n⋅Q1: What is the impact of adopting a decision-centric versus a global\napproach to inducing fairness in the decision-making region on predic-\ntive performance?\n⋅Q2: How do the size of the decision-making region and varying lev-\nels of discriminatory bias in the historical data affect the differences\nin predictive performance between a decision-centric versus a global\nfairness induction approach?\n⋅Q3: Which metric is most appropriate for evaluating decision-centric\nfairness, and how should a model be selected for deployment?\nTo address these questions, we selectively highlight relevant results. A com-\nprehensive set of results is presented in Appendix D.3.\n6.5.1\nQ1: Impact of decision-centric versus global fair-\nness approach on predictive performance\nWe first investigate how decision-centric fairness optimization compares to\nthe global approach by evaluating models trained with varying values of λ.\nFigures 6.4 and 6.5 show the results for three datasets using ABPCτ and\nABCCτ as the fairness metric, respectively.\nThe plots clearly illustrate the advantage of our proposed decision-centric\nmethod (orange curves) over the global approach (blue curves). As λ in-\ncreases, model performance moves along a trade-off curve, prioritizing fair-\nness at the expense of predictive performance. However, the decision-centric\napproach consistently delivers superior trade-offs, underscoring its effective-\nness in balancing fairness with predictive performance specifically within the\ndecision-making region.\nThe trade-off varies by dataset. For example, in the TelecomKaggle and\nChurn datasets, fairness can be substantially improved with relatively minor\n111\n Chapter 6: Decision-centric fairness\n0.1\n0.2\n0.3\nABPCτ\n0.3\n0.4\n0.5\n0.6\n0.7\nAUC-PRτ\nτ: 0.7\n(a) TelecomKaggle (bias\nrate 0.5)\n0.01\n0.02\n0.03\nABPCτ\n0.3\n0.4\n0.5\nAUC-PRτ\nτ: 0.7\n(b) Churn\nchurn\n0.1\n0.2\nABPCτ\n0.45\n0.50\n0.55\nAUC-PRτ\nτ: 0.5\nGlobal reg.\nLocal reg.\nBiased model\n(c) Adult\nadult\nFigure 6.4: Pareto fronts illustrating the trade-off between predictive perfor-\nmance and fairness (AUC-PRτ vs. ABPCτ) for the decision-centric (orange)\nand global (blue) fairness induction approaches for three datasets.\n0.00\n0.05\n0.10\nABCCτ\n0.3\n0.4\n0.5\n0.6\n0.7\nAUC-PRτ\nτ: 0.7\n(a) TelecomKaggle (bias\nrate 0.5)\n0.005\n0.010\nABCCτ\n0.3\n0.4\n0.5\nAUC-PRτ\nτ: 0.7\n(b) Churn\nchurn\n0.00\n0.02\n0.04\nABCCτ\n0.45\n0.50\n0.55\nAUC-PRτ\nτ: 0.5\nGlobal reg.\nLocal reg.\nBiased model\n(c) Adult\nadult\nFigure 6.5: Pareto fronts illustrating the trade-off between predictive perfor-\nmance and fairness (AUC-PRτ vs. ABCCτ) for the decision-centric (orange)\nand global (blue) fairness induction approaches for three datasets.\nreductions in predictive performance.\nConversely, for the Adult dataset,\neven a slight increase in λ leads to a drop in predictive performance, though\nthe decision-centric approach still yields a more favorable trade-off than the\nglobal method.\nMoreover, adding an unfairness penalty—whether decision-centric or global—\ncan further enhance both fairness and predictive performance. This effect is\nvisible in the plots, where certain models lie higher and further to the left\nthan the model without unfairness penalty (i.e., with λ = 0) marked with a\nred star. This shows that the unfairness penalty, in some cases, also serves\nas a regularization mechanism.\n112\n 6.5. Results and discussion\nABCCτ\n0.0\n0.2\n0.4\n0.6\n0.8\nAUC-PRτ\nBias: 0.25, τ: 0.5\nABCCτ\nAUC-PRτ\nBias: 0.25, τ: 0.7\nABCCτ\nAUC-PRτ\nBias: 0.25, τ: 0.8\nGlobal reg.\nLocal reg.\nBiased model\nABCCτ\n0.0\n0.2\n0.4\n0.6\n0.8\nAUC-PRτ\nBias: 0.5, τ: 0.5\nABCCτ\nAUC-PRτ\nBias: 0.5, τ: 0.7\nABCCτ\nAUC-PRτ\nBias: 0.5, τ: 0.8\n0.0\n0.1\n0.2\n0.3\nABCCτ\n0.0\n0.2\n0.4\n0.6\n0.8\nAUC-PRτ\nBias: 0.75, τ: 0.5\n0.0\n0.1\n0.2\n0.3\nABCCτ\nAUC-PRτ\nBias: 0.75, τ: 0.7\n0.0\n0.1\n0.2\n0.3\nABCCτ\nAUC-PRτ\nBias: 0.75, τ: 0.8\nFigure 6.6: Results on the TelecomKaggle dataset for different bias rates,\nwith decision-centric fairness measured by ABCCτ. The figure illustrates\nthe effect of a varying size of the decision-making region with τ = 0.5, 0.7, 0.8\n(columns) on the decision-centric fairness-predictive performance trade-off.\nThe orange and blue lines represent decision-centric and global fairness in-\nduction, respectively, while the model without unfairness penalty (i.e., with\nλ = 0) is marked with a red star.\n6.5.2\nQ2:\nImpact of decision-making region size and\nlevel of discriminatory bias in historical data\nWe analyze the influence of varying the decision threshold τ and semi-\nsynthetic bias rates on model performance, both in terms of predictive per-\nformance and fairness, using the TelecomKaggle dataset. Using the informed\n113\n Chapter 6: Decision-centric fairness\nlabel flipping method to generate semi-synthetic datasets with varying bias\nlevels allows us to systematically investigate the impact of bias on model\noutcomes. Results are shown in Figure 6.6, where rows represent different\nbias rates (0.25, 0.50, 0.75) and columns represent different decision thresh-\nolds (τ values of 0.5, 0.7, and 0.8), with fairness measured by ABCCτ. For\nfairness results in terms of ABPCτ, see Figure D.4 in the appendix.\nThe models without unfairness penalization (i.e., with λ = 0, indicated by\nred stars) illustrate how fairness deteriorates as either the bias increases or\nthe decision-making region becomes larger. Specifically, for a fixed τ, higher\nsemi-synthetic bias consistently reduces fairness, evidenced by a rightward\nshift of baseline points. Similarly, for a fixed bias rate, a larger decision-\nmaking region (lower τ) also negatively impacts fairness. This is because\nmore predictions are included in the decision-making region and thus af-\nfect decision-centric fairness calculations. Consequently, the best baseline\ndecision-centric fairness is observed with minimal bias and a higher τ, while\nthe worst-case scenario arises from a combination of high bias and a large\ndecision-making region.\nIn comparing global (blue) and decision-centric (orange) approaches, we\nobserve similar performance for the low bias rate (0.25), where fairness in-\nduction has a minimal potential impact as there is little bias present to be\neliminated. However, for the higher bias rates (0.50 and 0.75), the decision-\ncentric approach increasingly outperforms the global approach, demonstrat-\ning its effectiveness. The advantage of the decision-centric approach over\nthe global approach becomes more pronounced as the decision threshold τ\nincreases, resulting in a smaller decision-making region. Conversely, in the\nextreme case where τ = 0, both decision-centric and global fairness induc-\ntion methods coincide due to the decision-making region covering the entire\nprediction domain.\nFor results on varying the decision threshold τ for the Churn and Adult\ndatasets, we refer to Figures D.5–D.8 in the appendix. For the Adult dataset,\nin addition to τ values of 0.5 and 0.8, we also include τ = 0.4 because of\nthe class imbalance, which leads to fewer predictions in the decision-making\nregion.\n6.5.3\nQ3: Impact of decision-centric fairness metric used\nfor evaluation and model selection\nThe ABCC metric directly evaluates the 1-Wasserstein distance between\nthe score distributions of different demographic groups [289], and as such, it\naligns closely with our implementation of Lunfairness for the global approach.\nA similar alignment exists between the ABCCτ metric and the decision-\ncentric fairness approach, which may (partly) explain the larger differences\n114\n 6.6. Conclusion\nobserved between the Pareto fronts of the global and decision-centric ap-\nproaches for the TelecomKaggle and Churn datasets (compare the plots in\nFigures 6.4 and 6.5).\nBeyond the optimization-evaluation alignment, while the PDF-based ABPCτ\nmetric provides an intuitive way to quantify decision-centric fairness (see\nFigure 6.1), the ABCCτ metric is also better suited to the underlying prob-\nlem of using predicted scores for resource allocation, as it is sensitive to the\ndistance that probability mass must move to align the score distributions\nacross groups—capturing not only the existence but also the magnitud",
  "29": "tric directly evaluates the 1-Wasserstein distance between\nthe score distributions of different demographic groups [289], and as such, it\naligns closely with our implementation of Lunfairness for the global approach.\nA similar alignment exists between the ABCCτ metric and the decision-\ncentric fairness approach, which may (partly) explain the larger differences\n114\n 6.6. Conclusion\nobserved between the Pareto fronts of the global and decision-centric ap-\nproaches for the TelecomKaggle and Churn datasets (compare the plots in\nFigures 6.4 and 6.5).\nBeyond the optimization-evaluation alignment, while the PDF-based ABPCτ\nmetric provides an intuitive way to quantify decision-centric fairness (see\nFigure 6.1), the ABCCτ metric is also better suited to the underlying prob-\nlem of using predicted scores for resource allocation, as it is sensitive to the\ndistance that probability mass must move to align the score distributions\nacross groups—capturing not only the existence but also the magnitude of\nscore shifts. In contrast, ABPC (and therefore ABPCτ) only quantifies how\nmuch probability mass is placed differently between groups, but is invari-\nant to how far these differences are from each other. For instance, if two\ndistributions differ only in localized regions—say, around 0.75 and 0.9 in\none scenario, versus around 0.85 and 0.9 in another (with the density being\nlower around one value and higher around the other)—then ABPCτ with\nτ < 0.7 would yield the same value for both. However, ABCCτ would favor\nthe latter scenario. Since the likelihood of these differences—and thus the\nalgorithmic unfairness—being canceled out through thresholding (with the\ndecision threshold varying between τ and 1) increases when the distances be-\ntween them are smaller, ABCCτ is preferable for evaluating decision-centric\nfairness in resource allocation contexts.\nHence, to select a classification model for deployment in the context of re-\nsource allocation optimization, we advise decision-makers to choose a model\n(which, in our experiments, corresponds to selecting a fairness induction\nstrategy together with a value for λ) that offers a good trade-off between\nABCCτ and AUC-PRτ. Our results show that it is often possible to sub-\nstantially reduce ABCCτ through decision-centric fairness induction (and in\nsome cases also through global fairness induction), without compromising\nAUC-PRτ compared to a model without fairness induction. As such, when\nfairness is not mandated by strict regulatory requirements, these models are\nof particular interest. Performance in terms of ABPCτ can be used to further\nassess a (small) subset of candidate models in a complementary manner.\n6.6\nConclusion\nThis paper introduces and formalizes the concept of decision-centric fairness\nin classification models used for online resource allocation optimization with\ndynamic resource constraints. This novel approach aligns algorithmic fair-\nness considerations with their potential impact on real-world resource alloca-\ntion decisions, such as the fairness of credit risk scores used for loan approvals\nor churn propensity scores used in targeted marketing retention campaigns,\nwhile accommodating dynamic decision thresholds.\nDecision-centric fair-\n115\n Chapter 6: Decision-centric fairness\nness redefines fairness evaluation and induction by shifting the focus from\nfull classification score distributions to considering only the decision-making\nregion, which includes only the range of relevant decision thresholds for a\ngiven resource allocation problem.\nThis ensures that fairness constraints\nare considered and/or enforced only where they will impact real-world de-\ncisions. We propose a method to optimize classification models directly for\ndecision-centric fairness and demonstrate that by avoiding overly strict fair-\nness constraints, we can minimize the degradation of the predictive quality\nof the generated scores. Specifically, we empirically show that our decision-\ncentric fairness methodology often leads to better decision-centric predic-\ntive performance-fairness trade-offs compared to a global fairness approach,\nwhich applies fairness constraints more naively across the full score distri-\nbutions.\nFor certain resource allocation problems, such as targeted retention cam-\npaigns based on churn propensity scores, a causal uplift modeling approach\nhas been shown to outperform the predictive modeling approach adopted\nin this work, leading to improved profitability of retention campaigns [327].\nHowever, the predictive modeling approach remains valuable in practice, as\nit allows the use of a single customer churn prediction model across various\nretention campaigns—something that is not possible with the treatment-\ndependent uplift modeling approach. Nevertheless, adapting our proposed\ndecision-centric fairness approach to optimize uplift scores appears to be a\npromising direction for future research. Exploring this in the context of cost-\nsensitive uplift modeling [328], which leverages class-dependent or instance-\ndependent costs and benefits to improve decision-making, also seems worth-\nwhile, though potentially more challenging.\nAs a first step, incorporat-\ning decision-centric fairness into the cost-sensitive predictive modeling ap-\nproach [277] could serve as a starting point. In a similar vein to Devriendt,\nBerrevoets, and Verbeke [327], Vanderschueren, Baesens, Verdonck, et al.\n[329] recently demonstrated that framing resource allocation problems with\nstochastic resource constraints as a ranking problem, and subsequently rely-\ning on learning-to-rank techniques instead of classification techniques, leads\nto improved decision-making outcomes.\nThis ranking approach naturally\napplies to all resource allocation problems (in contrast to an uplift modeling\napproach). Hence, incorporating a decision-centric fairness perspective into\nthe optimization of learning-to-rank models appears to be a promising direc-\ntion for future research as well. Translating such fairness-enhanced learning-\nto-rank models to the uplift modeling setting, building on Devriendt, Van\nBelle, Guns, et al. [330], would help complete the picture.\nBeyond these methodological extensions, our work opens several addi-\ntional avenues for future research concerning the fairness perspective. First,\nthe fairness metrics used in this study center on a single group fairness no-\n116\n 6.6. Conclusion\ntion: decision-centric demographic parity. While practical and grounded in\nregulatory logic [284], [314], group-based metrics have notable limitations\nas they mask disparities at the individual level. Moreover, there might be\nadverse effects of reranking individuals (across or within sensitive groups),\nwhich may undermine the perceived validity of fairness [331]. Future re-\nsearch could examine alternative or complementary notions of fairness, such\nas equal opportunity [280] or counterfactual fairness [301].\nSecond, we\nadopt a percentile-based approach to operationalize decision-centric fairness\ninduction—aligning the top−k% of each group’s score distribution. This ap-\nproach, however, remains a proxy for the ideal case where fairness is enforced\nstrictly within the decision-making region. As discussed in Section 6.3.2, a\nquantile-based method that directly compares and penalizes disparities in\nscore distributions above a threshold τ would align more closely with the for-\nmalization of decision-centric fairness. However, our initial experiments with\nsuch quantile-based implementations revealed training instability. Therefore,\ninvestigating how to efficiently and robustly implement such a quantile-based\napproach is left for future research. Next, although not yet operationalized,\ndecision-centric fairness could naturally extend to multiple or intersecting\nprotected attributes, suggesting a direction for future work [16], [302]. Fi-\nnally, further strengthening the formal connection between decision-centric\nfairness and legal interpretations of actionability in anti-discrimination law\n[314] would be a valuable step toward ensuring legal as well as practical\nsoundness. Bridging the conceptual gap between legal standards of fairness\nand the integration of fairness considerations into algorithm evaluation and\ndesign remains a critical step in operationalizing fairness.\n117\n  7\nUplift modeling with continuous\ntreatments:\nA predict-then-optimize approach\nThe goal of uplift modeling is to recommend actions that optimize specific\noutcomes by determining which entities should receive treatment. One com-\nmon approach involves two steps: first, an inference step that estimates\nconditional average treatment effects (CATEs), and second, an optimiza-\ntion step that ranks entities based on their CATE values and assigns treat-\nment to the top k within a given budget. While uplift modeling typically\nfocuses on binary treatments, many real-world applications are character-\nized by continuous-valued treatments, i.e., a treatment dose.\nThis paper\npresents a predict-then-optimize framework for uplift modeling with con-\ntinuous treatments. First, in the inference step, conditional average dose\nresponses (CADRs) are estimated from data using causal machine learning\ntechniques. Second, in the optimization step, we frame the assignment task\nof continuous treatments as a dose-allocation problem and solve it using in-\nteger linear programming (ILP). This approach allows decision-makers to\nefficiently and effectively allocate treatment doses while balancing resource\navailability, with the possibility of adding extra constraints like fairness con-\nsiderations or adapting the objective function to take into account instance-\ndependent costs and benefits to maximize utility. The experiments compare\nseveral CADR estimators and illustrate the trade-offs between policy value\nand fairness, as well as the impact of an adapted objective function. This\ndemonstrates the framework’s advantages and flexibility across diverse ap-\nplications in healthcare, lending, and hum",
  "30": "-optimize framework for uplift modeling with con-\ntinuous treatments. First, in the inference step, conditional average dose\nresponses (CADRs) are estimated from data using causal machine learning\ntechniques. Second, in the optimization step, we frame the assignment task\nof continuous treatments as a dose-allocation problem and solve it using in-\nteger linear programming (ILP). This approach allows decision-makers to\nefficiently and effectively allocate treatment doses while balancing resource\navailability, with the possibility of adding extra constraints like fairness con-\nsiderations or adapting the objective function to take into account instance-\ndependent costs and benefits to maximize utility. The experiments compare\nseveral CADR estimators and illustrate the trade-offs between policy value\nand fairness, as well as the impact of an adapted objective function. This\ndemonstrates the framework’s advantages and flexibility across diverse ap-\nplications in healthcare, lending, and human resource management. All code\nis available on https://github.com/SimonDeVos/UMCT.\n119\n Chapter 7: Uplift modeling with continuous treatments\n7.1\nIntroduction\nIn many applications, decision-makers are interested in learning the causal\neffects of a treatment on a particular outcome [332], [333]. A crucial aspect\nis response heterogeneity, where entities respond differently to treatments\nbased on their characteristics [334]. In intervention-based decision-making,\nprecisely this heterogeneity is valuable to leverage for finding a treatment\nassignment policy that optimizes a specific outcome variable, based on his-\ntorical data, while adhering to certain constraints such as scarce resources.\nUplift modeling (UM) is a set of techniques to find such policies [67]. An\noften-used predict-then-optimize UM approach combines conditional aver-\nage treatment effect (CATE) estimation with an optimization step, where\nCATE estimation can be considered an inference step as part of a larger\ndecision-making process [335].\nMany real-world applications, as illustrated by Table 7.1, contain com-\nplexities beyond binary treatments and budget as the sole constraint and\nare better characterized by a treatment dose, i.e., where the intervention\ncan be applied across a continuous range of values [336], [337]. Moreover,\ncontinuous treatments have benefits for making treatment allocation possi-\nbly more effective and efficient compared to their binary counterpart because\nthe marginal utility of treatment can vary with different dose levels, and the\ntreatments can be allocated on fine granularity.\nTable 7.1:\nThis table displays exemplary applications of a UM setting\nwith continuous-valued treatments and their corresponding details. Cost-\nsensitivity can either refer to outcome benefits (o) or treatment costs (t)\nand Constraint to budget (b) or fairness (f). The examples provided are\nillustrative and not intended to be exhaustive or fully comprehensive.\nApplication\nOutcome\nCont. Treatment\nCost-sensitivity\nConstraint\nCredit\nDefault rate\nInterest rate\nLoss given default (o)\nRegul. compliance (f)\nHealthcare\nSickness\nMedication dose\nMedication price (t)\nEqual access (f)\nHR\nEmployee retention\nTraining hours\nHourly opportunity cost (t)\nInstructors (b)\nMaintenance\nMachine up-time\nMaintenance freq.\nMachine criticality (o)\nSpare parts avail. (b)\nWhile there is a growing body of literature focusing on conditional aver-\nage dose response (CADR) estimation (i.e., the continuous-valued counter-\npart of CATE), this line of work focuses on causal inference and finding opti-\nmal doses, largely ignoring the role of continuous treatments in constrained\ndecision-making contexts [338]–[340]. Constraints play a vital role in embed-\nding business requirements into decision-making, which we accomplish using\nan integer linear programming (ILP) formulation. While many requirements\nexist, fairness stands out as a key consideration and is the main exemplary\nconstraint throughout this work [341]. Though fairness constraints are not\n120\n 7.1. Introduction\nnew in machine learning for decision-making, they are often embedded as\nsoft constraints during model training [342], [343]. This approach, however,\nis limited in flexibility and modularity. By shifting the inclusion of fairness\nconsiderations to a post-processing step, i.e., after model training, decision-\nmaking is more flexible toward dynamic business requirements.\nTherefore, in this paper, we develop a UM framework with continuous\ntreatments. Our work addresses two key gaps in the current literature. First,\nUM has not been formally defined, with the distinction between treatment\neffect estimation and allocation optimization largely overlooked.\nSecond,\nas far as we are aware, there are no established methods in the literature\nextending UM to handle (i) continuous treatments, let alone methods that\nmanage the combined complexities of also including (ii) extra constraints\n(like fairness) and (iii) objective function alteration (to, for example, account\nfor cost-sensitivity).\nBy addressing these gaps, our contributions are fourfold:\n1. We provide a framework that clearly defines UM, and how this differ-\nentiates from a mere causal inference problem, i.e., treatment effect\nestimation.\n2. We extend UM methods to effectively handle continuous treatments\nby first leveraging state-of-the-art causal ML methods for CADR es-\ntimation and then defining the optimization part as a dose-allocation\nproblem. The ILP formulation offers flexibility to adapt the objective\nfunction (e.g., to incorporate cost-sensitivity) and to include additional\nconstraints (e.g., to enforce fairness considerations).\n3. We are the first to include fairness considerations in a UM setting as\nexplicit constraints in an ILP formulation. By extension, to the best\nof our knowledge, this is also the first application of such constraints\nin decision-making pipelines that utilize ML predictions as input.\n4. With a series of experiments, we show the capabilities of our frame-\nwork and demonstrate how incorporating fairness constraints or cost-\nsensitive objectives influences policy outcomes, crucial for applications\nsuch as those highlighted in Table 7.1.\nThe remainder of this paper is structured as follows. Section 7.2 defines\nUM and elaborates on related literature. Section 7.3 introduces the problem\nformulation and establishes the necessary notation. Section 7.4 focuses on\nthe methodology, explaining the predict-then-optimize approach, the pre-\ndictive models used to estimate CADRs, and the optimization techniques\nemployed for constrained treatment allocation. Section 7.5 details the ex-\nperimental setup, including data, evaluation metrics, and the experiments,\nfollowed by a discussion of the results.\nFinally, Section 7.6 presents our\nconclusions, discusses limitations, and outlines potential further research.\n121\n Chapter 7: Uplift modeling with continuous treatments\n7.2\nUplift modeling\nIn this section, we begin by discussing the purpose of UM, offering a clear\ndefinition and a detailed breakdown of its core elements. We then explore\nhow UM can be extended to accommodate continuous treatments. Finally,\nwe consider adjustments to UM, such as incorporating additional constraints\nor modifying the objective function, to better align with specific application\nrequirements, reflecting its relevance to the domain of prescriptive analytics.\n7.2.1\nPurpose and definition\nUM is a well-established set of methods in the field of personalized decision-\nmaking, closely aligned with the goals of prescriptive analytics [67]. Unlike\ntraditional predictive models that identify entities likely to yield a desired\noutcome, UM distinguishes between baseline responders (entities likely to\nshow positive outcomes even without intervention) and true responders that\nrespond because of the treatment. Consequently, UM focuses on outcome\nchanges directly attributable to the treatment assignment, rather than sim-\nply predicting positive outcome probabilities. This helps avoid suboptimal\ntargeting and ensures assignment policies capture their true incremental im-\npact. Traditional UM predominantly deals with binary treatments, where\nthe main goal is to rank individuals based on their expected response to\na treatment [344]. Works such as [67] provide a comprehensive overview,\nfocusing on a setting with binary treatment effects, various modeling tech-\nniques, and the associated challenges. However, the literature often lacks a\nunified definition of UM, with some studies equating it to treatment effect\ninference [345] while others focus primarily on ranking methods [346]. To\nresolve this ambiguity, we propose the following UM definition:\nDefinition 1. Uplift modeling refers to the collection of methods where\nthe task at hand is optimally allocating treatments, with the objective of\nmaximizing the total benefit generated by these treatments, determined by\nthe cumulative uplift over entities, under given constraints.\nThe above definition encompasses both the one-step approach (i.e., predict-\nand-optimize) and the two-step approach (i.e., predict-then-optimize). No-\ntably, this definition is method-agnostic to keep it inclusive of decision-focused\nmethods that learn policies directly and therefore do not output an explicit\ntreatment-effect estimate.\nAs Table 7.2 highlights, the methodological con-\ntribution of this work focuses on the two-step approach, where treatment\neffect estimation is a component of the broader UM framework, which also\nincludes treatment allocation as a critical task.\n122\n 7.2. Uplift modeling\nTable 7.2: A schematic overview of the positioning of our work within various\nUM approaches.\nThe focus of this paper is highlighted in grey.\nIn the\nDecision-Focused Learning (DFL) paradigm, the goal is to directly include\noptimal treatment allocation in the model learning task (i.e., predict-and-\noptimize). Prediction-Focused Learning (PFL) consists of (i) an inference\nand (ii) an optimization step (i",
  "31": "inition encompasses both the one-step approach (i.e., predict-\nand-optimize) and the two-step approach (i.e., predict-then-optimize). No-\ntably, this definition is method-agnostic to keep it inclusive of decision-focused\nmethods that learn policies directly and therefore do not output an explicit\ntreatment-effect estimate.\nAs Table 7.2 highlights, the methodological con-\ntribution of this work focuses on the two-step approach, where treatment\neffect estimation is a component of the broader UM framework, which also\nincludes treatment allocation as a critical task.\n122\n 7.2. Uplift modeling\nTable 7.2: A schematic overview of the positioning of our work within various\nUM approaches.\nThe focus of this paper is highlighted in grey.\nIn the\nDecision-Focused Learning (DFL) paradigm, the goal is to directly include\noptimal treatment allocation in the model learning task (i.e., predict-and-\noptimize). Prediction-Focused Learning (PFL) consists of (i) an inference\nand (ii) an optimization step (i.e., predict-then-optimize).\nTreatment\nPredict-and-optimize\nPredict-then-optimize\nBinary\nLearn to rank\n(i) CATE estimation\n(ii) Treatment-allocation problem\nContinuous\nLearn to allocate\n(i) CADR estimation\n(ii) Dose-allocation problem\n7.2.2\nTreatment effects\nA treatment refers to an intervention or action that can be applied to an\nentity to influence a particular outcome. Treatments can be binary, where\nthe action is treat or do not treat (e.g., offering or withholding discount).\nThe CATE measures the expected difference in potential outcomes between\ntreated and untreated groups, conditioned on entities’ characteristics [320].\nAdditionally, recent developments have extended traditional UM to multi-\ntreatment scenarios, which generalize the setup by considering more than\ntwo treatment options [347] (e.g., offering discounts through different chan-\nnels), and sequential treatments, where multiple treatments can be applied\nover time, each potentially influencing the outcome effects of subsequent\ntreatments [348]. However, these are not the focus of our paper and are\nconsidered future extensions.\nThe estimation of causal treatment effects from data is inherently chal-\nlenging due to the fundamental problem of causal inference, namely, the ab-\nsence of counterfactual outcomes [332]. In the case of observational data, this\nproblem is exacerbated by the possible non-random assignment of treatments\nand the resulting confounding bias [333]. A real-life example is the overes-\ntimation of workplace wellness programs’ impact on employee wellbeing,\nwhere non-random incentives cause treated populations to differ from the\ngeneral population [349]. A naive approach may overfit on the self-selected\ngroup, missing the true causal effect. This highlights how confounding can\nlead to issues like Simpson’s paradox, where aggregated data misrepresents\ntrends compared to stratified data [349]. While randomized controlled trials\n(RCTs) are the gold standard for mitigating such biases [350], they are often\nimpractical in real-world business settings because of cost, ethical, or op-\nerational reasons. Moreover, in the context of continuous treatments, with\n123\n Chapter 7: Uplift modeling with continuous treatments\nvirtually infinite possible doses, setting up an RCT for dose-response esti-\nmation becomes significantly more complex [337]. In contrast, observational\ndata is often cheap and readily accessible.\nTherefore, methods that aim\nto balance the treatment and control groups have been proposed, including\ntechniques like propensity score matching [351] or covariate balancing [320],\n[352].\nIn recent years, the consideration of continuous treatments—and conse-\nquently, CADR estimation — has gained traction because of its relevance in\napplications where understanding the effects of varying dose levels is criti-\ncal (see Table 7.1). Estimating CADRs is inherently more challenging than\nCATEs because it involves a spectrum of treatment doses, rather than val-\nues 0 and 1 [339]. Also, with the theoretically infinite number of possible\ndose values, data sparsity and the non-uniformity of dose assignment could\nbe challenging, making accurate effect estimation at each dose level difficult\n[353]. The Conditional Average Dose Effect (CADE), which can be directly\nderived from the CADR, generalizes the CATE for continuous treatments.\nOur paper specifically focuses on UM with this type of treatment.\nTra-\nditionally, dose effects have been derived from average dose-response curves\nusing methods like the Hirano-Imbens estimator [336], which extends propen-\nsity scores to continuous interventions through generalized propensity scores.\nHowever, these approaches often overlook individual-level heterogeneity in\nresponses. Recent advancements in ML have introduced techniques for learn-\ning individualized dose-response curves [338]–[340], [354], [355]. However,\nanalogously to the case of CATE estimation, these methods primarily focus\non the inference step and do not incorporate an optimization step where the\nlearned dose-response relationships are used to prescribe decisions [335].\nIn this paper, we do not make any assumptions about the shape of the\nCADR. Instead, we adopt a flexible, data-driven approach.\nIn contrast,\nexisting work, such as [356], relies on a strong structural assumption — the\nlaw of diminishing marginal utility — which implies that the marginal benefit\nof increasing the dose is strictly decreasing. While this assumption may hold\nin some applications, it limits model flexibility and generalizability.\nOur\nframework does not rely on such assumptions: the estimated dose-response\nrelationships in our approach are non-parametric and can capture arbitrary,\npotentially non-monotonic shapes.\n7.2.3\nAllocation task\nCumulative uplift & total benefit as driver\nThe allocation task aims to optimize an objective value driven by (cumula-\ntive) uplift. While uplift applies to a single entity, cumulative uplift reflects\nthe policy’s overall impact. For a policy treating K entities, cumulative up-\n124\n 7.2. Uplift modeling\nlift is the sum of CATEs for discrete treatments or CADEs for continuous\ntreatments across these treated entities. In certain applications, cumulative\nuplift directly aligns with the target objective, as seen in contexts where\nmaximizing cumulative uplift is the explicit goal [67]. However, cumulative\nuplift itself does not necessarily represent the objective. For example, in\nvalue-driven analytics, where uplift might disregard expected profit, and in\ncases where each entity may be associated with an individual benefit and\ntreatment cost [357]–[361]. Hence, when cumulative uplift is not the end\ngoal in itself, allocations with lower cumulative uplift may yield a greater\nincrease in objective value.\nPredict-and-optimize\nIn UM, the task of allocating treatments can follow one of two paradigms,\nrepresented by the two columns in Table 7.2. On the one hand, there is the\nparadigm of decision-focused learning, where the optimal treatment alloca-\ntion is directly learned in an integrated way [362], [363]. On the other hand,\nprediction-focused learning separates two distinct steps: First, an inference\ntask, where treatment effects are estimated, followed by an optimization task\nthat uses these estimates as input to allocate treatments [68].\nIn this paper, we adopt the predict-then-optimize paradigm, which is\nrepresented in the second column of Table 7.2.\nWith this approach, we\nclearly distinguish the treatment estimation task as a component of the larger\nUM approach, which additionally includes an optimization step that takes\nestimated treatment effects as input [335]. In the case of binary treatments,\nthe inference step involves estimating CATEs, followed by an optimization\nstep, i.e., finding a policy by solving the treatment-allocation problem to\ndetermine which entities should receive treatment. Typically, this is done\nusing a greedy ranking heuristic: entities are ranked based on their predicted\nCATEs, and treatments are assigned to the top-k entities until the budget\nis exhausted [364]. Alternatively, under a flexible budget, the optimal value\nof k is determined through cost-benefit analysis [360].\nIn contrast, the predict-and-optimize approach, or decision-focused learn-\ning (DFL), integrates prediction and optimization into a single end-to-end\nsystem [365]. This paradigm tailors predictions specifically for downstream\nobjectives, using regret-based loss functions to align predictions with opti-\nmization goals, where x⋆(c) represents the optimal decision with full infor-\nmation, and x⋆(ˆc) is based on estimated parameters [366]:\nLregret = f (x⋆(c), c) −f (x⋆(ˆc), c)\n(7.1)\nAlthough DFL approaches for UM exist [362], [363], we adopt the predict-\nthen-optimize framework due to its simplicity, flexibility, stability, and gen-\neralizability to other treatment types.\nIt allows using various predictive\n125\n Chapter 7: Uplift modeling with continuous treatments\nmodels without being restricted by a specific optimization setup, which is\nespecially useful for complex or pre-trained models without access to the\ntraining process [341]. Moreover, it enables flexible adjustments to the ob-\njective function and integration of additional constraints other than budget\nlimits, such as fairness considerations, allowing for different allocations under\nvarying budget conditions. Our approach requires modifying only the opti-\nmization step, leaving the prediction model unchanged and does not require\na complete model retraining.\nAllocation problem as ILP\nWhen considering continuous treatments, the optimization step becomes a\ndose-allocation problem, where the task is to determine the optimal treat-\nment dose for each entity. This generalizes the binary treatment-allocation\nproblem, which can often be addressed by ranking heuristics. However, since\nfor the dose-allocation problem the decision space expands, a greedy ranking\nheuristic is no longer feasible, and a more formal optimizati",
  "32": "icted by a specific optimization setup, which is\nespecially useful for complex or pre-trained models without access to the\ntraining process [341]. Moreover, it enables flexible adjustments to the ob-\njective function and integration of additional constraints other than budget\nlimits, such as fairness considerations, allowing for different allocations under\nvarying budget conditions. Our approach requires modifying only the opti-\nmization step, leaving the prediction model unchanged and does not require\na complete model retraining.\nAllocation problem as ILP\nWhen considering continuous treatments, the optimization step becomes a\ndose-allocation problem, where the task is to determine the optimal treat-\nment dose for each entity. This generalizes the binary treatment-allocation\nproblem, which can often be addressed by ranking heuristics. However, since\nfor the dose-allocation problem the decision space expands, a greedy ranking\nheuristic is no longer feasible, and a more formal optimization approach is\nrequired.\nTherefore, we formulate and solve the allocation problem as an ILP. To\nconsider binary choices per dose level, we discretize the estimated CADRs\ninto distinct dose options. This discretization is motivated by two factors:\n(i) it allows the ILP formulation and (ii) many practical applications only\npermit pre-defined dose levels with limited granularity [367].\nWhile this\ndiscretization introduces some loss of generalization, its impact is minimal\nsince the number of dose bins is flexible and only affects the optimization\nstep without altering the prediction step. The number of dose bins can be\nfreely adjusted, although more bins increase computational complexity.\nFlexible optimization constraints and objectives\nDuring the optimization step, constraints and objective functions can be\nflexibly adjusted. Any constraint compatible with ILP can be added, and the\nobjective function can be adapted, as long as it remains a linear combination\nof decision variables, as required by the ILP. Fairness constraints and cost-\nsensitive objective functions are exemplary, and further modifications, such\nas operational constraints, can also be incorporated\nFairness as constraint\nThe importance of fairness in algorithmic decision-\nmaking is well-recognized, especially as AI systems are increasingly deployed\nin high-stakes domains like criminal justice, hiring, and lending [293], [341],\n[368]. Algorithmic bias can lead to unfair outcomes, as demonstrated in no-\ntable cases like the COMPAS tool for recidivism prediction [369] and issues\nwith Amazon’s hiring algorithms, which were scrapped [17]. Group fairness\nconcepts include independence, ensuring predictions are unaffected by sensi-\ntive attributes; separation, requiring conditional independence of predictions\n126\n 7.2. Uplift modeling\ngiven the outcome; and sufficiency, mandating conditional independence of\nthe outcome given the prediction [341], [370]. The EU’s AI Act underscores\nthe regulatory focus on fairness, aiming to prevent AI from reinforcing dis-\ncrimination through a risk-based framework [18]. Our framework’s flexibility\nin setting fairness constraints enables its use across various risk categories,\nensuring compliance with these requirements and balancing fairness with\nutility [18].\nBalancing utility — such as profit and accuracy — against fairness, like\nequal treatment across demographics, remains a key challenge. [371] and\n[10] show that enhancing fairness often trades off with utility and that mul-\ntiple fairness definitions may even conflict. Traditional fairness assessments\nemphasize output-based metrics, such as demographic parity or equal op-\nportunity, which measure group-level outcome disparities [341]. Recent ap-\nproaches consider the entire distribution of predictions or decisions, incorpo-\nrating statistical moments and exploring distributional fairness [288], [342].\nIn decision-making, fairness extends beyond prediction to both allocation\nfairness—ensuring treatment levels are independent of sensitive attributes\nlike race or gender—and outcome fairness, which ensures equitable results\nacross groups. Although outcome predictions may be imperfect, enforcing\nfairness at the level of expected outcomes has been shown to be an effec-\ntive and ethically justified approach in decision-making settings [10], [372].\nWhile not ideal, these estimates still represent the best available proxy for\nassessing how different groups will benefit from the allocation policy.\nFair-\nness notions, which may conflict, can be incorporated as hard constraints\nduring optimization [10], similar to this work’s approach, or as soft con-\nstraints in multi-objective learning [341]. Recent research, including [343],\nhas largely focused on fairness in observational data with binary sensitive\nattributes, while [373] apply causal inference to multi-stage decision-making\nunder fairness constraints. However, these studies do not address fairness in\nthe context of continuous treatments, a critical gap our research aims to fill.\nValue-driven objective function\nIn managerial decision-making, profit\nmaximization or cost reduction is often the primary goal. Cost-sensitive or\nvalue-driven methods are crucial as they incorporate asymmetric costs and\nbenefits, aligning decisions with these business objectives [14]. Cost-sensitive\napproaches seek to balance costs and benefits, an aspect often ignored by\nstandard methods [357], [360]. In this work, costs and benefits are deter-\nministic while assuming stochastic treatment effects, setting it apart from\nstudies where both costs and benefits are modeled as stochastic [128], [374].\nCost-sensitive methods find applications across various domains, including\ncredit risk [375], fraud detection [25], [376], customer churn [374], business\nfailure prediction [377], and machine maintenance [378].\n127\n Chapter 7: Uplift modeling with continuous treatments\n𝑿\n𝑆\n𝑌\nFigure 7.1: This DAG represents the assumed causal relationships between\nvariables in the training data. X: entity’s pre-treatment features, S: treat-\nment dose, Y : outcome.\n7.3\nProblem formulation\n7.3.1\nNotation\nThe dataset D = {(xi, si, yi)}N\ni=1 has N tuples of pre-treatment features\nX ∈X ⊂Rk, treatment doses S ∈S = [0, 1], and outcomes Y ∈Y = [0, 1],\nwhere xi, si, and yi denote the respective values for instance i. We adopt the\nRubin-Neyman potential outcomes framework [334], [379], originally pro-\nposed for binary treatments Y (0) and Y (1), which can be extended to\nmulti-valued or continuous treatments Y (S) across S ∈[0, 1] [339]. For\neach instance, the potential outcome yi(s) reflects the response to dose s,\ngiven features xi. Data tuples (x, s, y) are generated by distributions p(X),\np(S), and the observed policy Πobs, which assigns treatment si based on xi,\npotentially introducing confounding bias. Figure 7.1 illustrates the assumed\ncausal structure.\nThe CADR function µ ∶S × X →[0, 1] is defined as\nµ(s, x) = E[Y (s) ∣X = x], representing the expected outcome for a given\ndose s and features x. The CADE function τ ∶S × X →[−1, 1] is then\nderived from the CADR and defined as τs(x) = E[Y (s) −Y (0) ∣X = x],\nwhich measures the difference in expected outcomes between dose s and the\nbaseline dose 0. Our notation is summarized in E.1.\n7.3.2\nPrediction step\nDuring the prediction step, our goal is to train a model that can accu-\nrately estimate ˆy(s), providing unbiased estimates of µ(s, x) and τs(x)\nacross the entire domain of S ∈[0, 1]. The estimated CADR for an in-\nstance with features x is defined as ˆµ(s, x) = E [ ˆY (s) ∣X = x]. The esti-\nmated CADE for a given dose s is defined as ˆτs(x) = ˆµ(s, x) −ˆµ(0, x) =\nE [ ˆY (s) −ˆY (0) ∣X = x]. To use the CADE estimates as input for the op-\ntimization step, we discretize the CADRs into δ bins. For a given δ, we\n128\n 7.3. Problem formulation\ndefine D = { d−1\nδ\n»»»»» d = 1, . . . , (δ + 1)} so that the vector ˆτ(x) = (ˆτDd(x))\nδ\nd=1\nthen contains an entity’s δ CADE estimates. We discuss model training and\noutcome estimation in Section 7.4.1.\n7.3.3\nOptimization step\nLet π ∶X →{0, 1}δ be an assignment policy defined for a single entity that\ntakes its features as input and assigns a dose to this entity. In this policy\noutput, the dth element equals 1 if dose s is assigned, and 0 otherwise. Dose\ns corresponds to element d in D (i.e., Dd = s).\nThe cost matrix C ∈RN×δ defines treatment costs, where ci,d is the\ntreatment cost for entity i at dose s, with ci,0 = 0. For each entity i, its\nrow in the cost matrix is denoted by Ci\n=\n(ci,0, ci,1, . . . , ci,δ−1), which\nlists all possible costs for that entity. For simplicity, we assume costs are\ndirectly proportional to dose levels (e.g., a dose of 0.2 has a cost of 0.2).\nThis cost function is modular and can be easily replaced with alternative\ncost structures to suit different scenarios or requirements. The cost of policy\nπ for instance i is:\nΨi(π) = ⟨π (Xi) , Ci⋅⟩\n(7.2)\nThe uplift for an instance i (Ui) is given by the dot product ⟨⋅, ⋅⟩between\nthe assignment policy π and the CADE vector τ. Similarly, the value gain\nfor instance i (Vi) is the same dot product, scaled by the instance-specific\nbenefit bi.\nDue to the fundamental problem of causal inference, historical data pro-\nvides only factual outcomes, not counterfactual outcomes. Thus, in realistic\nscenarios, we must estimate these counterfactual outcomes. In contrast, ex-\nperimental settings can leverage semi-synthetic datasets where the ground\ntruth is known because we control the data-generating process. For a de-\ntailed discussion on the fundamental problem of causal inference, we refer\nreaders to [333].\nWe define three distinct assignment policy values, each\ndepending on the availability and usage of ground-truth versus estimated\nCADR values.\nThe expected policy value (Eq. 7.6) uses estimated CADRs for both the\nassignment policy and evaluation. It represents the anticipated value achiev-\nable under realistic conditions without access to counterfactual ground-t",
  "33": "⟩between\nthe assignment policy π and the CADE vector τ. Similarly, the value gain\nfor instance i (Vi) is the same dot product, scaled by the instance-specific\nbenefit bi.\nDue to the fundamental problem of causal inference, historical data pro-\nvides only factual outcomes, not counterfactual outcomes. Thus, in realistic\nscenarios, we must estimate these counterfactual outcomes. In contrast, ex-\nperimental settings can leverage semi-synthetic datasets where the ground\ntruth is known because we control the data-generating process. For a de-\ntailed discussion on the fundamental problem of causal inference, we refer\nreaders to [333].\nWe define three distinct assignment policy values, each\ndepending on the availability and usage of ground-truth versus estimated\nCADR values.\nThe expected policy value (Eq. 7.6) uses estimated CADRs for both the\nassignment policy and evaluation. It represents the anticipated value achiev-\nable under realistic conditions without access to counterfactual ground-truth\ndata. The prescribed policy value V\npresc\ni\n(Eq. 7.7) employs estimated CADRs\nto determine the assignment policy but uses ground-truth CADRs for eval-\nuation. Policies determined by maximizing V\nexp\ni\nyield an observable policy\nvalue V\npresc\ni\n. Due to differences between estimated and true causal effects,\n129\n Chapter 7: Uplift modeling with continuous treatments\nV\npresc\ni\ntypically diverges from V\nexp\ni\n. The optimal policy value V\nopt\ni\n(Eq. 7.8)\nserves as a benchmark and uses ground-truth CADRs for both policy deter-\nmination and evaluation. This represents the maximum attainable policy\nvalue in a hypothetical setting where all counterfactual dose-response infor-\nmation is known precisely, a condition unattainable in practical applications.\nFormally, these are defined as:\nU\nexp\ni\n= ⟨π(ˆτ(Xi)), ˆτ(Xi)⟩,\n(7.3)\nU\npresc\ni\n= ⟨π(ˆτ(Xi)), τ(Xi)⟩,\n(7.4)\nU\nopt\ni\n= ⟨π(τ(Xi)), τ(Xi)⟩.\n(7.5)\nV\nexp\ni\n= U\nexp\ni\nbi = ⟨π(ˆτ(Xi)), ˆτ(Xi)⟩bi,\n(7.6)\nV\npresc\ni\n= U\npresc\ni\nbi = ⟨π(ˆτ(Xi)), τ(Xi)⟩bi,\n(7.7)\nV\nopt\ni\n= U\nopt\ni\nbi = ⟨π(τ(Xi)), τ(Xi)⟩bi.\n(7.8)\nFor N entities with features X and a budget B, the policy\nB\nΠ∶R+ ×\nX N →{0, 1}(N×δ) can be based on estimated or ground-truth CADRs. The\nexpected optimal policy\nB\nΠ\nexp\n(Eq. 7.9) maximizes value within the budget\nusing estimated CADRs. The prescribed policy assigns treatments according\nto\nB\nΠ\nexp\n, but uses ground-truth CADRs. The ground-truth optimal policy\nB\nΠ\nopt\n(Eq. 7.10), based solely on ground-truth CADRs, is the full-information\nbenchmark.\nB\nΠ\nexp\n= argmax {\nN\n∑\ni=1\nV\nexp\ni\n(π) ∶\nN\n∑\ni=1\nΨi(π) ≤B}\n(7.9)\nB\nΠ\nopt\n= argmax {\nN\n∑\ni=1\nV\nopt\ni\n(π) ∶\nN\n∑\ni=1\nΨi(π) ≤B}\n(7.10)\nThe implementation of this optimization formulation is discussed in Sec-\ntion 7.4.2.\n7.4\nMethodology\nFigure 7.2 summarizes the predict-then-optimize approach for UM with con-\ntinuous treatments under constraints.\n130\n 7.4. Methodology\nY\nS\n(a) Data:\nFactual observations sam-\npled\nfrom\nground-truth\ndose-response curves\nY\nS\n(b) Prediction step:\nDose-response estimation\nbased on factual observa-\ntions\nY\nS\n+\n+\n+\n(c) Optimization step:\nTreatment dose allocation\nbased on estimated dose-\nresponses\nFigure 7.2: Overview of predict-then-optimize approach for UM with con-\ntinuous treatment effects for three entities. The x-axis represents the dose\nS, the y-axis represents the outcome Y . Key elements include true dose-\nresponse curves (solid lines), estimated dose-response curves (dashed lines),\nprescribed policies with their estimated and true outcomes (resp. dotted\nand full-lined circles), and the corresponding expected and prescribed values\n(resp. dotted and full arrows in color). The black ✚’s and arrows correspond\nto the full-information solution.\n7.4.1\nPredictive model for CADR estimation\nTo accurately estimate CADRs ˆµ and their associated CADE estimations\nˆτ, not addressing confounding between S and X in observational data can\nlead to detrimental inaccuracies or biases [333]. A variety of methods for\ndisentangling treatment effects from confounding factors have been proposed\nin the literature [339], [352], [355], and this is not a prime concern in this\npaper. Instead, we consider the predictive methods as off-the-shelf solutions,\nwith methods that do not apply a debiasing scheme also being applied in\nthe experiments. One may refer to, e.g., [338], for assessments of the impact\nof confounding on outcomes.\nFollowing standard practices in causal inference, we make three assump-\ntions for estimating potential outcomes from observational data: Consis-\ntency, Ignorability, and Overlap [380], [381]. These assumptions ensure the\nidentifiability of the CADE function τs(x), i.e., they result in Equation E.5\n(see proof in E.2).\nAssumption 1. Consistency: ∀s ∈S ∶Y = Y (s). This means that for\nany entity with observed treatment dose S = s, the potential outcome for this\ntreatment dose is equal to the factual observed outcome.\nAssumption 2. Ignorability: {Y (s) ∣s ∈S} ⊥⊥S ∣X. This means that,\n131\n Chapter 7: Uplift modeling with continuous treatments\nconditional on an entity’s pre-treatment characteristics, the assigned treat-\nment dose S is independent of the potential outcomes.\nAssumption 3. Overlap: ∀x ∈X such that p(x) > 0, ∀s ∈S ∶0 < p(s ∣\nx) < 1.\nThis states that all entities had a non-zero probability of being\nassigned any dose.\nIn this work, we compare four different learning methods to estimate\nˆµ: an S-learner with random forests as the base learner (S-Learner (rf)),\nan S-learner with a feedforward multi-layer perceptron (MLP) without any\ndebiasing as the base learner (S-Learner (mlp)) [382], DRNet [339], and\nVCNet [340]. Further details on the hyperparameters can be found in E.5.\n7.4.2\nILP for the dose-allocation problem\nDeciding on the optimal treatment dose for each entity, represented by poli-\ncies\nB\nΠ\nexp\nand\nB\nΠ\nopt\n, is equivalent to solving Equations (7.9-7.10). In the case\nwhen only binary doses can be applied, i.e., S ∈{0, 1}, this problem is an\ninstance of the knapsack problem [383]. A sensible heuristic would be to\nrank allocations from high to low uplift until the budget capacity is reached.\nThis approach mirrors the calculation of a traditional Qini curve in a binary\ntreatment setting. In our more general setting, however, any dose S ∈[0, 1]\nis permitted. To formalize this, we introduce two related functions:\nU∶{0, 1}(N×δ) × [−1, 1](N×δ) →R\nV ∶{0, 1}(N×δ) × [−1, 1](N×δ) × RN →R.\nBoth take as input a policy\nB\nΠ∈{0, 1}(N×δ) (expressed as a matrix of decision\nvariables) and a matrix T ∈[−1, 1](N×δ) of CADE values (estimated ˆτ or\nground-truth τ) for the N entities. The function V additionally requires a\nbenefit vector b ∈RN. Note that U is a special case of V in which the\nbenefit vector b is set to all ones. For simplicity, we elaborate on V , with\nthe understanding that this case also encompasses U.\nTo solve Equations (7.9–7.10), we propose the following ILP (Equations\n7.11-7.18), where Vi(π) in the objective is replaced by V\nexp\ni\nwhen finding\nB\nΠ\nexp\nand by V\nopt\ni\nwhen finding\nB\nΠ\nopt\n. Additionally, we compare the ILP to\na greedy ranking heuristic (see Section 7.5.3) Although numerous business\nrequirements exist, we focus on fairness as the main example of constraints\nthroughout this work. Without loss of generalization, we consider one pro-\ntected binary sensitive attribute A (which is also included as an input fea-\nture to train the predictive model), where NA=0 is the index set of entities\n132\n 7.5. Experiments\nwhere the protected attribute A = 0, and NA=1 represents those with A = 1.\nThese two sets are mutually exclusive and collectively exhaustive, so that\n∣NA=0∣+ ∣NA=1∣= ∣N∣. To address fairness in allocation and outcomes,\nwe introduce two slack parameters: ϵDT ∈[0, 1] for allocation fairness and\nϵDO ∈[0, 1] for outcome fairness. A slack parameter value of ϵ = 0 guaran-\ntees perfect fairness between the groups NA=0 and NA=1, while parameter\nϵ = 1 removes this fairness constraint.\nAnalogous to Vi(π)exp, Vi(π)presc, Vi(π)opt (Equations (7.6-7.8), which\nare defined on instance-level), and Equations (7.9-7.10) in Section 7.3, we\nconsider three versions of V each differing in their dependence on ground-\ntruth or estimated CADEs:\nV\nexp=\nN\n∑\ni=1\nδ\n∑\nd=1\n(\nB\nΠ\nexp\ni,d ⋅ˆTi,d ⋅bi),\n(7.19)\nV\npresc=\nN\n∑\ni=1\nδ\n∑\nd=1\n(\nB\nΠ\nexp\ni,d ⋅Ti,d ⋅bi),\n(7.20)\nV\nopt=\nN\n∑\ni=1\nδ\n∑\nd=1\n(\nB\nΠ\nopt\ni,d ⋅Ti,d ⋅bi).\n(7.21)\n7.5\nExperiments\nIn this section, we demonstrate our framework using semi-synthetic data,\naiming to illustrate its applicability and performance in a controlled set-\nting. The main objective is to illustrate the working of both the prediction\nstep, i.e., the inference of continuous treatment effects, and the optimization\nstep. We introduce relevant performance metrics and evaluate the impact\nof adding fairness constraints and adjusting the objective function on policy\nvalue.\n7.5.1\nData\nOur experiments use a semi-synthetic approach based on a real dataset about\ninfant health and development programs (IHDP) [384]. This dataset origi-\nnates from a randomized controlled trial and is frequently used as a bench-\nmark for binary CATE estimation methods [320], [385]. Since we focus on\ncontinuous treatments, we adopt a semi-synthetic approach where both doses\nand outcomes are artificially generated. For this purpose, we follow the es-\ntablished literature [340]. Detailed information about the original dataset\nand the semi-synthetic data generation process is provided in E.3.\n133\n Chapter 7: Uplift modeling with continuous treatments\nmax\nN\n∑\ni=1\nVi(π)\n(7.11)\ns.t.\nN\n∑\ni=1\nΨi(π) ≤B\n(Budget constraint)\n(7.12)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅s ≥(1 −ϵDT ) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅s\n(Alloc. fairness 1)\n(7.13)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅s ≤(1 + ϵDT ) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅s\n(Alloc. fairness 2)\n(7.14)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅τs ≥(1 −ϵDO) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅τs\n(Outc. fairness 1)\n(7.15)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅τs ≤(1 + ϵDO) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅τs\n(Outc. fairness 2)\n(7.",
  "34": "from a randomized controlled trial and is frequently used as a bench-\nmark for binary CATE estimation methods [320], [385]. Since we focus on\ncontinuous treatments, we adopt a semi-synthetic approach where both doses\nand outcomes are artificially generated. For this purpose, we follow the es-\ntablished literature [340]. Detailed information about the original dataset\nand the semi-synthetic data generation process is provided in E.3.\n133\n Chapter 7: Uplift modeling with continuous treatments\nmax\nN\n∑\ni=1\nVi(π)\n(7.11)\ns.t.\nN\n∑\ni=1\nΨi(π) ≤B\n(Budget constraint)\n(7.12)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅s ≥(1 −ϵDT ) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅s\n(Alloc. fairness 1)\n(7.13)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅s ≤(1 + ϵDT ) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅s\n(Alloc. fairness 2)\n(7.14)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅τs ≥(1 −ϵDO) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅τs\n(Outc. fairness 1)\n(7.15)\n1\n∣NA=0∣\n∑\ni∈NA=0\n∑\ns∈D\nπs(Xi) ⋅τs ≤(1 + ϵDO) ⋅\n1\n∣NA=1∣\n∑\ni∈NA=1\n∑\ns∈D\nπs(Xi) ⋅τs\n(Outc. fairness 2)\n(7.16)\n∑\ns∈D\nπs(Xi) = 1, ∀i ∈{1, . . . , N}\n(Exactly one dose)\n(7.17)\nπs(Xi) ∈{0, 1}, ∀s ∈D, ∀i ∈{1, . . . , N}\n(Binary decisions)\n(7.18)\n134\n 7.5. Experiments\n7.5.2\nEvaluation metrics\nThe predict-then-optimize approach is evaluated in two distinct steps, with\nseparate metrics used to assess each step.\nPrediction step\nTo measure the accuracy of the predictive model in esti-\nmating individual dose-response curves, we use the Mean Integrated Squared\nError (MISE, Equation 7.22) [339], [386]. This metric evaluates how closely\nthe predicted dose-response curves match the true dose-response functions\nover the entire range S and hence requires information on semi-synthetic\nground-truth.\nMISE = 1\nN\nN\n∑\ni=1\n∫\ns∈S\n(µ (s, xi) −ˆµ (s, xi))\n2ds\n(7.22)\nOptimization step\nIn the optimization step, several metrics evaluate how\nwell the treatments are allocated, using the estimated CADEs, under the im-\nposed constraints. The primary metric is the total Value (V ), computed as\nthe sum of individual Vi across all N entities. We distinguish V exp (Eq.\n7.19), V presc (Eq. 7.20), and V opt (Eq. 7.21) corresponding to the expected,\nprescribed, and optimal values, respectively.\nThe cost-insensitive version,\nwhere costs and benefits are equal over all entities, is noted as U. Regret is\ndefined as the difference between the full-information optimal value and the\nvalue achieved by the prescribed decision [366]. In other words, it measures\nhow suboptimal the prescribed decision is compared to the optimal solution\nunder ground-truth parameters. When CADRs are perfectly estimated, re-\ngret is zero. However, even with imperfect CADR estimations, regret can\nstill be zero if the assignment policy remains optimal despite inaccuracies in\nthe CADR estimates.\nRegret = V opt −V presc = V (\nB\nΠ (τ), τ) −V (Π(ˆτ), τ)\n(7.23)\nTo make Regret scale-independent, we also include the normalized version:\nRegretNB = V\nopt −V\npresc\nV\nopt\n=\nV (\nB\nΠ (τ), τ) −V (\nB\nΠ (ˆτ), τ)\nV (\nB\nΠ (τ), τ)\n(7.24)\nAdditionally, we assess performance across a range of budgets. The Value\nCurve plots the function V (B) = ∑N\ni=1 Vi(πB), for B ∈[0, Bmax], where\nBmax is defined as the budget required when treating all entities with dose\n135\n Chapter 7: Uplift modeling with continuous treatments\ns = 1. We report the Area Under the Value Curve (AUVC) when costs and\nbenefits are instance-dependent and Area Under the Uplift Curve (AUUC)\nwhen costs and benefits are equal for all instances. In a traditional binary\ntreatment setting, the horizontal axis of the uplift curve typically represents\nthe ‘cumulative proportion of entities targeted’ [364]. In our case, this trans-\nlates to ‘budget used’ and differs in that the budget can target entities with\nfiner granularity. For example, while a value of 5 on the x-axis in the tradi-\ntional setting corresponds to 5 entities targeted, in our case, a budget of 5\ncould represent 10 entities receiving smaller doses.\n7.5.3\nResults and discussion\nIn this section, we describe the experimental evaluation of our proposed\npredict-then-optimize framework, focusing on its ability to handle continu-\nous treatments, optimize dose allocation under various constraints, and bal-\nance possibly conflicting objectives such as fairness and policy value. In all\nexperiments, the number of bins δ is fixed to 10, motivated by E.4, and the\nlevel of confounding bias is constant. All experiments are implemented using\nPython 3.9 and can be reproduced with the code available on Github. All\nILPs are solved using Gurobi 11.0. In all experimental scenarios presented,\nthe ILP solver converged, guaranteeing optimal solutions for the given prob-\nlem instances. Solver convergence was systematically verified by checking\nthe solver status after each optimization run.\nExperiment 1: Performance of dose-response estimators\nThe first experiment aims to compare the performance of different dose-\nresponse estimators in terms of both prediction accuracy and capacity for\ngood dose allocation. We evaluate four dose-response estimators: S-Learner\n(rf) and S-Learner (mlp) [382], DRNet [339], and VCNet [340]. For each\nestimator, we conduct an internal 5-fold cross-validation loop, tuning hyper-\nparameters across a grid search (details in E.5). The model with the lowest\naverage MSE on factual outcomes over all folds is selected. In this experi-\nment, we do not consider cost-sensitivity or fairness constraints. The bud-\nget is incrementally raised to observe the corresponding variation in U\npresc\nB\n.\nEach model’s estimated CADRs are visualized in E.6.1.\nWe allocate treatments using two distinct methods. The first approach\nutilizes the ILP introduced in Section 7.4.2. The second employs a heuristic\ninspired by the multi-treatment framework [347]. Here, each discrete dose\nis treated as a separate intervention and, formally, the optimal treatment\nfor an entity with features xi is argmax(ˆτ(xi)). Entities are then ranked\naccording to the uplift of their optimal treatment, and doses are assigned\n136\n 7.5. Experiments\n0\n50\n100\n150\n200\n250\nBudget\n0\n5\n10\n15\n20\n25\n30\n35\n40\nU\npresc\nMethods\nS-Learner (rf)\nS-Learner (mlp)\nDRNet\nVCNet\nILP\nHeuristic\nOptimal\nFigure 7.3: This figure shows U presc for four dose-response estimators as\nthe available budget levels increase. Doses are allocated with an ILP and a\nheuristic approach.\ngreedily in that order until the budget is depleted.\nThe results are displayed\nin Figure 7.3 and Table 7.3. The analysis provides two insights.\nFirst, compared to typical uplift curves from binary treatment scenarios\n(based on a ranking heuristic), both the multi-treatment heuristic and the\nILP with continuous treatments as input do not fully utilize the budget.\nAround a budget of 140, they find a point where further treatment becomes\ncounterproductive —i.e., it is more valuable to avoid overtreating certain\nentities— due to the non-monotonic nature of CADRs. From a budget of\n140 onwards, per estimator, the heuristic and ILP approach find the same\nsolution (i.e., selection of the dose with maximum estimated CADE) and\nTable 7.3: This table shows the MISE (prediction step) and AUUC for bud-\ngets 140 and 250 (optimization step) across four dose-response estimators.\nThe AUUC values are normalized against the optimal full-information solu-\ntion. The best results are highlighted in bold and the second-best in italics.\nEstimator\nMISE\nAUUC@140\nAUUC@250\nHeur.\nILP\nHeur.\nILP\nS-Learner (rf)\n0.060\n0.837\n0.892\n0.926\n0.951\nS-Learner (mlp)\n0.044\n0.813\n0.829\n0.877\n0.884\nDRNet\n0.049\n0.649\n0.799\n0.729\n0.797\nVCNet\n0.055\n0.827\n0.827\n0.922\n0.922\nOptimal\n0.000\n1.000\n1.000\n1.000\n1.000\n137\n Chapter 7: Uplift modeling with continuous treatments\nthey converge in terms of U presc.\nUnlike the heuristic approach, which\nranks binary treatment effects from high to low and selects the top k within\nthe budget, continuous treatment effects have a larger search space, allowing\nfor partial treatments. This is evident in Figure 7.3 where the Uplift curves\nflatten around a budget of 140. Therefore, in Table 7.3, we include AUUC\nnot only for the full budget (i.e., @250) but also for a partial budget of 140.\nAround this point, for all methods considered (and the full-information op-\ntimal solution), the policy value remains constant despite increasing budget.\nSecond, Table 7.3 shows a misalignment between the quality of CADR\nestimation (measured by MISE during prediction) and the quality of treat-\nment allocation (measured by AUUC during the optimization step). MISE\ndoes not fully capture downstream task performance, as errors in critical\nareas of the curve have a greater impact on treatment allocation than those\nin less important regions — which holds true for both heuristic and the ILP\napproaches.\nFor example, although S-Learner (mlp) has the lowest MISE, its final\nallocations rank second or third. DRNet shows the second-best MISE but\nperforms worst in terms of AUUC at both budget points, irrespective of the\ntreatment assignment method. Conversely, S-Learner (rf), despite its high\nMISE, outperforms all others in uplift effectiveness.\nThis suggests that the\nbest inference methods do not always result in optimal decision-making, a\nnotion similarly observed in cost-sensitive learning literature, where maxi-\nmizing predictive accuracy does not necessarily yield the highest profit [128].\nFigure E.2a illustrates this phenomenon: the S-Learner (rf) produces low-\nquality estimates for S ∈[0.4, 0.6], resulting in a relatively high MISE.\nHowever, errors in this region have minimal impact on optimization since\nthey correspond to low CADEs and are not selected anyway. DRNet, on\nthe other hand, has relatively small errors overall but struggles significantly\nin the crucial high-impact region of S ∈[0.9, 1.0], where the ground-truth\nCADR sharply declines (see Figure E.2c). Thus, accurate dose allocation\ndoesn’t strictly require perfect predictions, provided the predictions capture\nthe critical areas effectively.\nConversely, perfect treatment allocation does\nnot requi",
  "35": " its high\nMISE, outperforms all others in uplift effectiveness.\nThis suggests that the\nbest inference methods do not always result in optimal decision-making, a\nnotion similarly observed in cost-sensitive learning literature, where maxi-\nmizing predictive accuracy does not necessarily yield the highest profit [128].\nFigure E.2a illustrates this phenomenon: the S-Learner (rf) produces low-\nquality estimates for S ∈[0.4, 0.6], resulting in a relatively high MISE.\nHowever, errors in this region have minimal impact on optimization since\nthey correspond to low CADEs and are not selected anyway. DRNet, on\nthe other hand, has relatively small errors overall but struggles significantly\nin the crucial high-impact region of S ∈[0.9, 1.0], where the ground-truth\nCADR sharply declines (see Figure E.2c). Thus, accurate dose allocation\ndoesn’t strictly require perfect predictions, provided the predictions capture\nthe critical areas effectively.\nConversely, perfect treatment allocation does\nnot require flawless predictions; effective allocation is possible even with im-\nperfect dose-response estimates. A further analysis of runtime for increasing\nproblem sizes is provided in E.6.2. While the heuristic approach scales well\nfor increasing problem sizes, it does not allow additional side constraints like\nfairness, which are allowed for the ILP.\nExperiment 2: Fairness trade-offs in treatment allocation\nThe second experiment examines the trade-offs between policy value and\nbusiness requirements as constraints, with a particular focus on fairness.\nSpecifically, we examine both allocation fairness and outcome fairness, an-\nalyzing the impact of tightening or loosening these fairness constraints on\n138\n 7.5. Experiments\n1.00\n0.10\n0.01\nϵDT\n1.00\n0.10\n0.01\nTreatment fairness\nTreatment fairn.\nOpt. constr.\n(a) Allocation fairness\n1.00\n0.10\n0.01\nϵDO\n1.00\n0.10\n0.01\nOutcome fairness\nEst. outc. fairn.\nTrue outc. fairn.\nOpt. constr.\n(b) Outcome fairness\nFigure 7.4: The effect of slack parameters ϵDT and ϵDO on, respectively,\ndisparate treatment (allocation fairness) and disparate outcome (outcome\nfairness). Both panels use a logarithmic scale.\noverall policy value. In this experiment, we fix the budget and disregard\ncost-sensitivity. We use the S-Learner (rf), which performed best in terms\nof AUUC, as the dose-response estimator. We vary two key fairness con-\nstraints: allocation fairness, which measures the disparity in assigned doses\nbetween groups, and outcome fairness, which reflects the difference in treat-\nment outcomes. These constraints are regulated by two slack parameters:\nϵDT for allocation fairness and ϵDO for outcome fairness.\nFigure 7.4a shows the effect of parameter ϵDT on treatment fairness.\nSince treatment allocation is deterministic, there is no difference between\nthe ground truth and estimates. Treatment fairness can be adjusted to any\ndesired level by controlling ϵDT , making it straightforward to manage within\nthe given constraints.\nFigure 7.4b examines outcome fairness as regulated by the parameter\nϵDO, revealing a distinct challenge compared to treatment fairness: a discrep-\nancy between estimated and ground truth fairness. This divergence arises\nbecause optimization is based on estimated CADEs rather than true values.\nAlthough tightening ϵDO (illustrated by the red line) can restrict estimated\ndisparities, it does not guarantee alignment with ground truth fairness (blue\nline). This misalignment is driven by the quality of the CADE estimates and\nunderscores a broader challenge in predictive analytics—namely, that fair-\nness in predictions does not always translate into fairness in actual outcomes.\nNevertheless, we argue that incorporating constraints on expected outcomes\nacross groups remains a principled and ethically sound design choice. Even\nwhen individual-level outcomes are uncertain, they still provide the most in-\nformative basis available for assessing how different groups may benefit from\nthe allocation policy.\n139\n Chapter 7: Uplift modeling with continuous treatments\n10−2\n10−1\n100\nϵDT\n10−2\n10−1\n100\nϵDO\n0.2\n0.4\n0.6\n0.8\nNormalized U prescr\nFigure 7.5: The x-axis represents the fairness constraint parameter for dis-\nparate treatment (ϵDT ), and the y-axis represents the fairness constraint\nparameter for disparate outcome (ϵDO). Lower values of ϵ indicate stricter\nfairness constraints. U presc is normalized between 0 and 1 and is aggregated\nover multiple budgets (from 25 to 250 in increments of 25). The averaged\nnormalized U presc is color-coded, where blue means lower and yellow means\nhigher.\nFigure 7.5 presents the trade-off between fairness constraints and U presc.\nStricter enforcement of both treatment and outcome fairness leads to lower\nuplift. The effects of these constraints differ; without an outcome constraint,\nthe treatment constraint can be tightened with barely harming the policy\nvalue. Conversely, tightening the outcome constraint reduces policy value,\nespecially when the treatment constraint is also strict. Note that finding a\nsolution to the ILP to satisfy both fairness constraints is always possible,\nwith the trivial available option of not allocating any treatment.\nThe setup of Figure 7.6 is similar to the one in Figure 7.5, but also\nexamines varying data-generating processes where the two groups defined by\nthe protected feature are more different. This is controlled by the parameter\nγ, with higher values indicating greater differences in ground-truth average\ntreatment effects (ATEs) between the two protected groups. Figure 7.6 plots\nthe results for three increasing values of γ (the case for γ = 0 is displayed\nin Figure 7.5). As the difference in ATEs between the groups increases, the\neffect of slack parameters ϵDT and ϵDO on the objective value (normalized\nU presc, averaged over different budgets) becomes more pronounced. Strict\nslack parameters are tolerable without significant value loss when the groups\nare similar (γ = 0, Figure 7.5).\nIn contrast, even mild slack parameters\nsignificantly affect the objective value when ATEs are most different (γ = 10,\nPanel 7.6c).\n140\n 7.5. Experiments\n10−2\n10−1\n100\nϵDT\n10−2\n10−1\n100\nϵDO\n0.2\n0.4\n0.6\n0.8\nNormalized U prescr\n(a) γ = 1\n10−2\n10−1\n100\nϵDT\n10−2\n10−1\n100\nϵDO\n0.2\n0.4\n0.6\n0.8\nNormalized U prescr\n(b) γ = 5\n10−2\n10−1\n100\nϵDT\n10−2\n10−1\n100\nϵDO\n0.2\n0.4\n0.6\n0.8\nNormalized U prescr\n(c) γ = 10\nFigure 7.6: Fairness constraints and varying ground-truth ATEs. This figure\nexamines the link between the effect of the protected feature on the ground-\ntruth ATE and the ease of achieving fairness. One panel is provided for\neach value of γ ∈{1, 5, 10} (the case for γ = 0 is displayed in Figure 7.5) and\nis aggregated over multiple budgets (from 25 to 250 in increments of 25),\nusing the S-learner with random forests as the base learner. The averaged\nnormalized U presc is color-coded where blue means lower and yellow means\nhigher.\nExperiment 3: The impact of cost-sensitivity on utility\nIn many applications, the ultimate goal is profit or cost reduction, not just\nuplift. This experiment demonstrates that optimizing for the true objec-\ntive (value) yields different policies than optimizing for uplift alone.\nThis\nexperiment explores the effect of incorporating cost-sensitivity into policy\noptimization.\nBuilding upon the previous experiments, we now account\nfor cost-sensitivity in the adapted objective function of ILP, considering\ninstance-dependent treatment costs, C, and outcome benefits, b. Figure 7.7b\ndemonstrates that policies optimized with cost-sensitivity (green) perform\nbetter than cost-insensitive ones (blue) in terms of the cost-sensitive value\n(V presc), particularly at lower budget levels. Conversely, Figure 7.7a shows\nthe opposite scenario, emphasizing the need to align the optimization process\nwith the intended objective at hand.\nThis experiment highlights the importance of cost-sensitivity in optimiz-\ning treatment allocation under constrained budgets. By taking into account\ntreatment costs and outcome benefits, cost-sensitive policies achieve higher\nvalues in cost-sensitive metrics, especially when resources are limited and not\nall entities can receive full treatment. These policies prioritize entities with\nthe highest benefit-to-cost ratios, improving allocation efficiency. However,\nthey underperform on cost-insensitive utility metrics, underlining the need\nfor alignment between optimization objectives and specific decision-making\ngoals. The cost-sensitive policy, as expected, sacrifices some uplift if it leads\nto higher net value. We see differences in who gets treated: for example,\n141\n Chapter 7: Uplift modeling with continuous treatments\n0\n50\n100\n150\n200\n250\nBudget\n0\n10\n20\n30\n40\nU prescr\nCost-insensitive opt.\nCost-sensitive opt.\nOptimal solution\n(a) Cost-insensitive U\npresc across\nincreasing budget.\n0\n50\n100\n150\n200\n250\nBudget\n0\n10\n20\n30\n40\nV prescr\nCost-insensitive opt.\nCost-sensitive opt.\nOptimal solution\n(b) Cost-sensitive V\npresc across\nincreasing budget.\nFigure 7.7: Comparison of cost-sensitive and cost-insensitive optimization,\nillustrating differences in performance based on U presc and V presc.\nThe\nresults underscore the importance of aligning the optimization objective with\nthe downstream task.\nsome individuals with high uplift but low benefit may not be treated un-\nder the value-based policy, whereas they would under an uplift-only policy\nThese findings are relevant for applications such as healthcare, lending,\nand human resource management, where treatment costs and benefits vary\nacross entities, regardless of whether the goal is to maximize profitability\nor allocate scarce resources more efficiently. Furthermore, the modularity\nof the predict-then-optimize framework facilitates easy adaptation to chang-\ning objectives or constraints without requiring the retraining of predictive\nmodels. This makes it well-suited for dynamic environments where goals\nand resource availability evolve over time. An extension of this experiment\ncould explore the",
  "36": "Comparison of cost-sensitive and cost-insensitive optimization,\nillustrating differences in performance based on U presc and V presc.\nThe\nresults underscore the importance of aligning the optimization objective with\nthe downstream task.\nsome individuals with high uplift but low benefit may not be treated un-\nder the value-based policy, whereas they would under an uplift-only policy\nThese findings are relevant for applications such as healthcare, lending,\nand human resource management, where treatment costs and benefits vary\nacross entities, regardless of whether the goal is to maximize profitability\nor allocate scarce resources more efficiently. Furthermore, the modularity\nof the predict-then-optimize framework facilitates easy adaptation to chang-\ning objectives or constraints without requiring the retraining of predictive\nmodels. This makes it well-suited for dynamic environments where goals\nand resource availability evolve over time. An extension of this experiment\ncould explore the inclusion of application-dependent and entity-dependent\ntreatment cost functions and their interaction with the previously researched\nfairness constraints, adding complexity to the optimization process.\n7.6\nConclusion, limitations, and further research\nUM is extensively discussed in the literature, with applications such as lend-\ning, healthcare, HR, marketing, and maintenance. However, typically, no\nclear distinction between UM and CATE estimation is made, where CATE\nestimation should be viewed as a component of the broader UM field and is\nused as input for an optimization task. By clearly distinguishing these steps,\nour approach accommodates more complex treatments, as demonstrated in\n142\n 7.6. Conclusion, limitations, and further research\nour focus on continuous treatments, while allowing flexible integration of\nconstraints like fairness considerations and cost-sensitive objectives.\nOur\nmain contributions are (i) defining UM, (ii) extending it to handle contin-\nuous treatments with customizable constraints and objectives, (iii) defining\nfairness considerations as explicit ILP constraints, and (iv) demonstrating\nits capabilities through experiments. This framework’s flexibility enables the\nuse of pre-trained models and avoids challenges in decision-focused learning\nby allowing constraint integration without retraining, particularly advanta-\ngeous when, for example, unexpected operational constraints arise. Contin-\nuous treatments allow fine-grained dose-based interventions, better suited\nfor various applications, as shown in Table 7.1.\nOur contributions are supported by the formal outlining and working of\nour framework in Sections 7.2 and 7.3, as well as validation by a series of\nexperiments in Section 7.5. Experiment 1 shows the benefits of continuous\ntreatments in an uplift setting, enabling decision-makers to maximize the\ntotal benefit of treatment allocation. Interestingly, this experiment shows\nthat the most accurate predictive models are not always the best suited for\nthe downstream treatment allocation. Experiment 2 explores the trade-offs\nbetween fairness constraints and policy value, revealing that stricter fairness\nenforcement, both in treatment allocation and outcomes, reduces overall\nutility, particularly when there are strong differences in ground-truth ATEs\nbetween the two groups considered. This reflects the broader challenge in\nalgorithmic decision-making, where fairness may come at the cost of utility.\nFinally, Experiment 3 highlights the importance of aligning optimization\nwith the specific objectives of the decision-making context, whether focusing\non maximizing uplift or value.\nWhile effective, our framework has limitations. The ILP approach suffers\nfrom limited scalability. In contrast, the proposed heuristic handles larger\nproblem sizes more efficiently. However, it may yield suboptimal solutions\nand does not account for side constraints such as fairness.\nAdditionally,\nthe assumption of deterministic costs simplifies the problem but does not\naccount for stochastic or uncertain treatment costs, which are present in\nother settings [361]. Another limitation is the focus on single-phase treat-\nments, where many real-world scenarios involve sequential decisions.\nWe\nalso address fairness at the group level, leaving individual-level fairness for\nfuture exploration. Finally, experiments are based on a single dataset and\ndata-generating process, which may limit generalizability.\nThis work is the first step in defining and providing an initial solution for\nUM with continuous treatments. Future research could extend this work by\nexploring varied allocation schemes, treatment characteristics, or sequen-\ntial decision-making using dynamic policy or reinforcement learning.\nIn\ncases of time dependencies, treatments are administered multiple times over\n143\n Chapter 7: Uplift modeling with continuous treatments\na period, or treatment effects change over time, dynamic policy learning\nor reinforcement learning methods could be employed to handle sequential\ntreatments and long-term effects [387]. Extending the framework to accom-\nmodate multiple treatment types, whether mutually exclusive or not, would\ncreate new opportunities in situations where multiple intervention types are\navailable [367].\nFurther research could investigate treatment effects in a\nnetwork setting where spillover effects take place, making an entity’s out-\ncomes dependent on others’ assigned doses and violating the stable unit\ntreatment value assumption [388].\nAdditionally, while this study focuses\non one-dimensional dose-based treatments, other applications may involve\na high-dimensional treatment space [389]. When optimizing under uncer-\ntainty, incorporating conformal predictions, where CATE distributions or\nintervals around CADRs are estimated instead of point estimates, could\nbetter inform personalized decision-making [390], [391]. Finally, integrating\nDFL approaches, which, unlike our predict-then-optimize method, combine\nprediction and optimization in a single step, could potentially further im-\nprove policy outcomes by training models to directly make predictions that\nlead to better decisions [366]. One could also explore hybrid approaches ––\nfor example, incorporating fairness constraints directly into model training\n(a partial DFL approach) while still performing an optimization step for\nother parameters or constraints. Such methods might alleviate some of the\nburden on the post-prediction optimization\n144\n Epilogue\n145\n  8\nIndustry co-creation\nThis dissertation was developed as part of a Baekeland mandate, a funding\ninitiative supported by VLAIO that encourages close collaboration between\nacademic institutions and industry partners. This research was conducted in\npartnership with Acerta, a leading Belgian HR services provider offering so-\nlutions in, e.g., payroll, social security, legal services, and talent management\nto a broad range of clients, including entrepreneurs, SMEs, and large organi-\nzations across Belgium. The structure of this dissertation aligns largely with\nthe original project proposal. Although specific research questions evolved\nslightly during the course of the project, the overarching phases remain rec-\nognizable: a descriptive phase, a predictive phase, and a prescriptive phase.\nOver four years, I had the opportunity to divide my efforts between KU\nLeuven and Acerta. At the university, my focus was on the conceptual and\nmethodological development of advanced analytics techniques. At Acerta,\nI collaborated closely with BI specialists, HR analytics professionals, and\nvarious HR managers to understand their practical needs, identify pressing\nchallenges, and iteratively transform research insights into practical tools —\nand conversely, to let practical challenges guide the research agenda.\nThe dual nature of this mandate, academic and industrial, was therefore\nnot simply parallel, but inherently synergistic. Research questions frequently\nemerged directly from practical HR needs identified by Acerta and its clients,\nand these questions were refined through feedback from practitioners. This\ncollaborative setup demanded research that was scientifically rigorous yet\npractically relevant.\nMoreover, it provided an opportunity to bridge the\noften-cited gap between academic sophistication and practical applicability.\nThis chapter, therefore, reflects on the iterative feedback loop between\nacademia and industry, highlighting how practical challenges inspire novel\nresearch directions and how research outcomes inform the development of\nnew analytical tools. Additionally, it acknowledges that the valuable col-\nlaboration with Acerta, combined with data contributions from Acerta and\nits clients, significantly shaped the trajectory and outcomes of this doctoral\nwork.\nIn what follows, we assess how the three applied chapters (Part I) yielded\ninsights and resulted in tools of practical relevance for Acerta and its clients.\nFurthermore, we discuss potential avenues for implementing and valorizing\n147\n Chapter 8: Industry co-creation\nthe methodological contributions outlined in Part II. In a structured manner,\nwe discuss operationalization, advantages, challenges, valorization, and the\npotential to scale to other clients.\n8.1\nEmployee journey mapping\nChapter 2 presents a case study with Acerta where process discovery tech-\nniques are applied to HR event logs to generate EJMs, revealing the com-\nplexity of internal mobility. These maps highlight unexpected career paths,\nstepping stones to key roles, and hard-to-fill positions, offering a dynamic\nand flexible, descriptive tool for managing employee mobility.\nOperationalization\nIn addition to being implemented internally at Ac-\nerta, the tool was also deployed with two of Acerta’s clients. The opera-\ntionalization uses a Power BI dashboard that allows users to interactively\nfilter and select specific data slices to display certain mobility paths. Since\nPower BI does not natively generate process ",
  "37": "ues for implementing and valorizing\n147\n Chapter 8: Industry co-creation\nthe methodological contributions outlined in Part II. In a structured manner,\nwe discuss operationalization, advantages, challenges, valorization, and the\npotential to scale to other clients.\n8.1\nEmployee journey mapping\nChapter 2 presents a case study with Acerta where process discovery tech-\nniques are applied to HR event logs to generate EJMs, revealing the com-\nplexity of internal mobility. These maps highlight unexpected career paths,\nstepping stones to key roles, and hard-to-fill positions, offering a dynamic\nand flexible, descriptive tool for managing employee mobility.\nOperationalization\nIn addition to being implemented internally at Ac-\nerta, the tool was also deployed with two of Acerta’s clients. The opera-\ntionalization uses a Power BI dashboard that allows users to interactively\nfilter and select specific data slices to display certain mobility paths. Since\nPower BI does not natively generate process maps, a third-party module\n(through a paid license) was integrated to visualize the EJMs. A screen-\nshot of such an implementation is provided in Appendix F.1 (Figure F.1),\nillustrating how users can dynamically filter data and regenerate EJMs for\nvarious subsets of employees.\nAdvantages\nA key advantage of the EJM mapping exercise is its ease of\ndeployment, stemming from relatively light data requirements. The input is\nan event log that captures historical job roles — covering positions, transi-\ntions, and timings — within the organization. Once the Power BI template is\nset up, the tool can be readily applied to provide new insights with minimal\nadditional effort.\nOn one hand, it delivers directly actionable insights — for example, quan-\ntifying how many employees move from Role A to a lateral move versus how\nmany take a vertical promotion. On the other hand, it uncovers patterns that\nmay lead to follow-up questions. Indeed, this mapping exercise sparked inter-\nest in investigating paths leading to employee turnover, which subsequently\nmotivated the deeper exploration of turnover prediction in Chapter 3.\nChallenges\nAlthough the method itself is relatively simple, the main chal-\nlenge in implementing EJMs lies in the quality of longitudinal data. Because\nEJMs trace job histories over time, maintaining consistent and clean records\nacross years is non-trivial. Job titles may evolve, roles may split or merge,\nand inconsistent naming conventions can introduce noise.\nThese changes\nmust be handled carefully — for example, a modified job title should not be\nmistaken for a job transition. Accurate interpretation of mobility patterns\ndepends on reliably distinguishing true role changes from data artifacts.\n148\n 8.2. Turnover prediction\nValorization\nThe value of this EJM mapping exercise is twofold. First,\ndue to its low barrier to entry (in terms of data requirements and simplic-\nity of modeling using a directly-follows graph miner), it can be rolled out\nquickly to new clients as a part of an HR analytics toolbox. This speed and\nease make it an attractive “appetizer” for organizations new to data-driven\nHR: it provides tangible insights with minimal upfront investment. Second,\nthe EJMs themselves deliver immediate, concrete insights.\nFor example,\norganizations can see internal mobility rates, typical career pathways, and\nunexpected detours. These insights are valuable for HR strategy, such as\nidentifying why certain roles struggle to retain employees or how to create\nmore pathways for lateral career development. Additionally, positioning the\nEJM tool as an accessible introduction can inspire clients and stakeholders\nby showing the possibilities of more advanced analytics, potentially paving\nthe way for follow-up projects (e.g., predictive or prescriptive analytics ini-\ntiatives).\nPotential to scale to other clients\nBecause the EJM mapping relies\non fairly standard HR data (for the three implementations, it was readily\navailable from an ERP system with minimal preprocessing), extending it\nto other clients is rather straightforward once a template is in place. The\nmain time investment is in ensuring that data is properly formatted and\ncleaned to avoid misleading results.\nWith Power BI dashboards and the\nprocess mining extension, new client data can be plugged in to generate\norganization-specific EJMs. Thus, this tool is readily scalable, and Acerta\nhas indeed identified it as a valuable service offering. Its low implementation\ncomplexity and high interpretability make it easy to demonstrate value to\nclients, either as a standalone analysis or as part of a larger HR analytics\nengagement.\n8.2\nTurnover prediction\nChapter 3 tackles employee turnover prediction, a critical issue due to the\nhigh direct and indirect costs of losing employees. It contributes a scop-\ning review highlighting inconsistencies in the literature and a benchmarking\nexperiment comparing 14 classification methods across 9 datasets.\nOperationalization\nIn the industrial context at Acerta, the turnover pre-\ndiction work took two forms:\n1. Client-specific deep dive: A consulting project with a particular client,\nfocusing deeply on their internal data. This involved collecting and\npooling data from multiple sources for that client, including demo-\ngraphic data and even assessment reports from supervisors. The goal\n149\n Chapter 8: Industry co-creation\nwas to tailor a predictive model to the client’s unique context and\nanswer their specific questions about turnover risk.\n2. Generalized model across clients: A broader turnover prediction model\nwas trained on aggregated data from over 600 distinct organizations\n(for which Acerta manages payroll). This model is trained on a vast\npooled dataset covering 600,000+ employee-year observations between\n2019 and 2023.\nIn both cases, the approach followed a typical ML pipeline: data collec-\ntion and cleaning, followed by predictive modeling with a post-hoc model\nexplainer for interpretability (SHAP values in this case). Results were again\nreported through a Power BI dashboard, featuring interactive filters (by\nindividual, role, department, etc.)\nand visualizations to examine differ-\nences across various demographic or job-related subsets. Figure F.2 in Ap-\npendix F.2 shows an example Power BI output with a beeswarm plot of\nSHAP values, illustrating model explainability.\nAdvantages\nThe emphasis on explainability in the industrial rollout was\na notable advantage.\nUnlike the purely predictive focus of the academic\nstudy in Chapter 3, the industry implementation highlighted why a model\npredicts a certain employee as high risk. Reporting SHAP values increased\nthe validity and credibility of the model in the eyes of HR professionals and\nmanagement, which is crucial for adoption – stakeholders are more likely to\ntrust and act on the model’s predictions if they understand the drivers (e.g.,\ntenure, recent promotion, performance metrics) behind those predictions.\nFor the client-specific deep dive, the advantage was working with rich,\ncontext-specific data, enabling very tailored insights. The model could in-\ncorporate a wide array of features (including those unique to the client’s HR\nsystems or processes) and thus answer nuanced questions. For the general-\nized model across clients, the advantage lies in its scalability and immediate\nusefulness – a company can receive insights about turnover risk without\nhaving to provide their own data upfront. Acerta consultants could use this\nbroad model to engage with a client by showing them patterns derived from\nindustry-wide data (“This is what we see broadly; if you provide your data,\nwe can refine these insights for your organization”). This approach leverages\nthe shared structure across many companies and can serve as a conversation\nstarter or diagnostic tool.\nChallenges\nSeveral challenges emerged.\nData availability and integration (for the deep dive): Collecting and pre-\nprocessing the client’s data was labor-intensive. Data resided in different\nsystems (HR databases, performance evaluations, several isolated CSV files,\netc.), and had to be carefully merged. Legacy system incompatibilities (e.g.,\n150\n 8.2. Turnover prediction\nchanging employee ID formats over time) meant extra work to align records.\nAlso, fields were sometimes inconsistently filled or changed definitions over\nthe years, leading to missing or incoherent data for older records.\nData privacy and governance: Using detailed HR data triggered privacy\nconcerns. Legal approval was needed to ensure compliance with GDPR and\nthat employees’ data could be used for analytics. This cautious approach\nwas necessary, but it slowed down the project’s start.\nData quality and consistency: Even beyond integration, ensuring clean\nlongitudinal data was difficult. Similar to the EJM case, when working across\nmany years, changes in how data was recorded (job titles, departments, etc.)\ncould introduce pollution. For example, a departmental reorganization could\nmake it appear as though some people “left” when in fact only department\nnames changed. Vigilant cleaning and interpretation were required to avoid\nfalse signals.\nComplexity of the general model:\nThe broad model, while powerful\nin scale, only included features common across all clients (mostly payroll-\nrelated variables, roughly 30 features). This means it might overlook company-\nspecific predictors of turnover that are not captured in a generalized dataset.\nThere is a trade-off between (i) including many organizations, leading to\nmany entries with a lower number of common variables, and (ii) analyzing\njust one organization, but including more detailed variables.\nValorization\nThe value realized from these projects differs by approach.\nThe deep dive for this client resulted in a detailed, tailored report that\naddressed their key concerns. By closely analyzing their own data, we were\nable to highlight specific retention challenges — such as risk concentrated\nin certain departments or career stages — with clear ",
  "38": " some people “left” when in fact only department\nnames changed. Vigilant cleaning and interpretation were required to avoid\nfalse signals.\nComplexity of the general model:\nThe broad model, while powerful\nin scale, only included features common across all clients (mostly payroll-\nrelated variables, roughly 30 features). This means it might overlook company-\nspecific predictors of turnover that are not captured in a generalized dataset.\nThere is a trade-off between (i) including many organizations, leading to\nmany entries with a lower number of common variables, and (ii) analyzing\njust one organization, but including more detailed variables.\nValorization\nThe value realized from these projects differs by approach.\nThe deep dive for this client resulted in a detailed, tailored report that\naddressed their key concerns. By closely analyzing their own data, we were\nable to highlight specific retention challenges — such as risk concentrated\nin certain departments or career stages — with clear implications for their\nHR strategy. While the insights were highly useful, producing them required\nsignificant time and effort. Replicating this level of analysis for other clients\nwould involve a similarly heavy lift, making it valuable but not easily scal-\nable. This kind of work is most appropriate when a client is highly motivated\nto understand their attrition and is prepared to share in-depth data.\nThe generalized model offers clear value in its broad applicability.\nA\nclient can benefit from industry-wide insights with minimal effort on their\npart, which is a strong selling point. For Acerta, it can serve as a relatively\nlow-effort way to engage clients: “Here are some insights from our large-scale\nmodel; if you’re interested, we can dive deeper with your data to improve\nthese insights.” The broad model can deliver directly useful findings (like\nidentifying common turnover risk factors in the sector) and doubles as a\nmarketing or pre-sales tool to trigger further interest in advanced, tailored\nanalytics services.\nIn both cases, the integration of the turnover prediction tool into Acerta’s\n151\n Chapter 8: Industry co-creation\nofferings strengthens their HR analytics portfolio, either by immediately en-\nhancing decision-making or by laying the groundwork for deeper, customized\nprojects.\nPotential to scale to other clients\nScaling the turnover prediction so-\nlution to other clients depends on the approach.\nSince it is already trained on a wide variety of companies, it can be\napplied to a new client whose payroll is managed by Acerta to give an initial\nrisk assessment. Nearly any organization tracked in the pooled dataset can\nget a “turnover risk dashboard” immediately.\nMaintaining and updating\nthis model with new data over time can further improve its accuracy and\nrelevance.\nThe client-specific path requires more effort. To replicate the deep dive\nfor another client, that client must commit to gathering and sharing their\ndata. This doesn’t scale seamlessly because each new deep dive will face sim-\nilar hurdles (data integration, privacy checks, custom modeling). However,\nif a client sees the value in the generalized insights, they might be convinced\nto undertake this effort for a more precise analysis. Over time, as more such\nprojects are done, Acerta could streamline the process (develop standard\ndata request templates, cleaning scripts, etc.) to reduce the time required\nper client. In summary, with the combination of broad and deep modeling\napproaches, Acerta can both productize a turnover risk tool for the many\nand consult on bespoke analyses for those who are interested in a deeper\nanalysis.\n8.3\nInternal mobility recommender system\nChapter 4 introduces a prescriptive analytics approach to support internal\nmobility by developing a data-driven recommender system. Expanding on\nthe EJMs from Chapter 2, this recommender moves beyond simply describing\nexisting career paths and instead proactively suggests future career moves\nwithin the organization.\nOperationalization\nThe internal mobility recommender was implemented\nwithin Acerta (for their own organizational use) rather than directly at client\ncompanies. The prototype works as follows: given a query for a specific em-\nployee, a Python script finds several “neighbor” employees (those with similar\ncareer paths or profiles) and then suggests a next job for the queried em-\nployee based on what those neighbors have done next. In essence, “employees\nlike you went on to . . . ” as a recommendation.\nBecause precise performance scores of historical job-person matches were\nnot readily available, the implementation uses a proxy scoring function (as\n152\n 8.3. Internal mobility recommender system\ndescribed in the paper) that estimates a match quality based on tenure in a\nrole. The exact way this score is calculated involves modeling choices (e.g.,\nwhat tenure gives the highest score), and these parameters were kept flexible\nso they can be adjusted as needed.\nDuring internal pilot at Acerta, a significant adjustment was made com-\npared to the approach of Chapter 4: changing the granularity of “jobs” into\nbroader categories. In the original paper, each unique job title was consid-\nered a distinct item. However, for Acerta, this level of granularity (around\n200 distinct job titles) proved difficult — the data became too sparse, and\nthe recommender often suggested very common pathways (e.g., Payroll Of-\nficer I →Payroll Officer II →Payroll Officer III, which are frequent paths\nat Acerta). To make the system more useful, we mapped each job title to\na competency profile (an internal standard categorization), reducing about\n200 job titles to roughly 35 profiles. The idea was that multiple jobs cor-\nrespond to the same competency profile, which (i) makes the rating matrix\ndenser and the results more stable, and (ii) generalizes the recommendations\nbeyond the very specific job titles to slightly broader career moves. This way,\nthe recommender system can suggest moves that are not just the most com-\nmon steps, but also lateral or unconventional moves that share underlying\ncompetency requirements.\nIn practice, the implementation at Acerta thus evolved towards recom-\nmendations based on competency profile transitions rather than overly gran-\nular job sequences. Each competency profile was additionally characterized\nby specific competencies (e.g., leadership, information processing, and in-\nteraction), with corresponding scores, enabling business logic filters within\nrecommendations. For example, if a transition from competency profile A to\nB was recommended, constraints could be defined such that profile B must\nscore higher than A on certain competencies, such as leadership. Although\nleadership is just one illustrative example, any business rule or constraint\ncould be seamlessly integrated into the system.\nAt the time of writing, the recommender system is undergoing deploy-\nment as a Function App, a cloud-based solution within Microsoft Azure,\nwith the implementation being carried out by a third-party vendor. The\nimplementation serves as the first fully operational POC, intending to be\nfurther developed from a purely internal prototype into a robust, scalable\ntool.\nAdvantages\nThe recommender system’s prescriptive nature directly ad-\ndresses a practical need: guiding employees (and HR managers) in identify-\ning viable next career moves within the company. This has several benefits.\nFirst, it empowers employees by making them aware of career paths\nthey may not have previously considered. Conversely, for HR departments,\n153\n Chapter 8: Industry co-creation\nthe system facilitates talent management by proactively identifying suitable\ninternal candidates, including those who might not actively seek out new\npositions but possess desirable skills or experiences.\nNext, we conjecture that promoting internal mobility through tailored\nrecommendations could help organizations retain critical institutional knowl-\nedge, as internal career moves might reduce employee turnover and conse-\nquently mitigate its associated knowledge loss.\nFurthermore, there is a certain data synergy with EJMs. The system\nleverages the same event log perspective as EJMs, meaning organizations\nthat implemented Chapter 2’s descriptive approach have already laid the\ngroundwork for this prescriptive tool. It is a natural next step — first un-\nderstand the mobility patterns, then start recommending mobility opportu-\nnities.\nChallenges\nA limitation of the current implementation is that it is still\nbased on relatively limited data. At this stage, only historical event logs\nof job-employee matches (and, by extension, competency profile-employee\nmatches) are incorporated. To evolve the tool into a truly rich and usable\ninstrument, it would be beneficial to supplement it with additional data\nfrom various sources. For instance, adding more granular steps within func-\ntions, or incorporating data from assessments and training programs, could\nenhance the system’s depth. Allowing employees to input their own pref-\nerences for future roles could also make the system more personalized and\nflexible. However, these extensions are not yet part of the current POC and\nare envisioned for future iterations.\nAnother limitation is the absence of a direct measure of historical success\nin job-employee matches. Currently, we rely on the proxy of tenure duration\nwithin a role, which might be a suboptimal indicator. More nuance could be\nintroduced by making the evaluation job-specific, recognizing that the same\ntenure length might signal different outcomes depending on the role. Ideally,\nsuccess would be captured through direct assessments, such as structured\ninterviews or evaluation methods, providing a more accurate and meaningful\nmeasure.\nFinally, there is a critical human element: employee trust and accep-\ntance of the tool. Career recommendations touch on sensitive personal and\nprofessional areas, and there may initially be skepticism towards algorithm-\ngenerated ",
  "39": "llowing employees to input their own pref-\nerences for future roles could also make the system more personalized and\nflexible. However, these extensions are not yet part of the current POC and\nare envisioned for future iterations.\nAnother limitation is the absence of a direct measure of historical success\nin job-employee matches. Currently, we rely on the proxy of tenure duration\nwithin a role, which might be a suboptimal indicator. More nuance could be\nintroduced by making the evaluation job-specific, recognizing that the same\ntenure length might signal different outcomes depending on the role. Ideally,\nsuccess would be captured through direct assessments, such as structured\ninterviews or evaluation methods, providing a more accurate and meaningful\nmeasure.\nFinally, there is a critical human element: employee trust and accep-\ntance of the tool. Career recommendations touch on sensitive personal and\nprofessional areas, and there may initially be skepticism towards algorithm-\ngenerated suggestions. Clear communication emphasizing transparency —\nsuch as providing understandable reasons for each recommendation (e.g.,\nsimilarities in competencies or successful past transitions by similar col-\nleagues) — will be crucial for fostering trust and ensuring the system’s\neffective use.\n154\n 8.4. Potential implementation of other chapters\nValorization\nFrom a practical perspective, the recommender system offers\nclear benefits for both employees and managers. Employees gain a powerful\nnew tool to explore and pursue career opportunities proactively, often discov-\nering pathways they may not have independently considered. Similarly, HR\nmanagers gain valuable assistance in filling internal vacancies more efficiently\nand strategically, potentially surfacing less obvious but highly qualified in-\nternal candidates. This dual-sided functionality strengthens overall talent\nmanagement and succession planning within the organization.\nInternally, for Acerta, deploying this recommender system also represents\nleadership by example. Utilizing advanced analytics within its own HR pro-\ncesses allows Acerta to credibly demonstrate to clients its commitment and\nconfidence in data-driven HR solutions. Such internal use builds trust and\nshowcases the practical benefits these tools can provide, reinforcing Acerta’s\nreputation as an innovative and data-savvy partner.\nPotential to scale to other clients\nIn its current proof-of-concept state,\nthe recommender system appears relatively straightforward to scale to other\norganizations. Its foundational components — such as historical job and\ncompetency data — are common across most enterprises. However, mean-\ningful scalability and true commercial viability would be greatly enhanced by\nimplementing the previously mentioned improvements. Adding richer data\nstreams, employee-driven inputs, and a more nuanced measure of success-\nful job matches would significantly boost the tool’s effectiveness and market\nappeal.\nWhen scaling to other clients, careful consideration must be given to or-\nganizational differences. The granularity of job categorization suitable for\nAcerta may not directly translate elsewhere, so flexibility in defining com-\npetency profiles or job groupings will be essential. Furthermore, integrating\na user-friendly interface, potentially embedding the recommender within ex-\nisting HR platforms or systems, would be crucial for broad adoption.\n8.4\nPotential implementation of other chapters\nWhile Sections 8.1-8.3 discussed the operational details of the developed\ntools implemented within Acerta, several methodological advancements pre-\nsented in Part II (Chapters 5-7) also hold promise for practical application.\nSpecifically, cost-sensitive learning, decision-centric fairness, uplift modeling\n(with continuous treatments), and explicit fairness constraints have potential\nuse cases relevant to Acerta’s activities and client services.\nRobust cost-sensitive learning (Chapter 5) addresses scenarios in which\nprediction errors have uneven consequences — a context encountered by\n155\n Chapter 8: Industry co-creation\nAcerta, for instance, when predicting employee turnover.\nCurrently, Ac-\nerta’s turnover prediction (as discussed in Section 8.2) relies primarily on\npredictive performance in combination with explainability. However, differ-\nent employees and positions may incur varying degrees of replacement costs.\nIncorporating (robust) cost-sensitive methods would allow Acerta to help\ntheir clients prioritize retention efforts in a more nuanced manner, focusing\nresources on employees and hard-to-fill positions whose departure would en-\ntail the highest replacement costs. Successfully implementing this method\nwould, however, require close collaboration with clients to accurately quan-\ntify these varying costs — an aspect not addressed in the academic part, but\nan initial challenge that could enhance the value of Acerta’s services.\nDecision-centric fairness (Chapter 6) introduces fairness evaluation specif-\nically within subsets of model outputs relevant to actual decisions, such as,\nin an HR setting, shortlists for internal hiring or training and reskilling rec-\nommendations. Currently, Acerta supports numerous clients in managing\ninternal talent mobility (as demonstrated in Sections 8.1 and 8.3), but fair-\nness considerations have not yet been integrated. By incorporating decision-\ncentric fairness, Acerta could directly address ethical concerns in sensitive\nHR processes such as selection and promotion, ensuring that recommenda-\ntions or shortlists do not systematically disadvantage certain demographic\ngroups. Practically, this approach would help Acerta and its clients comply\nwith ethical and legal standards, improve perceptions of internal career pro-\ncesses, and therefore foster trust in a system, enhancing its transparency, le-\ngitimacy, and adoptability. To operationalize this successfully, Acerta would\nneed to explicitly define fairness criteria with client organizations about pro-\ntected features and relevant regulatory frameworks.\nUplift modeling with continuous treatments (Chapter 7) also presents\nclear opportunities, particularly in HR areas such as personalized employee\nretention programs or targeted training investments. Instead of merely pre-\ndicting whether employees might leave, Acerta could use uplift modeling\nto identify the optimal amount of resources to invest in retaining specific\nemployees. For example, rather than offering uniform retention incentives\nor training sessions, Acerta could guide clients toward individual-level re-\nsource allocation — such as tailored training hours — maximizing impact\non a desired outcome while minimizing costs. Implementing continuous up-\nlift modeling, however, relies on rich historical data capturing varying levels\nof past interventions (e.g., differentiated retention strategies previously im-\nplemented by clients). Acerta would need to facilitate such detailed data\ncollection efforts or pilot projects before fully integrating this methodology.\nFinally, both Chapters 6 and 7 address fairness but differ in their ap-\nproach. The method in Chapter 6 integrates fairness directly into model\ntraining, making it suitable for scenarios where decisions occur continuously\n156\n 8.4. Potential implementation of other chapters\n(in time) and individually. Conversely, Chapter 7 uses fairness constraints at\nthe batch decision-making level — ideal for scenarios involving budget allo-\ncation or the distribution of limited resources across defined groups. Acerta\ncould leverage these complementary approaches depending on specific client\nneeds, either embedding fairness at the individual decision level or ensuring\nequity across group-based interventions.\nIn summary, each of these advanced methods offers tangible opportu-\nnities for Acerta to expand its analytical offerings, aligning methodologi-\ncal advances with concrete HR challenges faced by its clients.\nRealizing\nthese benefits depends on clearly defined cost and fairness criteria, enhanced\ndata collection processes, and thoughtful alignment with legal and ethical\nguidelines — steps that would enable Acerta to reinforce its position as a\nforward-looking HR analytics provider.\n157\n  9\nConclusion\nModern data collection processes, methodological advancements, and com-\nputing resources facilitate complex pattern recognition through advanced\nanalytics [2]. Beyond descriptive analyses that reveal historical trends, pre-\ndictive models enable the mapping of future scenarios. Prescriptive methods\nlike uplift modeling guide concrete interventions under budget or fairness\nconstraints to steer towards desired future scenarios. Building on these en-\nabling factors, Part I and Part II work in tandem to contribute to both HR\napplications and methodological advances through six main chapters.\nClose collaboration with Acerta Consult keeps the research grounded\nin real-world HR processes, ensuring that each contribution –— from job-\nemployee matching to fairness-focused interventions –— is not merely novel,\nbut remains relevant and useful in organizational settings.\nThe following sections summarize this dissertation’s contributions, offer\nmanagerial implications, highlight limitations, and offer directions for future\nresearch.\n9.1\nContributions\nChapter 2 introduced the application of process mining to internal em-\nployee mobility data. It showed how EJMs offer a dynamic way to visual-\nize individual career paths, highlighting infrequent transitions, identifying\nstepping-stone positions, and revealing alternative paths beyond conven-\ntional career ladders. This approach provides HR professionals with con-\ncrete, data-driven insights on internal mobility and helps address the com-\nplexity of real-world career trajectories.\nChapter 3 focused on predicting employee turnover through a compre-\nhensive scoping review and benchmarking of current methods. It highlighted\nthe inconsistent methodologies in existing turnover research and then es",
  "40": "o fairness-focused interventions –— is not merely novel,\nbut remains relevant and useful in organizational settings.\nThe following sections summarize this dissertation’s contributions, offer\nmanagerial implications, highlight limitations, and offer directions for future\nresearch.\n9.1\nContributions\nChapter 2 introduced the application of process mining to internal em-\nployee mobility data. It showed how EJMs offer a dynamic way to visual-\nize individual career paths, highlighting infrequent transitions, identifying\nstepping-stone positions, and revealing alternative paths beyond conven-\ntional career ladders. This approach provides HR professionals with con-\ncrete, data-driven insights on internal mobility and helps address the com-\nplexity of real-world career trajectories.\nChapter 3 focused on predicting employee turnover through a compre-\nhensive scoping review and benchmarking of current methods. It highlighted\nthe inconsistent methodologies in existing turnover research and then es-\ntablished a rigorous experiment involving multiple classification algorithms\nacross various datasets. The findings provide a unified focal point for schol-\nars and practitioners.\nChapter 4 tackled job–employee matching with an emphasis on address-\ning data scarcity problems (e.g., new hires). Integrating historical perfor-\n159\n Chapter 9: Conclusion\nmance with personal employee attributes through a similarity regularization\nterm reduces cold-start issues. This similarity-driven extension proved ben-\neficial in improving match quality for internal mobility recommendations.\nChapter 5 improved the robustness of instance-dependent, cost-sensitive\nclassification by proposing r-cslogit, a method that detects and mitigates out-\nliers in cost and benefit parameters. Tested extensively on (semi-)synthetic\ndata, it showed better stability and performance gain when the outlier size in-\ncreased, particularly crucial in contexts where misclassification can be costly.\nChapter 6 advances fairness in resource allocation resulting from a clas-\nsification task. It enforces fair outcomes only where resource allocation oc-\ncurs, i.e., a predefined actionable decision region. This approach optimizes\nthe model’s predictive capability while ensuring fairness for those receiving\npositive classification.\nChapter 7 extends uplift modeling to continuous treatments, moving\nbeyond the traditional binary intervention framework. First, it estimates\nconditional average dose responses through causal machine learning; then,\nit optimizes treatment allocation via integer linear programming under bud-\nget and fairness constraints.\nThis enables effective and efficient resource\ndistribution, aligned with organizational goals.\n9.2\nManagerial implications\nThe enduring value of traditional HR theory.\nWhile analytics en-\nhances decision-making, traditional HR theory remains indispensable for in-\nterpreting deeper causes of outcomes like turnover and mobility. Although\nnot the focus of this dissertation, theory-based insights guide the design of\ndata collection, feature selection, and interventions, ensuring that predictive\nmodels align with established frameworks. Managers should integrate these\ntheoretical perspectives to maximize interpretability, strategic fit, and model\nperformance.\nPrioritizing data quality and compliance.\nSuccessful HR analytics\ninitiatives depend significantly on data quality, not merely on the quantity\nof data collected. Managers should prioritize the systematic collection of\nrelevant, high-quality data, informed by traditional HR theories and frame-\nworks that identify meaningful variables. Compliance with data protection\nregulations such as GDPR is also crucial. HR analytics inherently involves\nhandling sensitive employee information, and violations of privacy standards\ncan result in legal repercussions and reputational damage. Managers must\nimplement rigorous data governance practices, continuously evaluating their\nanalytics frameworks to ensure compliance, ethical responsibility, and overall\nreliability.\n160\n 9.2. Managerial implications\nPredict-and-optimize or predict-then-optimize?\nThis dissertation high-\nlights two distinct paradigms: Predict-and-optimize and Predict-then-optimize.\nEach has its advantages and disadvantages. Predict-and-optimize integrates\ndecision-making processes directly into the predictive modeling step, en-\nabling comprehensive management of uncertainty and potentially delivering\ndecisions closely aligned with downstream objectives. However, this integra-\ntion can introduce complexity, demanding greater computational resources\nand limiting flexibility. Conversely, Predict-then-optimize maintains modu-\nlarity and simplicity, allowing for easier implementation and updates, though\nit may propagate predictive errors into suboptimal decisions. Managers must\ncarefully consider their organization’s specific context, objectives, and re-\nsource availability when selecting between these paradigms.\nWith a new hammer, everything looks like a nail.\nWith the surge in\navailability and capabilities of analytical methods, there might be a growing\ntemptation to apply them broadly — even where simpler approaches would\nsuffice. Yet, not every problem requires a complex solution. Managers are\nadvised to start with small, targeted analytics initiatives grounded in clear\nbusiness objectives. For example, in turnover prediction, initiating analysis\nwith a logistic regression model and smart feature engineering can yield ac-\ntionable insights without unnecessary complexity. Avoiding solution-driven\napproaches –— where methods dictate problems –— is critical.\nImportance of post-deployment analytics.\nContinuous monitoring of\ndeployed analytical models is critical for sustained performance and align-\nment with changing organizational environments. Over time, data distri-\nbutions may shift, potentially resulting in data drift and decreased model\nperformance.\nManagers must establish robust monitoring protocols that\ndetect performance degradation early, facilitating interventions such as re-\ntraining, recalibrating thresholds, or other model adjustments.\nEffective\npost-deployment monitoring protects organizations from the risks of unno-\nticed model failures and ensures their reliability.\nAwareness of methodological assumptions.\nEvery analytical method\nemployed is built upon specific assumptions. Traditional statistics rest on as-\nsumptions about data distributions and sampling processes (e.g., the I.I.D.\nassumption), while causal inference methods rely on assumptions such as\nconsistency, ignorability, and overlap. Managers must remain aware of these\nunderlying assumptions and their implications in their conclusions. There\nshould be transparency regarding these assumptions, and they should regu-\nlarly be assessed against real-world conditions to prevent misinterpretations\nand suboptimal decisions.\n161\n Chapter 9: Conclusion\nAvoiding algorithmic aversion.\nManagers have a key responsibility in\nfostering an organizational culture that mitigates algorithmic aversion —\nthe tendency of employees and managers to distrust or avoid algorithmic\nsupport, even when evidence shows it improves decision-making [392]. Of\ncourse, this depends on the invasiveness of algorithmic support. But, if de-\nsired, managers can help by creating a cultural environment that encourages\ncontinuous learning and adaptability, enabling effective integration of new\ntechnologies by framing the use of algorithms as collaborative rather than\nimposed [392].\nModel output is not the same as probability.\nInterpreting model\npredictions as probabilities requires proper calibration.\nModels differ in-\nherently in calibration quality: logistic regression typically yields naturally\nwell-calibrated predictions, whereas boosting methods and SVMs often pro-\nduce overly confident outputs near 0 or 1, or overly conservative predictions\nclustered around 0.5 [393]. This issue becomes even more pronounced when\nmodels are trained using multi-objective criteria such as fairness considera-\ntions (see Chapter 6). These objectives can deliberately reshape the output\ndistribution to satisfy fairness requirements, often at the expense of cali-\nbration accuracy. Decision-makers must therefore exercise caution. Poor\ncalibration can result in suboptimal decisions; for example, selecting em-\nployees predicted to leave with a model score above 0.7 does not imply an\nactual 70% probability. Similarly, when predictions are inputs for subse-\nquent analyses, like calculating expected profit or loss, these outputs must\nbe properly calibrated. Metrics such as the Brier score can assess calibra-\ntion quality, while corrective techniques like Platt scaling can enhance the\nreliability of predicted probabilities [394].\n9.3\nLimitations and future work\nExplainability and interpretability of models.\nOne notable limita-\ntion of this dissertation is the limited focus on model explainability (except\nfor the industry implementation as discussed in Chapter 8). While predic-\ntive accuracy and prescriptive effectiveness are central in this manuscript, a\ndeeper exploration into opening the black box would enhance the practical\nadoption of analytics methods in HR. According to the distinction between\nprediction and explanation [61], HR analytics applications frequently require\na balance between accurate predictions and meaningful interpretations. Fu-\nture research could address this gap by developing advanced yet interpretable\nmodels or by systematically integrating post-hoc explainability techniques,\nfacilitating greater acceptance and trust.\n162\n 9.3. Limitations and future work\nIncorporation of unstructured data sources.\nAnother limitation is\nthe exclusive reliance on structured data, neglecting the growing availabil-\nity and usability of unstructured textual data. The rise of easily accessible,\nplug-and-play solutions leveraging large language models (LLMs) offers sub-\nstantial potential for enriching HR analytics applications. Future",
  "41": "er 8). While predic-\ntive accuracy and prescriptive effectiveness are central in this manuscript, a\ndeeper exploration into opening the black box would enhance the practical\nadoption of analytics methods in HR. According to the distinction between\nprediction and explanation [61], HR analytics applications frequently require\na balance between accurate predictions and meaningful interpretations. Fu-\nture research could address this gap by developing advanced yet interpretable\nmodels or by systematically integrating post-hoc explainability techniques,\nfacilitating greater acceptance and trust.\n162\n 9.3. Limitations and future work\nIncorporation of unstructured data sources.\nAnother limitation is\nthe exclusive reliance on structured data, neglecting the growing availabil-\nity and usability of unstructured textual data. The rise of easily accessible,\nplug-and-play solutions leveraging large language models (LLMs) offers sub-\nstantial potential for enriching HR analytics applications. Future research\nshould explore methods for systematically incorporating textual data from\nsources such as employee surveys, performance reviews, and exit interviews.\nBy using such sources of qualitative insights, analytics models could improve\npredictive accuracy, decision-making nuance, and contextual understanding.\nScope of fairness criteria.\nChapters 6 and 7 consider one fairness notion:\ngroup-level independence. Alternative fairness criteria (e.g., separation or\nsufficiency), intersectionality (e.g., the combination of ethnicity and gender),\nor individual-level instead of group fairness, might produce different trade-\noffs. Extending decision-centric fairness to broader or more complex fairness\nnotions constitutes an important next step.\nExplicit modeling of uncertainty.\nCurrently, the analytical methods\npresented — both predictive and prescriptive —– produce outputs or rec-\nommendations based on point estimates. An important avenue for future\nresearch lies in explicitly incorporating uncertainty into these models. Ad-\nvanced decision-making tools could integrate uncertainty scores or confidence\nintervals (through, e.g., conformal prediction), enabling more informed man-\nagerial decisions. Introducing options such as reject decisions or decision-\nfocused learning paradigms, linking predictive and decision-making param-\neters explicitly, could enhance the robustness and utility of prescriptive an-\nalytics models.\nFoundation models.\nFoundation models, pretrained on large and diverse\ndatasets, have become central in many machine learning applications –—\nmost notably in natural language processing, where LLMs dominate. Re-\ncently, foundation models tailored to tabular data have been developed, with\nTabPFN [395] being a prominent example. Their performance is promising\nand may represent a new state of the art for certain predictive tasks, includ-\ning those discussed in this manuscript. Although the current implementation\nof TabPFN performs best on smaller datasets, it shows great potential due\nto its minimal need for fine-tuning, low inference time, and its native ability\nto handle missing and categorical data.\n163\n  References\n[1]\nH. Chen, R. H. Chiang, and V. C. Storey, “Business intelligence and\nanalytics: From big data to big impact,” MIS quarterly, pp. 1165–\n1188, 2012.\n[2]\nF. Provost and T. Fawcett, “Data science and its relationship to big\ndata and data-driven decision making,” Big data, vol. 1, no. 1, pp. 51–\n59, 2013.\n[3]\nJ. H. Marler and J. W. Boudreau, “An evidence-based review of hr\nanalytics,” The International Journal of Human Resource Manage-\nment, vol. 28, no. 1, pp. 3–26, 2017.\n[4]\nD. Ulrich and J. H. Dulebohn, “Are we there yet? what’s next for\nhr?” Human resource management review, vol. 25, no. 2, pp. 188–\n204, 2015.\n[5]\nD. Angrave, A. Charlwood, I. Kirkpatrick, M. Lawrence, and M. Stu-\nart, “Hr and analytics: Why hr is set to fail the big data challenge,”\nHuman resource management journal, vol. 26, no. 1, pp. 1–11, 2016.\n[6]\nD. Pessach, G. Singer, D. Avrahami, H. C. Ben-Gal, E. Shmueli, and I.\nBen-Gal, “Employees recruitment: A prescriptive analytics approach\nvia machine learning and mathematical programming,” Decision sup-\nport systems, vol. 134, p. 113 290, 2020.\n[7]\nEuropean Commission, Regulation (eu) 2016/679 of the european par-\nliament and of the council of 27 april 2016 on the protection of natu-\nral persons with regard to the processing of personal data and on the\nfree movement of such data, and repealing directive 95/46/ec (general\ndata protection regulation) (text with eea relevance), May 2016.\n[8]\nE. Commission, EUR-Lex - 52021PC0206 - EN - EUR-Lex, eur-\nlex.europa.eu, Accessed on December 2022, Apr. 2021.\n[9]\nC. Elkan, “The foundations of cost-sensitive learning,” in Interna-\ntional joint conference on artificial intelligence, Lawrence Erlbaum\nAssociates Ltd, vol. 17, 2001, pp. 973–978.\n[10]\nS. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, “Al-\ngorithmic decision making and the cost of fairness,” in Proceedings of\nthe 23rd acm sigkdd international conference on knowledge discovery\nand data mining, 2017, pp. 797–806.\n165\n References\n[11]\nM. Raghavan, S. Barocas, J. Kleinberg, and K. Levy, “Mitigating bias\nin algorithmic hiring: Evaluating claims and practices,” in Proceedings\nof the 2020 conference on fairness, accountability, and transparency,\n2020, pp. 469–481.\n[12]\nS. Barocas and A. D. Selbst, “Big data’s disparate impact,” California\nLaw Review, vol. 104, no. 3, pp. 671–732, 2016.\n[13]\nW. Verbeke, D. Olaya, M.-A. Guerry, and J. Van Belle, “To do or not\nto do? cost-sensitive causal classification with individual treatment\neffect estimates,” European Journal of Operational Research, 2022.\n[14]\nS. Höppner, B. Baesens, W. Verbeke, and T. Verdonck, “Instance-\ndependent cost-sensitive learning for detecting transfer fraud,” Euro-\npean Journal of Operational Research, vol. 297, no. 1, pp. 291–300,\n2022.\n[15]\nW. S. Siebert and N. Zubanov, “Searching for the optimal level of em-\nployee turnover: A study of a large uk retail organization,” Academy\nof management journal, vol. 52, no. 2, pp. 294–313, 2009.\n[16]\nS. Barocas, M. Hardt, and A. Narayanan, Fairness and Machine\nLearning. fairmlbook.org, 2019.\n[17]\nJ. Dastin, “Amazon scraps secret ai recruiting tool that showed bias\nagainst women,” in Ethics of data and analytics, Auerbach Publica-\ntions, 2022, pp. 296–299.\n[18]\nEuropean Commission, Proposal for a Regulation of the European\nParliament and of the Council Laying Down Harmonised Rules on\nArtificial Intelligence (Artificial Intelligence Act) and Amending Cer-\ntain Union Legislative Acts, COM(2021) 206 final, 2021.\n[19]\nS. Corbett-Davies, J. D. Gaebler, H. Nilforoshan, R. Shroff, and S.\nGoel, “The measure and mismeasure of fairness,” The Journal of Ma-\nchine Learning Research, vol. 24, no. 1, pp. 14 730–14 846, 2023.\n[20]\nJ. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent trade-offs\nin the fair determination of risk scores,” arXiv preprint: 1609.05807,\n2016.\n[21]\nK. Makhlouf, S. Zhioua, and C. Palamidessi, “On the applicability\nof machine learning fairness notions,” ACM SIGKDD Explorations\nNewsletter, vol. 23, no. 1, pp. 14–23, 2021.\n[22]\nS. De Vos, J. De Smedt, C. Wuytens, and W. Verbeke, “Leveraging\nprocess mining to optimize internal employee mobility strategies,”\nin Business Process Management Cases Vol. 3: Implementation in\nPractice, Springer, 2025, pp. 15–28.\n166\n References\n[23]\nS. De Vos, C. Bockel-Rickermann, J. Van Belle, and W. Verbeke,\n“Predicting employee turnover: Scoping and benchmarking the state-\nof-the-art,” Business & Information Systems Engineering, pp. 1–20,\n2024.\n[24]\nS. De Vos, J. De Smedt, M. Verbruggen, and W. Verbeke, “Data-\ndriven internal mobility: Similarity regularization gets the job done,”\nKnowledge-Based Systems, vol. 295, p. 111 824, 2024.\n[25]\nS. De Vos, T. Vanderschueren, T. Verdonck, and W. Verbeke, “Robust\ninstance-dependent cost-sensitive classification,” Advances in Data\nAnalysis and Classification, vol. 17, no. 4, pp. 1057–1079, 2023.\n[26]\nS. De Vos, J. Van Belle, A. Algaba, W. Verbeke, and S. Verboven,\n“Decision-centric fairness: Evaluation and optimization for resource\nallocation problems,” arXiv preprint arXiv:2504.20642, 2025.\n[27]\nS. De Vos, C. Bockel-Rickermann, S. Lessmann, and W. Verbeke, “Up-\nlift modeling with continuous treatments: A predict-then-optimize\napproach,” arXiv preprint arXiv:2412.09232, 2024.\n[28]\nA. Raval, “Talent wars: Why businesses have to battle to hire the\nbest,” en, Financial Times, Sep. 2022.\n[29]\nJ. Boudreau and W. Cascio, “Human capital analytics: Why are we\nnot there?” Journal of Organizational Effectiveness: People and Per-\nformance, vol. 4, no. 2, pp. 119–126, Jan. 2017.\n[30]\nV. Fernandez and E. Gallardo-Gallardo, “Tackling the HR digitaliza-\ntion challenge: Key factors and barriers to HR analytics adoption,”\nen, Competitiveness Review Journal, vol. 31, no. 1, pp. 162–187, Jul.\n2020.\n[31]\nM. Dumas, M. La Rosa, J. Mendling, and H. Reijers, Fundamentals of\nBusiness Process Management, en. Springer Berlin Heidelberg, Mar.\n2013.\n[32]\nW. van der Aalst, “Data science in action,” in Process Mining: Data\nScience in Action, W. van der Aalst, Ed., Berlin, Heidelberg: Springer\nBerlin Heidelberg, 2016, pp. 3–23.\n[33]\nM. L. Van Eck, X. Lu, S. J. Leemans, and W. M. Van Der Aalst,\n“Pm: A process mining project methodology,” in Advanced Infor-\nmation Systems Engineering: 27th International Conference, CAiSE\n2015, Stockholm, Sweden, June 8-12, 2015, Proceedings, Springer,\n2015, pp. 297–313.\n[34]\nW. M. P. van der Aalst, “A practitioner’s guide to process mining:\nLimitations of the directly-follows graph,” Procedia Computer Sci-\nence, vol. 164, pp. 321–328, Jan. 2019.\n167\n References\n[35]\nP. W. Hom and R. W. Griffeth, Employee turnover (South-Western\nSeries in Human Resources Management). South-Western College\nPublishing, 1995, isbn: 9780538808736. [Online]. Available: https:\n//books.google.de/books?id=n%5C_cXAQAAMAAJ.\n[36]\nC. Per",
  "42": "eijers, Fundamentals of\nBusiness Process Management, en. Springer Berlin Heidelberg, Mar.\n2013.\n[32]\nW. van der Aalst, “Data science in action,” in Process Mining: Data\nScience in Action, W. van der Aalst, Ed., Berlin, Heidelberg: Springer\nBerlin Heidelberg, 2016, pp. 3–23.\n[33]\nM. L. Van Eck, X. Lu, S. J. Leemans, and W. M. Van Der Aalst,\n“Pm: A process mining project methodology,” in Advanced Infor-\nmation Systems Engineering: 27th International Conference, CAiSE\n2015, Stockholm, Sweden, June 8-12, 2015, Proceedings, Springer,\n2015, pp. 297–313.\n[34]\nW. M. P. van der Aalst, “A practitioner’s guide to process mining:\nLimitations of the directly-follows graph,” Procedia Computer Sci-\nence, vol. 164, pp. 321–328, Jan. 2019.\n167\n References\n[35]\nP. W. Hom and R. W. Griffeth, Employee turnover (South-Western\nSeries in Human Resources Management). South-Western College\nPublishing, 1995, isbn: 9780538808736. [Online]. Available: https:\n//books.google.de/books?id=n%5C_cXAQAAMAAJ.\n[36]\nC. Perryer, C. Jordan, I. Firns, and A. Travaglione, “Predicting turnover\nintentions: The interactive effects of organizational commitment and\nperceived organizational support,” Management Research Review, vol. 33,\nno. 9, pp. 911–923, 2010.\n[37]\nS. M. Soltis, F. Agneessens, Z. Sasovova, and G. Labianca, “A social\nnetwork perspective on turnover intentions: The role of distributive\njustice and social support,” Human Resource Management, vol. 52,\nno. 4, pp. 561–584, 2013.\n[38]\nR. W. Griffeth, P. W. Hom, and S. Gaertner, “A meta-analysis of\nantecedents and correlates of employee turnover: Update, moderator\ntests, and research implications for the next millennium,” Journal of\nManagement, vol. 26, no. 3, pp. 463–488, 2000.\n[39]\nM. Armstrong, A Handbook of Human Resource Management Prac-\ntice. Kogan Page Publishers, 2006.\n[40]\nA. Carmeli and J. Weisberg, “Exploring turnover intentions among\nthree professional groups of employees,” Human Resource Develop-\nment International, vol. 9, no. 2, pp. 191–206, 2006.\n[41]\nV. V. Saradhi and G. K. Palshikar, “Employee churn prediction,”\nExpert Systems with Applications, vol. 38, no. 3, pp. 1999–2006, 2011.\n[42]\nM. J. Kavanagh and R. D. Johnson, Human resource information\nsystems. SAGE Publications, Incorporated, 2020.\n[43]\nG. DeSanctis, “Human resource information systems: A current as-\nsessment,” MIS quarterly, pp. 15–27, 1986.\n[44]\nI. Teinemaa, M. Dumas, M. L. Rosa, and F. M. Maggi, “Outcome-\noriented predictive process monitoring: Review and benchmark,” ACM\nTransactions on Knowledge Discovery from Data (TKDD), vol. 13,\nno. 2, pp. 1–57, 2019.\n[45]\nS. Wenninger and C. Wiethe, “Benchmarking energy quantification\nmethods to predict heating energy performance of residential build-\nings in germany,” Business & Information Systems Engineering, vol. 63,\npp. 223–242, 2021.\n[46]\nS. Lessmann and S. Voß, “Customer-centric decision support: A bench-\nmarking study of novel versus established classification models,” Busi-\nness & Information Systems Engineering, vol. 2, pp. 79–93, 2010.\n168\n References\n[47]\nP. W. Hom, T. W. Lee, J. D. Shaw, and J. P. Hausknecht, “One\nhundred years of employee turnover theory and research.,” Journal of\nApplied Psychology, vol. 102, no. 3, p. 530, 2017.\n[48]\nE. Rombaut and M.-A. Guerry, “Predicting voluntary turnover through\nhuman resources database analysis,” Management Research Review,\nvol. 41, no. 1, pp. 96–112, 2018.\n[49]\nD. Pitts, J. Marvel, and S. Fernandez, “So hard to say goodbye?\nTurnover intention among US federal employees,” Public Administra-\ntion Review, vol. 71, no. 5, pp. 751–760, 2011.\n[50]\nT. W. Ng and D. C. Feldman, “Re-examining the relationship between\nage and voluntary turnover,” Journal of Vocational Behavior, vol. 74,\nno. 3, pp. 283–294, 2009.\n[51]\nJ. A. Grissom, S. L. Viano, and J. L. Selin, “Understanding employee\nturnover in the public sector: Insights from research on teacher mobil-\nity,” Public Administration Review, vol. 76, no. 2, pp. 241–251, 2016.\n[52]\nA. L. Kalleberg and K. A. Loscocco, “Aging, values, and rewards:\nExplaining age differences in job satisfaction,” American Sociological\nReview, pp. 78–90, 1983.\n[53]\nT. W. Ng and D. C. Feldman, “The relationships of age with job atti-\ntudes: A meta-analysis,” Personnel Psychology, vol. 63, no. 3, pp. 677–\n718, 2010.\n[54]\nA. Clark, A. Oswald, and P. Warr, “Is job satisfaction U-shaped in\nage?” Journal of Occupational and Organizational Psychology, vol. 69,\nno. 1, pp. 57–81, 1996.\n[55]\nC. D. Crossley, R. J. Bennett, S. M. Jex, and J. L. Burnfield, “De-\nvelopment of a global measure of job embeddedness and integration\ninto a traditional model of voluntary turnover.,” Journal of Applied\nPsychology, vol. 92, no. 4, p. 1031, 2007.\n[56]\nO. A. Ayodele, A. Chang-Richards, and V. González, “Factors affect-\ning workforce turnover in the construction sector: A systematic re-\nview,” Journal of Construction Engineering and Management, vol. 146,\nno. 2, p. 03 119 010, 2020.\n[57]\nS. M. Thin, B. Chongmelaxme, S. Watcharadamrongkun, T. Kan-\njanarach, B. A. Sorofman, and T. Kittisopee, “A systematic review\non pharmacists’ turnover and turnover intention,” Research in Social\nand Administrative Pharmacy, vol. 18, no. 11, pp. 3884–3894, 2022.\n[58]\nJ. W. Han, “A review of antecedents of employee turnover in the\nhospitality industry on individual, team and organizational levels,”\nInternational Hospitality Review, vol. 36, no. 1, pp. 156–173, 2020.\n169\n References\n[59]\nP. W. Hom, T. R. Mitchell, T. W. Lee, and R. W. Griffeth, “Reviewing\nemployee turnover: Focusing on proximal withdrawal states and an\nexpanded criterion.,” Psychological Bulletin, vol. 138, no. 5, p. 831,\n2012.\n[60]\nT. W. Lee and T. R. Mitchell, “An alternative approach: The unfold-\ning model of voluntary employee turnover,” Academy of Management\nReview, vol. 19, no. 1, pp. 51–89, 1994.\n[61]\nG. Shmueli, “To explain or to predict?” Statistical Science, vol. 25,\nno. 3, pp. 289–310, 2010. doi: 10.1214/10-STS330. [Online]. Avail-\nable: https://doi.org/10.1214/10-STS330.\n[62]\nT. Pape, “Prioritising data items for business analytics: Framework\nand application to human resources,” European Journal of Opera-\ntional Research, vol. 252, no. 2, pp. 687–698, 2016.\n[63]\nD. Pessach, G. Singer, D. Avrahami, H. Chalutz Ben-Gal, E. Shmueli,\nand I. Ben-Gal, “Employees recruitment: A prescriptive analytics ap-\nproach via machine learning and mathematical programming,” en,\nDecision Support Systems, vol. 134, p. 113 290, Jul. 2020.\n[64]\nC.-F. Chien and L.-F. Chen, “Data mining to improve personnel se-\nlection and enhance human capital: A case study in high-technology\nindustry,” Expert Systems with Applications, vol. 34, no. 1, pp. 280–\n290, 2008.\n[65]\nJ. M. Kirimi and C. A. Moturi, “Application of data mining classifi-\ncation in employee performance prediction,” International Journal of\nComputer Applications, vol. 146, no. 7, pp. 28–35, 2016.\n[66]\nJ.-J. Decorte, J. Van Hautte, T. Demeester, and C. Develder, “Job-\nbert: Understanding job titles through skills,” arXiv preprint arXiv:2109.09605,\n2021.\n[67]\nF. Devriendt, D. Moldovan, and W. Verbeke, “A literature survey and\nexperimental evaluation of the state-of-the-art in uplift modeling: A\nstepping stone toward the development of prescriptive analytics,” Big\nData, vol. 6, no. 1, pp. 13–41, 2018.\n[68]\nF. Devriendt, J. Berrevoets, and W. Verbeke, “Why you should stop\npredicting customer churn and start using uplift models,” Information\nSciences, vol. 548, pp. 497–515, 2021.\n[69]\nL. Breiman, “Statistical modeling: The two cultures,” Statistical sci-\nence, vol. 16, no. 3, pp. 199–231, 2001.\n[70]\nH. Arksey and L. O’Malley, “Scoping studies: Towards a methodolog-\nical framework,” International Journal of Social Research Methodol-\nogy, vol. 8, no. 1, pp. 19–32, 2005.\n170\n References\n[71]\nZ. Munn, M. D. Peters, C. Stern, C. Tufanaru, A. McArthur, and\nE. Aromataris, “Systematic review or scoping review? Guidance for\nauthors when choosing between a systematic or scoping review ap-\nproach,” BMC Medical Research Methodology, vol. 18, pp. 1–7, 2018.\n[72]\nR. Pranckut˙e, “Web of Science (WoS) and Scopus: The titans of bibli-\nographic information in today’s academic world,” Publications, vol. 9,\nno. 1, p. 12, 2021.\n[73]\nV. Nagadevara, V. Srinivasan, and R. Valk, Establishing a link be-\ntween employee turnover and withdrawal behaviours: Application of\ndata mining techniques, 2008.\n[74]\nH.-Y. Chang, “Employee turnover: A novel prediction solution with\neffective feature selection,” WSEAS Transactions on Information Sci-\nence and Applications, vol. 6, no. 3, pp. 417–426, 2009.\n[75]\nQ. A. Al-Radaideh and E. Al Nagi, “Using data mining techniques to\nbuild a classification model for predicting employees performance,”\nInternational Journal of Advanced Computer Science and Applica-\ntions, vol. 3, no. 2, 2012.\n[76]\nM. A. Valle, S. Varas, and G. A. Ruz, “Job performance prediction\nin a call center using a naive Bayes classifier,” Expert Systems with\nApplications, vol. 39, no. 11, pp. 9939–9945, 2012.\n[77]\nM. A. Valle and G. A. Ruz, “Turnover prediction in a call center: Be-\nhavioral evidence of loss aversion using random forest and naive Bayes\nalgorithms,” Applied Artificial Intelligence, vol. 29, no. 9, pp. 923–942,\n2015.\n[78]\nA. M. Esmaieeli Sikaroudi, R. Ghousi, and A. Sikaroudi, “A data\nmining approach to employee turnover prediction (case study: Arak\nautomotive parts manufacturing),” Journal of Industrial and Systems\nEngineering, vol. 8, no. 4, pp. 106–121, 2015.\n[79]\nD. S. Sisodia, S. Vishwakarma, and A. Pujahari, “Evaluation of ma-\nchine learning models for employee churn prediction,” in 2017 Inter-\nnational Conference on Inventive Computing and Informatics (ICICI),\nIEEE, 2017, pp. 1016–1020.\n[80]\nİ. O. Yiğit and H. Shourabizadeh, “An approach for predicting em-\nployee churn by using data mining,” in 2017 International Artificial\nIntelligence and Data Processing Symposium (IDAP), IEEE, 2017,\npp. 1–4.\n[81]\nM. A. Valle, G. A. Ruz, and V. H. Masías, “Using se",
  "43": ". 9939–9945, 2012.\n[77]\nM. A. Valle and G. A. Ruz, “Turnover prediction in a call center: Be-\nhavioral evidence of loss aversion using random forest and naive Bayes\nalgorithms,” Applied Artificial Intelligence, vol. 29, no. 9, pp. 923–942,\n2015.\n[78]\nA. M. Esmaieeli Sikaroudi, R. Ghousi, and A. Sikaroudi, “A data\nmining approach to employee turnover prediction (case study: Arak\nautomotive parts manufacturing),” Journal of Industrial and Systems\nEngineering, vol. 8, no. 4, pp. 106–121, 2015.\n[79]\nD. S. Sisodia, S. Vishwakarma, and A. Pujahari, “Evaluation of ma-\nchine learning models for employee churn prediction,” in 2017 Inter-\nnational Conference on Inventive Computing and Informatics (ICICI),\nIEEE, 2017, pp. 1016–1020.\n[80]\nİ. O. Yiğit and H. Shourabizadeh, “An approach for predicting em-\nployee churn by using data mining,” in 2017 International Artificial\nIntelligence and Data Processing Symposium (IDAP), IEEE, 2017,\npp. 1–4.\n[81]\nM. A. Valle, G. A. Ruz, and V. H. Masías, “Using self-organizing\nmaps to model turnover of sales agents in a call center,” Applied Soft\nComputing, vol. 60, pp. 763–774, 2017.\n171\n References\n[82]\nM. M. Alam, K. Mohiuddin, M. K. Islam, M. Hassan, M. A.-U.\nHoque, and S. M. Allayear, “A machine learning approach to ana-\nlyze and reduce features to a significant number for employee’s turn\nover prediction model,” in Intelligent Computing: Proceedings of the\n2018 Computing Conference, Volume 2, Springer, 2019, pp. 142–159.\n[83]\nA. Alamsyah and N. Salma, “A comparative study of employee churn\nprediction model,” in 2018 4th International Conference on Science\nand Technology (ICST), IEEE, 2018, pp. 1–4.\n[84]\nS. S. Alduayj and K. Rajpoot, “Predicting employee attrition using\nmachine learning,” in 2018 International Conference on Innovations\nin Information Technology (IIT), IEEE, 2018, pp. 93–98.\n[85]\nR. Jain and A. Nayyar, “Predicting employee attrition using XG-\nBoost machine learning approach,” in 2018 International Conference\non System Modeling & Advancement in Research Trends (SMART),\nIEEE, 2018, pp. 113–120.\n[86]\nS. N. Khera and Divya, “Predictive modelling of employee turnover\nin Indian IT industry using machine learning techniques,” Vision,\nvol. 23, no. 1, pp. 12–21, 2018.\n[87]\nR. S. Shankar, J. Rajanikanth, V. Sivaramaraju, and K. Murthy,\n“Prediction of employee attrition using datamining,” in 2018 IEEE\nInternational Conference on System, Computation, Automation and\nNetworking (ICSCAN), IEEE, 2018, pp. 1–8.\n[88]\nS. Yadav, A. Jain, and D. Singh, “Early prediction of employee attri-\ntion using data mining techniques,” in 2018 IEEE 8th International\nAdvance Computing Conference (IACC), IEEE, 2018, pp. 349–354.\n[89]\nL. Alaskar, M. Crane, and M. Alduailij, “Employee turnover pre-\ndiction using machine learning,” in Advances in Data Science, Cy-\nber Security and IT Applications: First International Conference on\nComputing, ICC 2019, Riyadh, Saudi Arabia, December 10–12, 2019,\nProceedings, Part I 1, Springer, 2019, pp. 301–316.\n[90]\nX. Gao, J. Wen, and C. Zhang, “An improved random forest algo-\nrithm for predicting employee turnover,” Mathematical Problems in\nEngineering, vol. 2019, 2019.\n[91]\nN. Bhartiya, S. Jannu, P. Shukla, and R. Chapaneri, “Employee at-\ntrition prediction using classification models,” in 2019 IEEE 5th In-\nternational Conference for Convergence in Technology (I2CT), IEEE,\n2019, pp. 1–6.\n[92]\nJ. Vasa and K. Masrani, “Foreseeing employee attritions using diverse\ndata mining strategies,” International Journal of Recent Technology\nand Engineering, vol. 3, pp. 620–626, 2019.\n172\n References\n[93]\nY. Zhao, M. K. Hryniewicki, F. Cheng, B. Fu, and X. Zhu, “Employee\nturnover prediction with machine learning: A reliable approach,” in\nIntelligent Systems and Applications: Proceedings of the 2018 In-\ntelligent Systems Conference (IntelliSys) Volume 2, Springer, 2019,\npp. 737–758.\n[94]\nY. Sun, F. Zhuang, H. Zhu, X. Song, Q. He, and H. Xiong, “The im-\npact of person-organization fit on talent management: A structure-\naware convolutional neural network approach,” in Proceedings of the\n25th ACM SIGKDD International Conference on Knowledge Discov-\nery & Data Mining, 2019, pp. 1625–1633.\n[95]\nX. Cai, J. Shang, Z. Jin, et al., “DBGE: Employee turnover prediction\nbased on dynamic bipartite graph embedding,” IEEE Access, vol. 8,\npp. 10 390–10 402, 2020.\n[96]\nN. El-Rayes, M. Fang, M. Smith, and S. M. Taylor, “Predicting em-\nployee attrition using tree-based models,” International Journal of\nOrganizational Analysis, 2020.\n[97]\nF. Fallucchi, M. Coladangelo, R. Giuliano, and E. William De Luca,\n“Predicting employee attrition using machine learning techniques,”\nComputers, vol. 9, no. 4, p. 86, 2020.\n[98]\nP. K. Jain, M. Jain, and R. Pamula, “Explaining and predicting em-\nployees’ attrition: A machine learning approach,” SN Applied Sci-\nences, vol. 2, pp. 1–11, 2020.\n[99]\nZ. Jin, J. Shang, Q. Zhu, C. Ling, W. Xie, and B. Qiang, “RFRSF:\nEmployee turnover prediction based on random forests and survival\nanalysis,” in Web Information Systems Engineering–WISE 2020: 21st\nInternational Conference, Amsterdam, The Netherlands, October 20–\n24, 2020, Proceedings, Part II 21, Springer, 2020, pp. 503–515.\n[100]\nL. Liu, S. Akkineni, P. Story, and C. Davis, “Using HR analytics to\nsupport managerial decisions: A case study,” in Proceedings of the\n2020 ACM Southeast Conference, 2020, pp. 168–175.\n[101]\nA. Mhatre, A. Mahalingam, M. Narayanan, A. Nair, and S. Jaju,\n“Predicting employee attrition along with identifying high risk em-\nployees using big data and machine learning,” in 2020 2nd Interna-\ntional Conference on Advances in Computing, Communication Con-\ntrol and Networking (ICACCCN), IEEE, 2020, pp. 269–276.\n[102]\nF. Ozdemir, M. Coskun, C. Gezer, and V. C. Gungor, “Assessing\nemployee attrition using classifications algorithms,” in Proceedings of\nthe 2020 the 4th International Conference on Information System and\nData Mining, 2020, pp. 118–122.\n173\n References\n[103]\nS. Al-Darraji, D. G. Honi, F. Fallucchi, A. I. Abdulsada, R. Giuliano,\nand H. A. Abdulmalik, “Employee attrition prediction using deep\nneural networks,” Computers, vol. 10, no. 11, p. 141, 2021.\n[104]\nN. B. Yahia, J. Hlel, and R. Colomo-Palacios, “From big data to deep\ndata to support people analytics for employee attrition prediction,”\nIEEE Access, vol. 9, pp. 60 447–60 458, 2021.\n[105]\nR. Chakraborty, K. Mridha, R. N. Shaw, and A. Ghosh, “Study and\nprediction analysis of the employee turnover using machine learning\napproaches,” in 2021 IEEE 4th International Conference on Comput-\ning, Power and Communication Technologies (GUCON), IEEE, 2021,\npp. 1–6.\n[106]\nT. Juvitayapun, “Employee turnover prediction: The impact of em-\nployee event features on interpretable machine learning methods,” in\n2021 13th international conference on knowledge and smart technol-\nogy (kst), IEEE, 2021, pp. 181–185.\n[107]\nN. Jain, A. Tomar, and P. K. Jana, “A novel scheme for employee\nchurn problem using multi-attribute decision making approach and\nmachine learning,” Journal of Intelligent Information Systems, vol. 56,\npp. 279–302, 2021.\n[108]\nN. Mansor, N. S. Sani, and M. Aliff, “Machine learning for predict-\ning employee attrition,” International Journal of Advanced Computer\nScience and Applications, vol. 12, no. 11, 2021.\n[109]\nS. Najafi-Zangeneh, N. Shams-Gharneh, A. Arjomandi-Nezhad, and\nS. Hashemkhani Zolfani, “An improved machine learning-based em-\nployees attrition prediction framework with emphasis on feature se-\nlection,” Mathematics, vol. 9, no. 11, p. 1226, 2021.\n[110]\nM. Pratt, M. Boudhane, and S. Cakula, “Employee attrition esti-\nmation using random forest algorithm,” Baltic Journal of Modern\nComputing, vol. 9, no. 1, pp. 49–66, 2021.\n[111]\nP. R. Srivastava and P. Eachempati, “Intelligent employee retention\nsystem for attrition rate analysis and churn prediction: An ensem-\nble machine learning and multi-criteria decision-making approach,”\nJournal of Global Information Management (JGIM), vol. 29, no. 6,\npp. 1–29, 2021.\n[112]\nZ. Tao, C. Wu, and S. Zhao, “Research on the prediction of employee\nturnover behavior and its interpretability,” in Proceedings of the 2021\n5th International Conference on Electronic Information Technology\nand Computer Engineering, 2021, pp. 760–767.\n174\n References\n[113]\nX. Wang and J. Zhi, “A machine learning-based analytical framework\nfor employee turnover prediction,” Journal of Management Analytics,\nvol. 8, no. 3, pp. 351–370, 2021.\n[114]\nA. B. Wild Ali, “Prediction of employee turn over using random forest\nclassifier with intensive optimized pca algorithm,” Wireless Personal\nCommunications, vol. 119, no. 4, pp. 3365–3382, 2021.\n[115]\nQ. Zhu, J. Shang, X. Cai, L. Jiang, F. Liu, and B. Qiang, “CoxRF:\nEmployee turnover prediction based on survival analysis,” in 2019\nIEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced\n& Trusted Computing, Scalable Computing & Communications, Cloud\n& Big Data Computing, Internet of People and Smart City Innovation\n(SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), IEEE, 2019,\npp. 1123–1130.\n[116]\nF. K. Alsheref, I. E. Fattoh, and W. M Ead, “Automated prediction of\nemployee attrition using ensemble model based on machine learning\nalgorithms,” Computational Intelligence and Neuroscience, vol. 2022,\n2022.\n[117]\nS. M. Arqawi, M. A. A. Rumman, E. A. Zitawi, et al., “Predict-\ning employee attrition and performance using deep learning,” Journal\nof Theoretical and Applied Information Technology, vol. 100, no. 21,\n2022.\n[118]\nS. Bhatta, I. U. Zaman, N. Raisa, S. I. Fahim, and S. Momen, “Ma-\nchine learning approach to predicting attrition among employees at\nwork,” in Artificial Intelligence Trends in Systems: Proceedings of\n11th Computer Science On-line Conference 2022, Vol. 2, Springer,\n2022, pp. 285–294.\n[119]\nK. Naz, I. F. Siddiqui, J. Koo, M. A. Khan, and N. M. F. Qureshi,\n“Predictive modeling of employee churn analysis for IoT-enabled soft-\nware industry,” Applied Sciences, vol. 12, no. 20, p. 10 495, 202",
  "44": "\n(SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), IEEE, 2019,\npp. 1123–1130.\n[116]\nF. K. Alsheref, I. E. Fattoh, and W. M Ead, “Automated prediction of\nemployee attrition using ensemble model based on machine learning\nalgorithms,” Computational Intelligence and Neuroscience, vol. 2022,\n2022.\n[117]\nS. M. Arqawi, M. A. A. Rumman, E. A. Zitawi, et al., “Predict-\ning employee attrition and performance using deep learning,” Journal\nof Theoretical and Applied Information Technology, vol. 100, no. 21,\n2022.\n[118]\nS. Bhatta, I. U. Zaman, N. Raisa, S. I. Fahim, and S. Momen, “Ma-\nchine learning approach to predicting attrition among employees at\nwork,” in Artificial Intelligence Trends in Systems: Proceedings of\n11th Computer Science On-line Conference 2022, Vol. 2, Springer,\n2022, pp. 285–294.\n[119]\nK. Naz, I. F. Siddiqui, J. Koo, M. A. Khan, and N. M. F. Qureshi,\n“Predictive modeling of employee churn analysis for IoT-enabled soft-\nware industry,” Applied Sciences, vol. 12, no. 20, p. 10 495, 2022.\n[120]\nE. Pekel Ozmen and T. Ozcan, “A novel deep learning model based on\nconvolutional neural networks for employee churn prediction,” Jour-\nnal of Forecasting, vol. 41, no. 3, pp. 539–550, 2022.\n[121]\nM. Prathilothamai, A. Sri Sakthi Maheswari, A. Chandravadhana,\nand R. Goutham, “Efficient approach to employee attrition prediction\nby handling class imbalance,” in Advances in Computing and Data\nSciences: 6th International Conference, ICACDS 2022, Kurnool, In-\ndia, April 22–23, 2022, Revised Selected Papers, Part II, Springer,\n2022, pp. 263–277.\n175\n References\n[122]\nA. Raza, K. Munir, M. Almutairi, F. Younas, and M. M. S. Fareed,\n“Predicting employee attrition using machine learning approaches,”\nApplied Sciences, vol. 12, no. 13, p. 6424, 2022.\n[123]\nS. R. Seelam, K. H. Kumar, M. S. Supritha, G. Gnaneswar, and\nV. V. M. Reddy, “Comparative study of predictive models to estimate\nemployee attrition,” in 2022 7th International Conference on Com-\nmunication and Electronics Systems (ICCES), IEEE, 2022, pp. 1602–\n1607.\n[124]\nD. Chung, J. Yun, J. Lee, and Y. Jeon, “Predictive model of employee\nattrition based on stacking ensemble learning,” Expert Systems with\nApplications, vol. 215, p. 119 364, 2023.\n[125]\nF. Guerranti and G. M. Dimitri, “A comparison of machine learn-\ning approaches for predicting employee attrition,” Applied Sciences,\nvol. 13, no. 1, p. 267, 2023.\n[126]\nC. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine\nlearning. Springer, 2006, vol. 4.\n[127]\nG. E. Batista, R. C. Prati, and M. C. Monard, “A study of the behav-\nior of several methods for balancing machine learning training data,”\nSIGKDD Explorations, vol. 6, no. 1, pp. 20–29, 2004.\n[128]\nW. Verbeke, K. Dejaeger, D. Martens, J. Hur, and B. Baesens, “New\ninsights into churn prediction in the telecommunication sector: A\nprofit driven data mining approach,” European Journal of Operational\nResearch, vol. 218, no. 1, pp. 211–229, 2012.\n[129]\nC. Bockel-Rickermann, T. Verdonck, and W. Verbeke, “Fraud analyt-\nics: A decade of research organizing challenges and solutions in the\nfield,” Expert Systems with Applications, p. 120 605, 2023.\n[130]\nS. Lessmann, B. Baesens, H.-V. Seow, and L. C. Thomas, “Bench-\nmarking state-of-the-art classification algorithms for credit scoring:\nAn update of research,” European Journal of Operational Research,\nvol. 247, no. 1, pp. 124–136, 2015.\n[131]\nJ. Demšar, “Statistical comparisons of classifiers over multiple data\nsets,” The Journal of Machine Learning Research, vol. 7, pp. 1–30,\n2006.\n[132]\nI. Wod, “Weight of evidence: A brief survey,” Bayesian Statistics,\nvol. 2, pp. 249–270, 1985.\n[133]\nD. J. Hand, “Measuring classifier performance: A coherent alternative\nto the area under the ROC curve,” Machine Learning, vol. 77, no. 1,\npp. 103–123, 2009.\n176\n References\n[134]\nR. Van Belle, B. Baesens, and J. De Weerdt, “CATCHM: A novel\nnetwork-based credit card fraud detection method using node repre-\nsentation learning,” Decision Support Systems, vol. 164, p. 113 866,\n2023.\n[135]\nY. Hochberg, “A sharper Bonferroni procedure for multiple tests of\nsignificance,” Biometrika, vol. 75, no. 4, pp. 800–802, Dec. 1988, issn:\n0006-3444. doi: 10 . 1093 / biomet / 75 . 4 . 800. eprint: https : / /\nacademic.oup.com/biomet/article-pdf/75/4/800/1170595/75-\n4-800.pdf. [Online]. Available: https://doi.org/10.1093/biomet/\n75.4.800.\n[136]\nY. Freund and R. E. Schapire, “A decision-theoretic generalization of\non-line learning and an application to boosting,” Journal of Computer\nand System Sciences, vol. 55, no. 1, pp. 119–139, 1997.\n[137]\nT. Chen and C. Guestrin, “XGBoost: A scalable tree boosting sys-\ntem,” in Proceedings of the 22nd ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining, 2016, pp. 785–\n794.\n[138]\nF. Provost, “Machine learning from imbalanced data sets 101,” 2008.\n[139]\nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,\n“SMOTE: Synthetic minority over-sampling technique,” Journal of\nArtificial Intelligence Research, vol. 16, pp. 321–357, 2002.\n[140]\nH. He, Y. Bai, E. A. Garcia, and S. Li, “ADASYN: Adaptive synthetic\nsampling approach for imbalanced learning,” in 2008 IEEE Interna-\ntional Joint Conference on Neural Networks (IEEE World Congress\non Computational Intelligence), IEEE, 2008, pp. 1322–1328.\n[141]\nR. A. Bauder, T. M. Khoshgoftaar, and T. Hasanin, “Data sampling\napproaches with severely imbalanced big data for medicare fraud de-\ntection,” in 2018 IEEE 30th International Conference on Tools with\nArtificial Intelligence (ICTAI), IEEE, 2018, pp. 137–142.\n[142]\nR. Bauder and T. Khoshgoftaar, “Medicare fraud detection using ran-\ndom forest with class imbalanced big data,” in 2018 IEEE Interna-\ntional Conference on Information Reuse and Integration (IRI), IEEE,\n2018, pp. 80–87.\n[143]\nD. Martens, J. Vanthienen, W. Verbeke, and B. Baesens, “Perfor-\nmance of classification models from a user perspective,” Decision Sup-\nport Systems, vol. 51, no. 4, pp. 782–793, 2011.\n[144]\nM. Craven and J. Shavlik, “Extracting tree-structured representations\nof trained networks,” Advances in Neural Information Processing Sys-\ntems, vol. 8, 1995.\n177\n References\n[145]\nH. Blockeel, L. Devos, B. Frénay, G. Nanfack, and S. Nijssen, “Deci-\nsion trees: From efficient prediction to responsible AI,” Frontiers in\nArtificial Intelligence, vol. 6, 2023.\n[146]\nA. Fisher, C. Rudin, and F. Dominici, “All models are wrong, but\nmany are useful: Learning a variable’s importance by studying an\nentire class of prediction models simultaneously,” Journal of Machine\nLearning Research, vol. 20, no. 177, pp. 1–81, 2019.\n[147]\nA. Benlian, M. Wiener, W. A. Cram, et al., “Algorithmic manage-\nment: Bright and dark sides, practical implications, and research op-\nportunities,” Business & Information Systems Engineering, vol. 64,\nno. 6, pp. 825–839, 2022.\n[148]\nD. Martens and F. Provost, “Explaining data-driven document clas-\nsifications,” MIS Quarterly, vol. 38, no. 1, pp. 73–100, 2014.\n[149]\nS. De Winne, E. Marescaux, L. Sels, I. Van Beveren, and S. Vanormelin-\ngen, “The impact of employee turnover and turnover volatility on la-\nbor productivity: A flexible non-linear approach,” The International\nJournal of Human Resource Management, vol. 30, no. 21, pp. 3049–\n3079, 2019.\n[150]\nF. Provost and T. Fawcett, “Data science and its relationship to big\ndata and data-driven decision making,” en, Big Data, vol. 1, no. 1,\npp. 51–59, Mar. 2013.\n[151]\nC. Holsapple, A. Lee-Post, and R. Pakath, “A unified foundation for\nbusiness analytics,” Decision Support Systems, vol. 64, pp. 130–141,\nAug. 2014.\n[152]\nA. Behl, P. Dutta, S. Lessmann, Y. K. Dwivedi, and S. Kar, “A\nconceptual framework for the adoption of big data analytics by e-\ncommerce startups: A case-based approach,” Information systems and\ne-business management, vol. 17, pp. 285–318, 2019.\n[153]\nD. Angrave, A. Charlwood, I. Kirkpatrick, M. Lawrence, and M. Stu-\nart, “HR and analytics: Why HR is set to fail the big data challenge,”\nen, Human Resource Management Journal, vol. 26, no. 1, pp. 1–11,\nJan. 2016.\n[154]\nJ. H. Marler and J. W. Boudreau, “An evidence-based review of HR\nanalytics,” en, The International Journal of Human Resource Man-\nagement, vol. 28, no. 1, pp. 3–26, Jan. 2017.\n[155]\nT. Rasmussen and D. Ulrich, “Learning from practice: How HR an-\nalytics avoids being a management fad,” Organizational Dynamics,\nvol. 44, no. 3, pp. 236–242, Jul. 2015.\n178\n References\n[156]\nD. G. Collings, K. Mellahi, and W. F. Cascio, The Oxford Handbook\nof Talent Management, en. Oxford University Press, 2017, pp. 283–\n300.\n[157]\nP. Cappelli, “Talent on demand: Managing talent in an uncertain\nage,” Harvard Business School Press, Boston, MA, 2008.\n[158]\nP. Osterman, “Choice of employment systems in internal labor mar-\nkets,” Industrial Relations: A Journal of Economy and Society, vol. 26,\nno. 1, pp. 46–67, 1987.\n[159]\nP. G. O’Shea and K. E. Puente, “How is technology changing talent\nmanagement?” en, in The Oxford Handbook of Talent Management,\nD. G. Collings, K. Mellahi, and W. F. Cascio, Eds., Oxford University\nPress, Sep. 2017.\n[160]\nP. Rogiers, S. Viaene, and J. Leysen, “The digital future of inter-\nnal staffing: A vision for transformational electronic human resource\nmanagement,” Intelligent Systems in Accounting, Finance and Man-\nagement, vol. 27, no. 4, pp. 182–196, 2020.\n[161]\nM. J. Belizón and S. Kieran, “Human resources analytics: A legiti-\nmacy process,” Human Resource Management Journal, vol. 32, no. 3,\npp. 603–630, 2022.\n[162]\nA. De Vos, S. Jacobs, and M. Verbruggen, “Career transitions and\nemployability,” Journal of Vocational Behavior, vol. 126, p. 103 475,\n2021.\n[163]\nM. Verbruggen, R. De Cooman, and S. Vansteenkiste, “When and\nwhy are internal job transitions successful: Transition challenges, hin-\ndrances, and resources influencing motivation and retention through\nbasic needs satisfaction,” Group & Organization Management, vol. 40,\nno. 6, pp. 744–775, 2015.\n[164]\nJ. Lu, D. Wu, M. Mao, W. Wang, and G. Zhan",
  "45": "t,\nD. G. Collings, K. Mellahi, and W. F. Cascio, Eds., Oxford University\nPress, Sep. 2017.\n[160]\nP. Rogiers, S. Viaene, and J. Leysen, “The digital future of inter-\nnal staffing: A vision for transformational electronic human resource\nmanagement,” Intelligent Systems in Accounting, Finance and Man-\nagement, vol. 27, no. 4, pp. 182–196, 2020.\n[161]\nM. J. Belizón and S. Kieran, “Human resources analytics: A legiti-\nmacy process,” Human Resource Management Journal, vol. 32, no. 3,\npp. 603–630, 2022.\n[162]\nA. De Vos, S. Jacobs, and M. Verbruggen, “Career transitions and\nemployability,” Journal of Vocational Behavior, vol. 126, p. 103 475,\n2021.\n[163]\nM. Verbruggen, R. De Cooman, and S. Vansteenkiste, “When and\nwhy are internal job transitions successful: Transition challenges, hin-\ndrances, and resources influencing motivation and retention through\nbasic needs satisfaction,” Group & Organization Management, vol. 40,\nno. 6, pp. 744–775, 2015.\n[164]\nJ. Lu, D. Wu, M. Mao, W. Wang, and G. Zhang, “Recommender sys-\ntem application developments: A survey,” Decision Support Systems,\nvol. 74, pp. 12–32, 2015.\n[165]\nD. Wei, K. R. Varshney, and M. Wagman, “Optigrow: People ana-\nlytics for job transfers,” in 2015 IEEE International Congress on Big\nData, IEEE, 2015, pp. 535–542.\n[166]\nC. de Ruijt and S. Bhulai, “Job recommender systems: A review,”\narXiv preprint arXiv:2111.13576, 2021.\n[167]\nW. Van Der Aalst, “Process mining,” Communications of the ACM,\nvol. 55, no. 8, pp. 76–83, Aug. 2012.\n179\n References\n[168]\nJ. Lu, D. Wu, M. Mao, W. Wang, and G. Zhang, “Recommender sys-\ntem application developments: A survey,” Decision Support Systems,\nvol. 74, pp. 12–32, Jun. 2015.\n[169]\nB. Lika, K. Kolomvatsos, and S. Hadjiefthymiades, “Facing the cold\nstart problem in recommender systems,” Expert systems with appli-\ncations, vol. 41, no. 4, pp. 2065–2073, 2014.\n[170]\nJ. Garcia-Arroyo and A. Osca, “Big data contributions to human re-\nsource management: A systematic review,” The International Journal\nof Human Resource Management, vol. 32, no. 20, pp. 4337–4362, Nov.\n2021.\n[171]\nE. Ribes, K. Touahri, and B. Perthame, “Employee turnover pre-\ndiction and retention policies design: A case study,” arXiv preprint\narXiv:1707.01377, 2017.\n[172]\nP. Ajit, “Prediction of employee turnover in organizations using ma-\nchine learning algorithms,” algorithms, vol. 4, no. 5, p. C5, 2016.\n[173]\nC.-F. Chien and L.-F. Chen, “Data mining to improve personnel se-\nlection and enhance human capital: A case study in high-technology\nindustry,” Expert Systems with Applications, vol. 34, no. 1, pp. 280–\n290, Jan. 2008.\n[174]\nK.-Y. Wang and H.-Y. Shun, “Applying back propagation neural net-\nworks in the prediction of management associate work retention for\nsmall and medium enterprises,” Universal Journal of Management,\nvol. 4, no. 5, pp. 223–227, 2016.\n[175]\nX.-L. Qu, “A decision tree applied to the grass-roots staffs’ turnover\nproblem—take cr group as an example,” in 2015 IEEE International\nConference on Grey Systems and Intelligent Services (GSIS), IEEE,\n2015, pp. 378–382.\n[176]\nE. Sikaroudi, A. Mohammad, R. Ghousi, and A. Sikaroudi, “A data\nmining approach to employee turnover prediction (case study: Arak\nautomotive parts manufacturing),” International Journal of Indus-\ntrial and Systems Engineering, vol. 8, no. 4, pp. 106–121, 2015.\n[177]\nX. Gui, Z. Hu, J. Zhang, and Y. Bao, “Assessing personal performance\nwith M-SVMs,” in 2014 Seventh International Joint Conference on\nComputational Sciences and Optimization, Jul. 2014, pp. 598–601.\n[178]\nC.-Y. Fan, P.-S. Fan, T.-Y. Chan, and S.-H. Chang, “Using hybrid\ndata mining and machine learning clustering analysis to predict the\nturnover rate for technology professionals,” Expert Systems with Ap-\nplications, vol. 39, no. 10, pp. 8844–8851, Aug. 2012.\n180\n References\n[179]\nY.-M. Li, C.-Y. Lai, and C.-P. Kao, “Incorporate personality trait\nwith support vector machine to acquire quality matching of personnel\nrecruitment,” in 4th international conference on business and infor-\nmation, 2008, pp. 1–11.\n[180]\nS. Mehta, R. Pimplikar, A. Singh, L. R. Varshney, and K. Visweswariah,\n“Efficient multifaceted screening of job applicants,” in Proceedings of\nthe 16th International Conference on Extending Database Technology,\n2013, pp. 661–671.\n[181]\nJ. C. Sesil, Applying Advanced Analytics to HR Management Deci-\nsions: Methods for Selection, Developing Incentives, and Improving\nCollaboration (Paperback). FT Press, 2013.\n[182]\nE. Rombaut and M.-A. Guerry, “Predicting voluntary turnover through\nhuman resources database analysis,” Management Research Review,\nvol. 41, no. 1, pp. 96–112, 2018.\n[183]\nV. V. Saradhi and G. K. Palshikar, “Employee churn prediction,”\nExpert Systems with Applications, vol. 38, no. 3, pp. 1999–2006, Mar.\n2011.\n[184]\nY. Zhao, M. K. Hryniewicki, F. Cheng, B. Fu, and X. Zhu, “Employee\nturnover prediction with machine learning: A reliable approach,” in\nProceedings of SAI intelligent systems conference, Springer, 2018,\npp. 737–758.\n[185]\nS. H. Dolatabadi and F. Keynia, “Designing of customer and employee\nchurn prediction model based on data mining method and neural\npredictor,” en, in 2017 2nd International Conference on Computer\nand Communication Systems (ICCCS), Krakow, Poland: IEEE, Jul.\n2017.\n[186]\nS. N. Mishra, D. R. Lama, Y. Pal, et al., “Human resource predictive\nanalytics (hrpa) for hr management in organizations,” International\nJournal of Scientific & Technology Research, vol. 5, no. 5, pp. 33–35,\n2016.\n[187]\nA. Levenson and A. Fink, “Human capital analytics: Too much data\nand analysis, not enough models and business insights,” Journal of\nOrganizational Effectiveness: People and Performance, vol. 4, no. 2,\npp. 145–156, Jan. 2017.\n[188]\nT. Peeters, J. Paauwe, and K. Van De Voorde, “People analytics ef-\nfectiveness: Developing a framework,” en, Journal of Organizational\nEffectiveness: People and Performance, vol. 7, no. 2, pp. 203–219, Jul.\n2020.\n181\n References\n[189]\nW. A. Schiemann, J. H. Seibert, and M. H. Blankenship, “Putting\nhuman capital analytics to work: Predicting and driving business suc-\ncess,” Hum. Resour. Manage., vol. 57, no. 3, pp. 795–807, May 2018.\n[190]\nC. B.-G. Hila, “An ROI-based review of HR analytics: Practical im-\nplementation tools,” Personnel Review, vol. 48, no. 6, pp. 1429–1448,\nJan. 2019.\n[191]\nD. B. Minbaeva, “Building credible human capital analytics for or-\nganizational competitive advantage,” Human Resource Management,\nvol. 57, no. 3, pp. 701–713, May 2018.\n[192]\nM. R. Edwards and K. Edwards, Predictive HR analytics: Mastering\nthe HR metric. Kogan Page Publishers, 2019.\n[193]\nJ. Wang, Y. Zhang, C. Posse, and A. Bhasin, “Is it time for a career\nswitch?” In Proceedings of the 22nd international conference on World\nWide Web, 2013, pp. 1377–1388.\n[194]\nB. Heap, A. Krzywicki, W. Wobcke, M. Bain, and P. Compton, “Com-\nbining career progression and profile matching in a job recommender\nsystem,” in PRICAI 2014: Trends in Artificial Intelligence: 13th Pa-\ncific Rim International Conference on Artificial Intelligence, Gold\nCoast, QLD, Australia, December 1-5, 2014. Proceedings 13, Springer,\n2014, pp. 396–408.\n[195]\nC. Zhu, H. Zhu, H. Xiong, et al., “Person-job fit: Adapting the right\ntalent for the right job with joint representation learning,” ACM\nTransactions on Management Information Systems (TMIS), vol. 9,\nno. 3, pp. 1–17, 2018.\n[196]\nM. Liu, J. Wang, K. Abdelfatah, and M. Korayem, “Tripartite vec-\ntor representations for better job recommendation,” arXiv preprint\narXiv:1907.12379, 2019.\n[197]\nS. Bian, X. Chen, W. X. Zhao, et al., “Learning to match jobs with\nresumes from sparse interaction data using multi-view co-teaching\nnetwork,” in Proceedings of the 29th ACM International Conference\non Information & Knowledge Management, 2020, pp. 65–74.\n[198]\nC. Qin, H. Zhu, T. Xu, et al., “An enhanced neural network approach\nto person-job fit in talent recruitment,” ACM Transactions on Infor-\nmation Systems (TOIS), vol. 38, no. 2, pp. 1–33, 2020.\n[199]\nA. Giabelli, L. Malandri, F. Mercorio, M. Mezzanzanica, and A.\nSeveso, “Skills2job: A recommender system that encodes job offer\nembeddings on graph databases,” Applied Soft Computing, vol. 101,\np. 107 049, 2021.\n182\n References\n[200]\nT. A.-O. Shaha and Y. Mourad, “A survey of job recommender sys-\ntems,” International Journal of Physical Sciences, vol. 7, no. 29, pp. 5127–\n5142, 2012.\n[201]\nZ. Siting, H. Wenxing, Z. Ning, and Y. Fan, “Job recommender sys-\ntems: A survey,” in 2012 7th International Conference on Computer\nScience & Education (ICCSE), Jul. 2012, pp. 920–924.\n[202]\nM. N. Freire and L. N. de Castro, “E-recruitment recommender sys-\ntems: A systematic review,” Knowledge and Information Systems,\nvol. 63, no. 1, pp. 1–20, 2021.\n[203]\nP. K. Roy, S. S. Chowdhary, and R. Bhatia, “A machine learning ap-\nproach for automation of resume recommendation system,” Procedia\nComputer Science, vol. 167, pp. 2318–2327, 2020.\n[204]\nA. Janusz, S. Stawicki, M. Drewniak, K. Ciebiera, D. Ślęzak, and K.\nStencel, “How to match jobs and candidates - a recruitment support\nsystem based on feature engineering and advanced analytics,” in In-\nformation Processing and Management of Uncertainty in Knowledge-\nBased Systems. Theory and Foundations, Springer International Pub-\nlishing, 2018, pp. 503–514.\n[205]\nL. Zhang, D. Zhou, H. Zhu, et al., “Attentive heterogeneous graph\nembedding for job mobility prediction,” in Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery & Data Mining,\nser. KDD ’21, Virtual Event, Singapore: Association for Computing\nMachinery, Aug. 2021, pp. 2192–2201.\n[206]\nJ. Bollinger, D. Hardtke, and B. Martin, “Using social data for resume\njob matching,” in Proceedings of the 2012 workshop on Data-driven\nuser behavioral modelling and mining from social media, ser. DUB-\nMMSM ’12, Maui, Hawaii, USA: Association for Computing Machin-\nery, Oct. 2012, pp. 27–30.\n[207]\nB. J. Bret, H. J. Walker, J. B. Gilstrap, and P. H. Schwager, “Social\nmedia",
  "46": "era, D. Ślęzak, and K.\nStencel, “How to match jobs and candidates - a recruitment support\nsystem based on feature engineering and advanced analytics,” in In-\nformation Processing and Management of Uncertainty in Knowledge-\nBased Systems. Theory and Foundations, Springer International Pub-\nlishing, 2018, pp. 503–514.\n[205]\nL. Zhang, D. Zhou, H. Zhu, et al., “Attentive heterogeneous graph\nembedding for job mobility prediction,” in Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery & Data Mining,\nser. KDD ’21, Virtual Event, Singapore: Association for Computing\nMachinery, Aug. 2021, pp. 2192–2201.\n[206]\nJ. Bollinger, D. Hardtke, and B. Martin, “Using social data for resume\njob matching,” in Proceedings of the 2012 workshop on Data-driven\nuser behavioral modelling and mining from social media, ser. DUB-\nMMSM ’12, Maui, Hawaii, USA: Association for Computing Machin-\nery, Oct. 2012, pp. 27–30.\n[207]\nB. J. Bret, H. J. Walker, J. B. Gilstrap, and P. H. Schwager, “Social\nmedia snooping on job applicants: The effects of unprofessional so-\ncial media information on recruiter perceptions,” Personnel Review,\nvol. 48, no. 5, pp. 1261–1280, Jan. 2019.\n[208]\nR. Slovensky and W. H. Ross, “Should human resource managers use\nsocial media to screen job applicants? managerial and legal issues in\nthe USA,” Info, vol. 14, no. 1, pp. 55–69, Jan. 2012.\n[209]\nJ. Dastin, “Amazon scraps secret ai recruiting tool that showed bias\nagainst women,” in Ethics of Data and Analytics, Auerbach Publica-\ntions, 2018, pp. 296–299.\n183\n References\n[210]\nW. Samek and K.-R. Müller, “Towards explainable artificial intelli-\ngence,” in Explainable AI: Interpreting, Explaining and Visualizing\nDeep Learning, W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen,\nand K.-R. Müller, Eds., Cham: Springer International Publishing,\n2019, pp. 5–22.\n[211]\nA. Stevens, J. De Smedt, and J. Peeperkorn, “Quantifying explain-\nability in outcome-oriented predictive process monitoring,” in Process\nMining Workshops, J. Munoz-Gama and X. Lu, Eds., Cham: Springer\nInternational Publishing, 2022, pp. 194–206, isbn: 978-3-030-98581-3.\n[212]\nA. V. Konstantinov and L. V. Utkin, “Interpretable machine learning\nwith an ensemble of gradient boosting machines,” Knowledge-Based\nSystems, vol. 222, p. 106 993, 2021.\n[213]\nS. De Vos, J. De Smedt, C. Wuytens, and W. Verbeke, “Leveraging\nprocess mining to optimize internal employee mobility strategies,” in\nBusiness Process Management Cases Vol. 3, Springer, 2024 (forth-\ncoming).\n[214]\nW. Van Der Aalst, A. Adriansyah, A. K. A. De Medeiros, et al., “Pro-\ncess mining manifesto,” in Business Process Management Workshops:\nBPM 2011 International Workshops, Clermont-Ferrand, France, Au-\ngust 29, 2011, Revised Selected Papers, Part I 9, Springer, 2012,\npp. 169–194.\n[215]\nJ. Bobadilla, F. Ortega, A. Hernando, and A. Gutiérrez, “Recom-\nmender systems survey,” Knowledge-Based Systems, vol. 46, pp. 109–\n132, 2013.\n[216]\nH. Liu, Z. Hu, A. Mian, H. Tian, and X. Zhu, “A new user similarity\nmodel to improve the accuracy of collaborative filtering,” Knowledge-\nbased systems, vol. 56, pp. 156–166, 2014.\n[217]\nA. A. Amer, H. I. Abdalla, and L. Nguyen, “Enhancing recommenda-\ntion systems performance using highly-effective similarity measures,”\nKnowledge-Based Systems, vol. 217, p. 106 842, 2021.\n[218]\nH. Khojamli and J. Razmara, “Survey of similarity functions on neighborhood-\nbased collaborative filtering,” Expert Systems with Applications, vol. 185,\np. 115 482, 2021.\n[219]\nA. Paterek, “Improving regularized singular value decomposition for\ncollaborative filtering,” in Proceedings of KDD cup and workshop,\nvol. 2007, 2007, pp. 5–8.\n[220]\nG. Takács, I. Pilászy, B. Németh, and D. Tikk, “Major components\nof the gravity recommendation system,” Acm Sigkdd Explorations\nNewsletter, vol. 9, no. 2, pp. 80–83, 2007.\n184\n References\n[221]\nH. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King, “Recommender\nsystems with social regularization,” in Proceedings of the fourth ACM\ninternational conference on Web search and data mining, ser. WSDM\n’11, Hong Kong, China: Association for Computing Machinery, Feb.\n2011, pp. 287–296.\n[222]\nM. Meire, M. Ballings, and D. Van den Poel, “The added value of\nsocial media data in b2b customer acquisition systems: A real-life\nexperiment,” Decision Support Systems, vol. 104, pp. 26–37, 2017.\n[223]\nC. Stanfill and D. Waltz, “Toward memory-based reasoning,” Com-\nmunications of the ACM, vol. 29, no. 12, pp. 1213–1228, Dec. 1986.\n[224]\nP. Tambe, P. Cappelli, and V. Yakubovich, “Artificial intelligence\nin human resources management: Challenges and a path forward,”\nCalifornia Management Review, vol. 61, no. 4, pp. 15–42, 2019.\n[225]\nA. J. Bowlus, “Matching workers and jobs: Cyclical fluctuations in\nmatch quality,” Journal of Labor Economics, vol. 13, no. 2, pp. 335–\n350, 1995.\n[226]\nB. Jovanovic, “Firm-specific capital and turnover,” Journal of political\neconomy, vol. 87, no. 6, pp. 1246–1260, 1979.\n[227]\nG. A. Akerlof, A. K. Rose, J. L. Yellen, L. Ball, and R. E. Hall,\n“Job switching and job satisfaction in the us labor market,” Brookings\npapers on economic activity, vol. 1988, no. 2, pp. 495–594, 1988.\n[228]\nM. J. Van der Laan and J. M. Robins, Unified methods for censored\nlongitudinal data and causality. Springer, 2003, vol. 5.\n[229]\nS. Geuens, K. Coussement, and K. W. De Bock, “A framework for con-\nfiguring collaborative filtering-based recommendations derived from\npurchase data,” European Journal of Operational Research, vol. 265,\nno. 1, pp. 208–218, 2018.\n[230]\nM. Bogaert, J. Lootens, D. Van den Poel, and M. Ballings, “Evaluat-\ning multi-label classifiers and recommender systems in the financial\nservice sector,” European Journal of Operational Research, vol. 279,\nno. 2, pp. 620–634, 2019.\n[231]\nD. Lemire and A. Maclachlan, “Slope one predictors for online rating-\nbased collaborative filtering,” in Proceedings of the 2005 SIAM Inter-\nnational Conference on Data Mining, SIAM, 2005, pp. 471–475.\n[232]\nN. Hug, “Surprise: A python library for recommender systems,” Jour-\nnal of Open Source Software, vol. 5, no. 52, p. 2174, 2020. doi: 10.\n21105/joss.02174. [Online]. Available: https://doi.org/10.\n21105/joss.02174.\n185\n References\n[233]\nS. Holm, “A simple sequentially rejective multiple test procedure,”\nScandinavian journal of statistics, pp. 65–70, 1979.\n[234]\nY. Koren and R. Bell, “Advances in collaborative filtering,” Recom-\nmender Systems Handbook, pp. 77–118, 2015.\n[235]\nL. Van der Maaten and G. Hinton, “Visualizing data using t-sne.,”\nJournal of machine learning research, vol. 9, no. 11, 2008.\n[236]\nM. Ge, C. Delgado-Battenfeld, and D. Jannach, “Beyond accuracy:\nEvaluating recommender systems by coverage and serendipity,” in\nProceedings of the fourth ACM conference on Recommender systems,\nser. RecSys ’10, Barcelona, Spain: Association for Computing Ma-\nchinery, Sep. 2010, pp. 257–260.\n[237]\nL. Iaquinta, M. de Gemmis, P. Lops, G. Semeraro, M. Filannino, and\nP. Molino, “Introducing serendipity in a Content-Based recommender\nsystem,” in 2008 Eighth International Conference on Hybrid Intelli-\ngent Systems, Sep. 2008, pp. 168–173.\n[238]\nD. Kotkov, S. Wang, and J. Veijalainen, “A survey of serendipity in\nrecommender systems,” Knowledge-Based Systems, vol. 111, pp. 180–\n192, Nov. 2016.\n[239]\nB. M. Marlin and R. S. Zemel, “Collaborative prediction and rank-\ning with non-random missing data,” in Proceedings of the third ACM\nconference on Recommender systems, ser. RecSys ’09, New York, New\nYork, USA: Association for Computing Machinery, Oct. 2009, pp. 5–\n12.\n[240]\nE. Bareinboim and J. Pearl, “Controlling selection bias in causal in-\nference,” Proceedings of Machine Learning Research, vol. 22, N. D.\nLawrence and M. Girolami, Eds., pp. 100–108, 2012.\n[241]\nW. Verbeke, D. Olaya, J. Berrevoets, S. Verboven, and S. Maldonado,\n“The foundations of cost-sensitive causal classification,” Jul. 2020.\narXiv: 2007.12582 [cs.LG].\n[242]\nG. Petrides, D. Moldovan, L. Coenen, T. Guns, and W. Verbeke,\n“Cost-sensitive learning for profit-driven credit scoring,” J. Oper. Res.\nSoc., vol. 73, no. 2, pp. 338–350, 2022.\n[243]\nS. Lessmann, J. Haupt, K. Coussement, and K. W. De Bock, “Target-\ning customers for profit: An ensemble learning framework to support\nmarketing decision-making,” Information Sciences, vol. 557, pp. 286–\n301, 2021.\n[244]\nG. Petrides and W. Verbeke, “Cost-sensitive ensemble learning: A\nunifying framework,” Data Mining and Knowledge Discovery, vol. 36,\nno. 1, pp. 1–28, 2022.\n186\n References\n[245]\nT. Vanderschueren, T. Verdonck, B. Baesens, and W. Verbeke, “Predict-\nthen-optimize or predict-and-optimize? an empirical evaluation of\ncost-sensitive learning strategies,” Information Sciences, vol. 594, pp. 400–\n415, 2022.\n[246]\nU. Brefeld, P. Geibel, and F. Wysotzki, “Support vector machines\nwith example dependent costs,” in European Conference on Machine\nLearning, Springer, 2003, pp. 23–34.\n[247]\nW. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan, “Adacost: Misclassifi-\ncation cost-sensitive boosting,” in Icml, Citeseer, vol. 99, 1999, pp. 97–\n105.\n[248]\nY. Zelenkov, “Example-dependent cost-sensitive adaptive boosting,”\nExpert Systems with Applications, vol. 135, pp. 71–82, 2019.\n[249]\nY. Sahin, S. Bulkan, and E. Duman, “A cost-sensitive decision tree ap-\nproach for fraud detection,” Expert Systems with Applications, vol. 40,\nno. 15, pp. 5916–5923, 2013.\n[250]\nA. C. Bahnsen, D. Aouada, and B. Ottersten, “Example-dependent\ncost-sensitive decision trees,” Expert Systems with Applications, vol. 42,\nno. 19, pp. 6609–6619, 2015.\n[251]\nA. C. Bahnsen, D. Aouada, and B. Ottersten, “Example-dependent\ncost-sensitive logistic regression for credit scoring,” in 2014 13th In-\nternational Conference on Machine Learning and Applications, 2014,\npp. 263–269. doi: 10.1109/ICMLA.2014.48.\n[252]\nA. C. Bahnsen, D. Aouada, A. Stojanovic, and B. Ottersten, “Feature\nengineering strategies for credit card fraud detection,” Expert Systems\nwith Applications, vol. 51, pp. 134–142, 2016.\n[",
  "47": "st-sensitive boosting,” in Icml, Citeseer, vol. 99, 1999, pp. 97–\n105.\n[248]\nY. Zelenkov, “Example-dependent cost-sensitive adaptive boosting,”\nExpert Systems with Applications, vol. 135, pp. 71–82, 2019.\n[249]\nY. Sahin, S. Bulkan, and E. Duman, “A cost-sensitive decision tree ap-\nproach for fraud detection,” Expert Systems with Applications, vol. 40,\nno. 15, pp. 5916–5923, 2013.\n[250]\nA. C. Bahnsen, D. Aouada, and B. Ottersten, “Example-dependent\ncost-sensitive decision trees,” Expert Systems with Applications, vol. 42,\nno. 19, pp. 6609–6619, 2015.\n[251]\nA. C. Bahnsen, D. Aouada, and B. Ottersten, “Example-dependent\ncost-sensitive logistic regression for credit scoring,” in 2014 13th In-\nternational Conference on Machine Learning and Applications, 2014,\npp. 263–269. doi: 10.1109/ICMLA.2014.48.\n[252]\nA. C. Bahnsen, D. Aouada, A. Stojanovic, and B. Ottersten, “Feature\nengineering strategies for credit card fraud detection,” Expert Systems\nwith Applications, vol. 51, pp. 134–142, 2016.\n[253]\nP. J. Huber and E. Ronchetti, “Robust statistics. 2nd john wiley &\nsons,” Hoboken, NJ, vol. 2, 2009.\n[254]\nR. A. Maronna, R. D. Martin, V. J. Yohai, and M. Salibián-Barrera,\nRobust statistics: theory and methods (with R). John Wiley & Sons,\n2019.\n[255]\nP. J. Rousseeuw and A. M. Leroy, Robust Regression and Outlier\nDetection. John Wiley & Sons, Inc., 1987, isbn: 9780471725381.\n[256]\nE. Cantoni and E. Ronchetti, “Robust inference for generalized lin-\near models,” Journal of the American Statistical Association, vol. 96,\nno. 455, pp. 1022–1030, 2001.\n[257]\nA. Bergesio and V. J. Yohai, “Projection estimators for generalized\nlinear models,” Journal of the American Statistical Association, vol. 106,\nno. 494, pp. 661–671, 2011.\n187\n References\n[258]\nM. Valdora and V. J. Yohai, “Robust estimators for generalized lin-\near models,” Journal of Statistical Planning and Inference, vol. 146,\npp. 31–48, 2014.\n[259]\nA. Ghosh and A. Basu, “Robust estimation in generalized linear mod-\nels: The density power divergence approach,” TEST, vol. 25, no. 2,\npp. 269–290, 2016.\n[260]\nN. Štefelová, A. Alfons, J. Palarea-Albaladejo, P. Filzmoser, and K.\nHron, “Robust regression with compositional covariates including cell-\nwise outliers,” Adv. Data Anal. Classif., vol. 15, no. 4, pp. 869–909,\nDec. 2021.\n[261]\nH. R. Künsch, L. A. Stefanski, and R. J. Carroll, “Conditionally unbi-\nased bounded-influence estimation in general regression models, with\napplications to generalized linear models,” Journal of the American\nStatistical Association, vol. 84, no. 406, pp. 460–466, 1989.\n[262]\nS. Morgenthaler, “Least-absolute-deviations fits for generalized linear\nmodels,” Biometrika, vol. 79, no. 4, pp. 747–754, 1992.\n[263]\nR. J. Carroll and S. Pederson, “On robustness in the logistic regression\nmodel,” Journal of the Royal Statistical Society: Series B (Method-\nological), vol. 55, no. 3, pp. 693–706, 1993.\n[264]\nA. M. Bianco and V. J. Yohai, “Robust estimation in the logistic\nregression model,” in Robust statistics, data analysis, and computer\nintensive methods, Springer, 1996, pp. 17–34.\n[265]\nC. Croux and G. Haesbroeck, “Implementing the bianco and yohai es-\ntimator for logistic regression,” Computational statistics & data anal-\nysis, vol. 44, no. 1-2, pp. 273–295, 2003.\n[266]\nH. D. Bondell, “Minimum distance estimation for the logistic regres-\nsion model,” Biometrika, vol. 92, no. 3, pp. 724–731, 2005.\n[267]\nH. D. Bondell, “A characteristic function approach to the biased sam-\npling model, with application to robust logistic regression,” Journal\nof Statistical Planning and Inference, vol. 138, no. 3, pp. 742–755,\n2008.\n[268]\nG. S. Monti and P. Filzmoser, “Robust logistic zero-sum regression\nfor microbiome compositional data,” Adv. Data Anal. Classif., Sep.\n2021.\n[269]\nS. Hosseinian and S. Morgenthaler, “Robust binary regression,” Jour-\nnal of statistical planning and inference, vol. 141, no. 4, pp. 1497–\n1509, 2011.\n188\n References\n[270]\nN. Thai-Nghe, Z. Gantner, and L. Schmidt-Thieme, “Cost-sensitive\nlearning methods for imbalanced data,” in The 2010 International\nJoint Conference on Neural Networks (IJCNN), 2010, pp. 1–8. doi:\n10.1109/IJCNN.2010.5596486.\n[271]\nG. I. W. Claude Sammut, Encyclopedia of Machine Learning and\nData Mining. Springer US, 2017, isbn: 9781489976864.\n[272]\nC. Whitrow, D. J. Hand, P. Juszczak, D. Weston, and N. M. Adams,\n“Transaction aggregation as a strategy for credit card fraud detec-\ntion,” Data mining and knowledge discovery, vol. 18, no. 1, pp. 30–\n55, 2009.\n[273]\nP. J. Huber, “Robust estimation of a location parameter,” The An-\nnals of Mathematical Statistics, vol. 35, no. 1, pp. 73–101, 1964, issn:\n00034851. [Online]. Available: http://www.jstor.org/stable/\n2238020.\n[274]\nP. J. Rousseeuw and M. Hubert, “Robust statistics for outlier detec-\ntion,” en, Wiley Interdiscip. Rev. Data Min. Knowl. Discov., vol. 1,\nno. 1, pp. 73–79, Jan. 2011.\n[275]\nM. L. G. .-. ULB, Anonymized credit card transactions labeled as\nfraudulent or genuine, https://www.kaggle.com/mlg-ulb/creditcardfraud,\n2018. (visited on 05/17/2021).\n[276]\nW. Verbeke, D. Martens, C. Mues, and B. Baesens, “Building com-\nprehensible customer churn prediction models with advanced rule in-\nduction techniques,” Expert Systems with Applications, vol. 38, no. 3,\npp. 2354–2364, 2011.\n[277]\nW. Verbeke, K. Dejaeger, D. Martens, J. Hur, and B. Baesens, “New\ninsights into churn prediction in the telecommunication sector: A\nprofit driven data mining approach,” European Journal of Operational\nResearch, vol. 218, no. 1, pp. 211–229, 2012.\n[278]\nS. Lessmann, B. Baesens, H.-V. Seow, and L. C. Thomas, “Bench-\nmarking state-of-the-art classification algorithms for credit scoring:\nAn update of research,” European Journal of Operational Research,\nvol. 247, no. 1, pp. 124–136, 2015.\n[279]\nV. Van Vlasselaer, T. Eliassi-Rad, L. Akoglu, M. Snoeck, and B. Bae-\nsens, “GOTCHA! Network-based fraud detection for social security\nfraud,” Management Science, vol. 63, no. 9, pp. 3090–3110, 2017.\n[280]\nM. Hardt, E. Price, E. Price, and N. Srebro, “Equality of opportunity\nin supervised learning,” in Advances in Neural Information Processing\nSystems, 2016.\n189\n References\n[281]\nM. Lindholm, R. Richman, A. Tsanakas, and M. V. Wüthrich, “Discrimination-\nfree insurance pricing,” ASTIN Bulletin: The Journal of the IAA,\nvol. 52, no. 1, pp. 55–89, 2022.\n[282]\nE. W. Frees and F. Huang, “The discriminating (pricing) actuary,”\nNorth American Actuarial Journal, vol. 27, no. 1, pp. 2–24, 2023.\n[283]\nX. Xin and F. Huang, “Antidiscrimination insurance pricing: Reg-\nulations, fairness criteria, and models,” North American Actuarial\nJournal, vol. 28, no. 2, pp. 285–319, 2024.\n[284]\nEuropean Union, “Guidelines on the application of council directive\n2004/113/ec to insurance, in the light of the judgment of the court of\njustice of the european union in case c-236/09 (test-achats),” Official\nJournal of the European Union, vol. C11, pp. 1–11, 2012. [Online].\nAvailable: https://eur-lex.europa.eu/LexUriServ/LexUriServ.\ndo?uri=OJ:C:2012:011:0001:0011:EN:PDF.\n[285]\nC. Hurlin, C. Pérignon, and S. Saurin, “The fairness of credit scoring\nmodels,” Management Science, 2024.\n[286]\nA. C. B. Garcia, M. G. P. Garcia, and R. Rigobon, “Algorithmic\ndiscrimination in the credit domain: What do we know about it?” AI\n& SOCIETY, vol. 39, no. 4, pp. 2059–2098, 2024.\n[287]\nF. A. Khan and J. Stoyanovich, “The unbearable weight of massive\nprivilege: Revisiting bias-variance trade-offs in the context of fair pre-\ndiction,” arXiv preprint: 2302.08704, 2023.\n[288]\nF. A. Khan, D. Herasymuk, and J. Stoyanovich, “On fairness and\nstability: Is estimator variance a friend or a foe?” arXiv preprint:\n2302.04525, 2023.\n[289]\nX. Han, Z. Jiang, H. Jin, et al., “Retiring DeltaDP: New distribution-\nlevel metrics for demographic parity,” Transactions on Machine Learn-\ning Research, 2023, issn: 2835-8856.\n[290]\nJ. Peeperkorn and S. De Vos, “Achieving group fairness through inde-\npendence in predictive process monitoring,” arXiv preprint arXiv:2412.04914,\n2024.\n[291]\nÖ. G. Ali and U. Arıtürk, “Dynamic churn prediction framework with\nmore effective use of rare event data: The case of private banking,” Ex-\npert Systems with Applications, vol. 41, no. 17, pp. 7889–7903, 2014.\n[292]\nH. Weerts, L. Royakkers, and M. Pechenizkiy, “Does the end jus-\ntify the means? On the moral justification of fairness-aware machine\nlearning,” arXiv, vol. 2022, pp. 2202–08 536, 2022.\n190\n References\n[293]\nN. Kozodoi, J. Jacob, and S. Lessmann, “Fairness in credit scoring:\nAssessment, implementation and profit implications,” European Jour-\nnal of Operational Research, vol. 297, no. 3, pp. 1083–1094, 2022.\n[294]\nS. Fazelpour and D. Danks, “Algorithmic bias: Senses, sources, solu-\ntions,” Philosophy Compass, vol. 16, no. 8, e12760, 2021.\n[295]\nA. E. Prince and D. Schwarcz, “Proxy discrimination in the age\nof artificial intelligence and big data,” Iowa Law Review, vol. 105,\npp. 1257–1318, 2019.\n[296]\nS. Barocas and A. D. Selbst, “Big data’s disparate impact,” California\nLaw Review, vol. 104, no. 3, pp. 671–732, 2016.\n[297]\nM. Lindholm, R. Richman, A. Tsanakas, and M. V. Wüthrich, “What\nis fair? Proxy discrimination vs. demographic disparities in insurance\npricing,” Scandinavian Actuarial Journal, pp. 1–36, 2024.\n[298]\nJ. Adams-Prassl, R. Binns, and A. Kelly-Lyth, “Directly discrimina-\ntory algorithms,” The Modern Law Review, vol. 86, no. 1, pp. 144–\n175, 2023.\n[299]\nC. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness\nthrough awareness,” in Proceedings of the 3rd Innovations in Theo-\nretical Computer Science Conference, 2012.\n[300]\nW. Fleisher, “What’s fair about individual fairness?” In Proceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021,\npp. 480–490.\n[301]\nM. J. Kusner, J. Loftus, C. Russell, and R. Silva, “Counterfactual fair-\nness,” in Advances in Neural Information Processing Systems, 2017.\n[302]\nR. Binns, “On the apparent conflict between individual and",
  "48": "D. Selbst, “Big data’s disparate impact,” California\nLaw Review, vol. 104, no. 3, pp. 671–732, 2016.\n[297]\nM. Lindholm, R. Richman, A. Tsanakas, and M. V. Wüthrich, “What\nis fair? Proxy discrimination vs. demographic disparities in insurance\npricing,” Scandinavian Actuarial Journal, pp. 1–36, 2024.\n[298]\nJ. Adams-Prassl, R. Binns, and A. Kelly-Lyth, “Directly discrimina-\ntory algorithms,” The Modern Law Review, vol. 86, no. 1, pp. 144–\n175, 2023.\n[299]\nC. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness\nthrough awareness,” in Proceedings of the 3rd Innovations in Theo-\nretical Computer Science Conference, 2012.\n[300]\nW. Fleisher, “What’s fair about individual fairness?” In Proceedings\nof the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021,\npp. 480–490.\n[301]\nM. J. Kusner, J. Loftus, C. Russell, and R. Silva, “Counterfactual fair-\nness,” in Advances in Neural Information Processing Systems, 2017.\n[302]\nR. Binns, “On the apparent conflict between individual and group\nfairness,” in Proceedings of the Conference on Fairness, Accountabil-\nity, and Transparency, 2020.\n[303]\nJ. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent trade-offs\nin the fair determination of risk scores,” arXiv preprint:1609.05807,\n2016.\n[304]\nM. Kearns, S. Neel, A. Roth, and Z. S. Wu, “Preventing fairness ger-\nrymandering: Auditing and learning for subgroup fairness,” in Pro-\nceedings of the 35th International Conference on Machine Learning,\n2018.\n[305]\nA. Chouldechova, “Fair prediction with disparate impact: A study of\nbias in recidivism prediction instruments,” Big Data, vol. 5, no. 2,\npp. 153–163, 2017.\n191\n References\n[306]\nG. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger,\n“On fairness and calibration,” Advances in Neural Information Pro-\ncessing Systems, vol. 30, 2017.\n[307]\nA. Algaba, C. Mazijn, C. Prunkl, J. Danckaert, and V. Ginis, “LUCID-\nGAN: Conditional generative models to locate unfairness,” in World\nConference on Explainable Artificial Intelligence, Springer, 2023, pp. 346–\n367.\n[308]\nC. Mazijn, C. Prunkl, A. Algaba, J. Danckaert, and V. Ginis, “LU-\nCID: Exposing algorithmic bias through inverse design,” in Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol. 37, 2023,\npp. 14 391–14 399.\n[309]\nM. De-Arteaga, S. Feuerriegel, and M. Saar-Tsechansky, “Algorithmic\nfairness in business analytics: Directions for research and practice,”\nProduction and Operations Management, vol. 31, no. 10, pp. 3749–\n3770, 2022.\n[310]\nK. W. De Bock, K. Coussement, A. De Caigny, et al., “Explainable\nai for operational research: A defining framework, methods, appli-\ncations, and a research agenda,” European Journal of Operational\nResearch, vol. 317, no. 2, pp. 249–272, 2024.\n[311]\nC. Mazijn, J. Danckaert, and V. Ginis, “How do the score distributions\nof subpopulations influence fairness notions?” In Proceedings of the\n2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021.\n[312]\nR. Binns, “Fairness in machine learning: Lessons from political philos-\nophy,” in Conference on Fairness, Accountability and Transparency,\n2018.\n[313]\nS. A. Friedler, C. Scheidegger, and S. Venkatasubramanian, “On the\n(im)possibility of fairness,” arXiv preprint: 1609.07236, 2016.\n[314]\nM. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkata-\nsubramanian, “Certifying and removing disparate impact,” in Pro-\nceedings of the 21th ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, 2015, pp. 259–268.\n[315]\nS. Radovanović, G. Savić, B. Delibašić, and M. Suknović, “FairDEA—\nremoving disparate impact from efficiency scores,” European Journal\nof Operational Research, vol. 301, no. 3, pp. 1088–1098, 2022.\n[316]\nM. B. Zafar, I. Valera, M. Gomez-Rodriguez, and K. P. Gummadi,\n“Fairness constraints: A flexible approach for fair classification,” Jour-\nnal of Machine Learning Research, vol. 20, no. 75, pp. 1–42, 2019.\n[317]\nM. Hanson, G. Lewkowicz, and S. Verboven, “Engineering the law-\nmachine learning translation problem: Developing legally aligned mod-\nels,” arXiv preprint arXiv:2504.16969, 2025.\n192\n References\n[318]\nS. Dutta, D. Wei, H. Yueksel, P.-Y. Chen, S. Liu, and K. Varshney,\n“Is there a trade-off between fairness and accuracy? a perspective\nusing mismatched hypothesis testing,” in International conference on\nmachine learning, PMLR, 2020, pp. 2803–2813.\n[319]\nM. Wick, J.-B. Tristan, et al., “Unlocking fairness: A trade-off re-\nvisited,” Advances in neural information processing systems, vol. 32,\n2019.\n[320]\nU. Shalit, F. D. Johansson, and D. Sontag, “Estimating individual\ntreatment effect: Generalization bounds and algorithms,” in Inter-\nnational Conference on Machine Learning, PMLR, 2017, pp. 3076–\n3085.\n[321]\nE. Ustinova and V. Lempitsky, “Learning deep embeddings with his-\ntogram loss,” Advances in Neural Information Processing Systems,\nvol. 29, 2016.\n[322]\nJ. Chen, N. Kallus, X. Mao, G. Svacha, and M. Udell, “Fairness un-\nder unawareness: Assessing disparity when protected class is unob-\nserved,” in Proceedings of the conference on fairness, accountability,\nand transparency, 2019, pp. 339–348.\n[323]\nA. Coston, K. N. Ramamurthy, D. Wei, et al., “Fair transfer learn-\ning with missing protected attributes,” in Proceedings of the 2019\nAAAI/ACM Conference on AI, Ethics, and Society, 2019, pp. 91–98.\n[324]\nC. Villani, Optimal transport: Old and new. Springer, 2008, vol. 338.\n[325]\nM. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal\ntransport,” Advances in Neural Information Processing Systems, vol. 26,\n2013.\n[326]\nM. Cuturi and A. Doucet, “Fast computation of wasserstein barycen-\nters,” in International Conference on Machine Learning, PMLR, 2014,\npp. 685–693.\n[327]\nF. Devriendt, J. Berrevoets, and W. Verbeke, “Why you should stop\npredicting customer churn and start using uplift models,” Information\nSciences, vol. 548, pp. 497–515, 2021.\n[328]\nW. Verbeke, D. Olaya, M.-A. Guerry, and J. Van Belle, “To do or not\nto do? Cost-sensitive causal classification with individual treatment\neffect estimates,” European Journal of Operational Research, vol. 305,\nno. 2, pp. 838–852, 2023.\n[329]\nT. Vanderschueren, B. Baesens, T. Verdonck, and W. Verbeke, “A new\nperspective on classification: Optimally allocating limited resources to\nuncertain tasks,” Decision Support Systems, vol. 179, p. 114 151, 2024.\n193\n References\n[330]\nF. Devriendt, J. Van Belle, T. Guns, and W. Verbeke, “Learning to\nrank for uplift modeling,” IEEE Transactions on Knowledge and Data\nEngineering, vol. 34, no. 10, pp. 4888–4904, 2020.\n[331]\nS. Goethals and T. Calders, “Reranking individuals: The effect of fair\nclassification within-groups,” arXiv, pp. 1–16, 2024.\n[332]\nP. W. Holland, “Statistics and causal inference,” Journal of the Amer-\nican statistical Association, vol. 81, no. 396, pp. 945–960, 1986.\n[333]\nJ. Pearl, “Causal inference in statistics: An overview,” Statistics Sur-\nveys, vol. 3, pp. 96–146, 2009. doi: 10.1214/09-SS057.\n[334]\nD. B. Rubin, “Causal inference using potential outcomes: Design,\nmodeling, decisions,” Journal of the American Statistical Association,\nvol. 100, no. 469, pp. 322–331, 2005.\n[335]\nC. Fernández-Loría and F. Provost, “Causal decision making and\ncausal effect estimation are not the same. . . and why it matters,”\nINFORMS Journal on Data Science, vol. 1, no. 1, pp. 4–16, 2022.\n[336]\nK. Hirano and G. Imbens, The propensity score with continuous treat-\nments. applied bayesian modeling and causal inference from incomplete-\ndata perspectives, 2004.\n[337]\nT. Holland-Letz and A. Kopp-Schneider, “Optimal experimental de-\nsigns for dose–response studies with continuous endpoints,” Archives\nof toxicology, vol. 89, pp. 2059–2068, 2015.\n[338]\nI. Bica, J. Jordon, and M. van der Schaar, “Estimating the effects\nof continuous-valued interventions using generative adversarial net-\nworks,” Advances in Neural Information Processing Systems, vol. 33,\npp. 16 434–16 445, 2020.\n[339]\nP. Schwab, L. Linhardt, S. Bauer, J. M. Buhmann, and W. Karlen,\n“Learning counterfactual representations for estimating individual dose-\nresponse curves,” in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 34, 2020, pp. 5612–5619.\n[340]\nL. Nie, M. Ye, Q. Liu, and D. Nicolae, “Vcnet and functional targeted\nregularization for learning causal effects of continuous treatments,”\narXiv preprint arXiv:2103.07861, 2021.\n[341]\nS. Barocas, M. Hardt, and A. Narayanan, Fairness and machine\nlearning: Limitations and opportunities. MIT Press, 2023.\n[342]\nX. Han, Z. Jiang, H. Jin, et al., “Retiring deltadp: New distribution-\nlevel metrics for demographic parity,” Transactions on Machine Learn-\ning Research, 2023, issn: 2835-8856.\n[343]\nD. Frauen, V. Melnychuk, and S. Feuerriegel, “Fair off-policy learning\nfrom observational data,” arXiv preprint arXiv:2303.08516, 2023.\n194\n References\n[344]\nW. Zhang, J. Li, and L. Liu, “A unified survey of treatment effect het-\nerogeneity modelling and uplift modelling,” ACM Computing Surveys\n(CSUR), vol. 54, no. 8, pp. 1–36, 2021.\n[345]\nP. Gutierrez and J.-Y. Gérardy, “Causal inference and uplift mod-\nelling: A review of the literature,” in International conference on pre-\ndictive applications and APIs, PMLR, 2017, pp. 1–13.\n[346]\nF. Devriendt, J. Van Belle, T. Guns, and W. Verbeke, “Learning to\nrank for uplift modeling,” IEEE Transactions on Knowledge and Data\nEngineering, vol. 34, no. 10, pp. 4888–4904, 2020.\n[347]\nD. Olaya, K. Coussement, and W. Verbeke, “A survey and bench-\nmarking study of multitreatment uplift modeling,” Data Mining and\nKnowledge Discovery, vol. 34, pp. 273–308, 2020.\n[348]\nT. Hatt and S. Feuerriegel, “Sequential deconfounding for causal in-\nference with unobserved confounders,” in Causal Learning and Rea-\nsoning, PMLR, 2024, pp. 934–956.\n[349]\nD. Jones, D. Molitor, and J. Reif, “What do workplace wellness pro-\ngrams do? evidence from the illinois workplace wellness study,” The\nQuarterly Journal of Economics, vol. 134, no. 4, pp. 1747–1791, 2019.\n[350]\nK. ",
  "49": ", vol. 54, no. 8, pp. 1–36, 2021.\n[345]\nP. Gutierrez and J.-Y. Gérardy, “Causal inference and uplift mod-\nelling: A review of the literature,” in International conference on pre-\ndictive applications and APIs, PMLR, 2017, pp. 1–13.\n[346]\nF. Devriendt, J. Van Belle, T. Guns, and W. Verbeke, “Learning to\nrank for uplift modeling,” IEEE Transactions on Knowledge and Data\nEngineering, vol. 34, no. 10, pp. 4888–4904, 2020.\n[347]\nD. Olaya, K. Coussement, and W. Verbeke, “A survey and bench-\nmarking study of multitreatment uplift modeling,” Data Mining and\nKnowledge Discovery, vol. 34, pp. 273–308, 2020.\n[348]\nT. Hatt and S. Feuerriegel, “Sequential deconfounding for causal in-\nference with unobserved confounders,” in Causal Learning and Rea-\nsoning, PMLR, 2024, pp. 934–956.\n[349]\nD. Jones, D. Molitor, and J. Reif, “What do workplace wellness pro-\ngrams do? evidence from the illinois workplace wellness study,” The\nQuarterly Journal of Economics, vol. 134, no. 4, pp. 1747–1791, 2019.\n[350]\nK. Verstraete, I. Gyselinck, H. Huts, et al., “Estimating individual\ntreatment effects on copd exacerbations by causal machine learning\non randomised controlled trials,” Thorax, vol. 78, no. 10, pp. 983–\n989, 2023, issn: 0040-6376. doi: 10.1136/thorax- 2022- 219382.\neprint: https://thorax.bmj.com/content/78/10/983.full.pdf.\n[Online]. Available: https://thorax.bmj.com/content/78/10/983.\n[351]\nP. R. Rosenbaum and D. B. Rubin, “The central role of the propensity\nscore in observational studies for causal effects,” Biometrika, vol. 70,\nno. 1, pp. 41–55, 1983.\n[352]\nF. Johansson, U. Shalit, and D. Sontag, “Learning representations\nfor counterfactual inference,” in International conference on machine\nlearning, PMLR, 2016, pp. 3020–3029.\n[353]\nC. Bockel-Rickermann, T. Vanderschueren, T. Verdonck, and W. Ver-\nbeke, “Sources of gain: Decomposing performance in conditional av-\nerage dose response estimation,” arXiv preprint arXiv:2406.08206,\n2024.\n[354]\nY.-F. Zhang, H. Zhang, Z. C. Lipton, L. E. Li, and E. P. Xing, “Ex-\nploring transformer backbones for heterogeneous treatment effect es-\ntimation,” arXiv preprint arXiv:2202.01336, 2022.\n195\n References\n[355]\nC. Bockel-Rickermann, T. Vanderschueren, J. Berrevoets, T. Ver-\ndonck, and W. Verbeke, “Learning continuous-valued treatment ef-\nfects through representation balancing,” arXiv preprint arXiv:2309.03731,\n2023.\n[356]\nH. Zhou, S. Li, G. Jiang, J. Zheng, and D. Wang, “Direct hetero-\ngeneous causal learning for resource allocation problems in market-\ning,” in Proceedings of the AAAI Conference on Artificial Intelligence,\nvol. 37, 2023, pp. 5446–5454.\n[357]\nR. M. Gubela, S. Lessmann, and S. Jaroszewicz, “Response transfor-\nmation and profit decomposition for revenue uplift modeling,” Euro-\npean Journal of Operational Research, vol. 283, no. 2, pp. 647–661,\n2020.\n[358]\nA. Lemmens and S. Gupta, “Managing churn to maximize profits,”\nMarketing Science, vol. 39, no. 5, pp. 956–973, 2020.\n[359]\nR. M. Gubela and S. Lessmann, “Uplift modeling with value-driven\nevaluation metrics,” Decision Support Systems, vol. 150, p. 113 648,\n2021.\n[360]\nW. Verbeke, D. Olaya, M.-A. Guerry, and J. Van Belle, “To do or not\nto do? cost-sensitive causal classification with individual treatment\neffect estimates,” European Journal of Operational Research, vol. 305,\nno. 2, pp. 838–852, 2023.\n[361]\nJ. Haupt and S. Lessmann, “Targeting customers under response-\ndependent costs,” European Journal of Operational Research, vol. 297,\nno. 1, pp. 369–379, 2022.\n[362]\nA. Betlei, E. Diemert, and M.-R. Amini, “Uplift modeling with gen-\neralization guarantees,” in Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining, 2021, pp. 55–\n65.\n[363]\nT. Vanderschueren, W. Verbeke, F. Moraes, and H. M. Proença, “Met-\nalearners for ranking treatment effects,” arXiv preprint arXiv:2405.02183,\n2024.\n[364]\nM. Jaskowski and S. Jaroszewicz, “Uplift modeling for clinical trial\ndata,” in ICML workshop on clinical data analysis, vol. 46, 2012,\npp. 79–95.\n[365]\nA. N. Elmachtoub and P. Grigas, “Smart “predict, then optimize”,”\nManagement Science, vol. 68, no. 1, pp. 9–26, 2022.\n[366]\nJ. Mandi, J. Kotary, S. Berden, et al., “Decision-focused learning:\nFoundations, state of the art, benchmark and future opportunities,”\narXiv preprint arXiv:2307.13565, 2023.\n196\n References\n[367]\nB. Zhan, C. Liu, Y. Li, and C. Wu, “Weighted doubly robust learn-\ning: An uplift modeling technique for estimating mixed treatments’\neffect,” Decision Support Systems, vol. 176, p. 114 060, 2024.\n[368]\nA. Chouldechova and A. Roth, “A snapshot of the frontiers of fairness\nin machine learning,” Communications of the ACM, vol. 63, no. 5,\npp. 82–89, 2020.\n[369]\nJ. Dressel and H. Farid, “The accuracy, fairness, and limits of pre-\ndicting recidivism,” Science advances, vol. 4, no. 1, eaao5580, 2018.\n[370]\nK. Makhlouf, S. Zhioua, and C. Palamidessi, “On the applicability\nof machine learning fairness notions,” ACM SIGKDD Explorations\nNewsletter, vol. 23, no. 1, pp. 14–23, 2021.\n[371]\nJ. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent trade-offs\nin the fair determination of risk scores,” arXiv preprint arXiv:1609.05807,\n2016.\n[372]\nT. Scantamburlo, J. Baumann, and C. Heitz, “On prediction-modelers\nand decision-makers: Why fairness requires more than a fair predic-\ntion model,” AI & SOCIETY, pp. 1–17, 2024.\n[373]\nR. Nabi, D. Malinsky, and I. Shpitser, “Learning optimal fair poli-\ncies,” in International Conference on Machine Learning, PMLR, 2019,\npp. 4674–4682.\n[374]\nT. Verbraken, W. Verbeke, and B. Baesens, “A novel profit maxi-\nmizing metric for measuring classification performance of customer\nchurn prediction models,” IEEE transactions on knowledge and data\nengineering, vol. 25, no. 5, pp. 961–973, 2012.\n[375]\nA. C. Bahnsen, D. Aouada, and B. Ottersten, “Example-dependent\ncost-sensitive logistic regression for credit scoring,” in 2014 13th In-\nternational conference on machine learning and applications, IEEE,\n2014, pp. 263–269.\n[376]\nC. O. Vasquez, J. De Weerdt, and S. vanden Broucke, “The hidden\ncost of fraud: An instance-dependent cost-sensitive approach for pos-\nitive and unlabeled learning,” in Fourth International Workshop on\nLearning with Imbalanced Domains: Theory and Applications, PMLR,\n2022, pp. 53–67.\n[377]\nK. W. De Bock, K. Coussement, and S. Lessmann, “Cost-sensitive\nbusiness failure prediction when misclassification costs are uncertain:\nA heterogeneous ensemble selection approach,” European Journal of\nOperational Research, vol. 285, no. 2, pp. 612–630, 2020.\n197\n References\n[378]\nT. Vanderschueren, R. Boute, T. Verdonck, B. Baesens, and W. Ver-\nbeke, “Optimizing the preventive maintenance frequency with causal\nmachine learning,” International Journal of Production Economics,\nvol. 258, p. 108 798, 2023.\n[379]\nD. B. Rubin, “Direct and indirect causal effects via potential out-\ncomes,” Scandinavian Journal of Statistics, vol. 31, no. 2, pp. 161–\n170, 2004.\n[380]\nG. W. Imbens, “The role of the propensity score in estimating dose-\nresponse functions,” Biometrika, vol. 87, no. 3, pp. 706–710, 2000.\n[381]\nM. Lechner, Identification and estimation of causal effects of multiple\ntreatments under the conditional independence assumption. Springer,\n2001.\n[382]\nS. R. Künzel, J. S. Sekhon, P. J. Bickel, and B. Yu, “Metalearners\nfor estimating heterogeneous treatment effects using machine learn-\ning,” Proceedings of the national academy of sciences, vol. 116, no. 10,\npp. 4156–4165, 2019.\n[383]\nG. B. Dantzig, “Discrete-variable extremum problems,” Operations\nResearch, vol. 5, no. 2, pp. 266–288, 1957.\n[384]\nJ. Brooks-Gunn, F.-r. Liaw, and P. K. Klebanov, “Effects of early in-\ntervention on cognitive function of low birth weight preterm infants,”\nThe Journal of pediatrics, vol. 120, no. 3, pp. 350–359, 1992.\n[385]\nR. Guo, L. Cheng, J. Li, P. R. Hahn, and H. Liu, “A survey of learn-\ning causality with data: Problems and methods,” ACM Computing\nSurveys (CSUR), vol. 53, no. 4, pp. 1–37, 2020.\n[386]\nR. Silva, “Observational-interventional priors for dose-response learn-\ning,” in Advances in Neural Information Processing Systems, D. Lee,\nM. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29,\nCurran Associates, Inc., 2016.\n[387]\nJ. Berrevoets, S. Verboven, and W. Verbeke, “Treatment effect op-\ntimisation in dynamic environments,” Journal of Causal Inference,\nvol. 10, no. 1, pp. 106–122, 2022.\n[388]\nZ. Zhao, Y. Bai, R. Xiong, et al., “Learning individual treatment\neffects under heterogeneous interference in networks,” ACM Trans-\nactions on Knowledge Discovery from Data, vol. 18, no. 8, pp. 1–21,\n2024.\n[389]\nJ. Berrevoets, J. Jordon, I. Bica, M. van der Schaar, et al., “Or-\nganite: Optimal transplant donor organ offering using an individual\ntreatment effect,” Advances in neural information processing systems,\nvol. 33, pp. 20 037–20 050, 2020.\n198\n References\n[390]\nT. Vanderschueren, J. Berrevoets, and W. Verbeke, “Noflite: Learning\nto predict individual treatment effect distributions,” Transactions on\nMachine Learning Research, 2023.\n[391]\nM. Schröder, D. Frauen, J. Schweisthal, K. Heß, V. Melnychuk, and\nS. Feuerriegel, “Conformal prediction for causal effects of continuous\ntreatments,” arXiv preprint arXiv:2407.03094, 2024.\n[392]\nB. J. Dietvorst, J. P. Simmons, and C. Massey, “Overcoming algo-\nrithm aversion: People will use imperfect algorithms if they can (even\nslightly) modify them,” Management science, vol. 64, no. 3, pp. 1155–\n1170, 2018.\n[393]\nA. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning, 2005, pp. 625–632.\n[394]\nJ. Platt et al., “Probabilistic outputs for support vector machines and\ncomparisons to regularized likelihood methods,” Advances in large\nmargin classifiers, vol. 10, no. 3, pp. 61–74, 1999.\n[395]\nN. Hollmann, S. Müller, L. Purucker, et al., “Accurate predictions\non small data with a tabular foundation model,” Nature, vol. ",
  "50": "butions,” Transactions on\nMachine Learning Research, 2023.\n[391]\nM. Schröder, D. Frauen, J. Schweisthal, K. Heß, V. Melnychuk, and\nS. Feuerriegel, “Conformal prediction for causal effects of continuous\ntreatments,” arXiv preprint arXiv:2407.03094, 2024.\n[392]\nB. J. Dietvorst, J. P. Simmons, and C. Massey, “Overcoming algo-\nrithm aversion: People will use imperfect algorithms if they can (even\nslightly) modify them,” Management science, vol. 64, no. 3, pp. 1155–\n1170, 2018.\n[393]\nA. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning, 2005, pp. 625–632.\n[394]\nJ. Platt et al., “Probabilistic outputs for support vector machines and\ncomparisons to regularized likelihood methods,” Advances in large\nmargin classifiers, vol. 10, no. 3, pp. 61–74, 1999.\n[395]\nN. Hollmann, S. Müller, L. Purucker, et al., “Accurate predictions\non small data with a tabular foundation model,” Nature, vol. 637,\nno. 8045, pp. 319–326, 2025.\n[396]\nD. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” arXiv preprint arXiv:1412.6980, 2014.\n[397]\nB. Neal, “Introduction to causal inference,” Course Lecture Notes\n(draft), vol. 132, 2020.\n[398]\nJ. L. Hill, “Bayesian nonparametric modeling for causal inference,”\nJournal of Computational and Graphical Statistics, vol. 20, no. 1,\npp. 217–240, 2011.\n199\n  Appendices\n201\n  A\nPredicting employee turnover:\nScoping and benchmarking the\nstate-of-the-art\n203\n Appendix A: Predicting employee turnover\nA.1\nSearch query\nTable A.1: Complete search queries for the Scopus and WoS databases\nDatabase\nQuery\nScopus\nTITLE-ABS-KEY ((prediction OR predicting OR forecasting OR prognosis) AND\n(employee OR worker OR laborer OR jobholder) AND (turnover OR attrition OR\nchurn OR departure) AND (analytics OR data OR \"machine learning\")) AND PUBYEAR >\n2008 AND (LIMIT-TO (DOCTYPE, \"ar\") OR LIMIT-TO (DOCTYPE, \"cp\")) AND (LIMIT-TO\n(SUBJAREA, \"COMP\") OR LIMIT-TO (SUBJAREA , \"BUSI\") OR LIMIT-TO (SUBJAREA,\n\"ENGI\") OR LIMIT-TO (SUBJAREA, \"MATH\") OR LIMIT-TO (SUBJAREA , \"ECON\")) AND\n(LIMIT-TO (LANGUAGE, \"English\"))\nWoS\nTS=((prediction OR predicting OR forecasting OR prognosis ) AND ( employee\nOR worker OR laborer OR jobholder) AND (turnover OR attrition OR churn OR\ndeparture) AND (analytics OR data OR \"machine learning\")) and Article OR\nProceeding Paper (Document Types) and Business OR Computer Science Information\nSystems OR Economics or Management OR Computer Science Interdisciplinary\nApplications OR Computer Science Software Engineering OR Engineering Industrial\nOR Computer Science Artificial Intelligence OR Computer Science Theory Methods\nOR Engineering Electrical Electronic OR Engineering Multidisciplinary (Web of\nScience Categories) AND English (Languages) AND PY > 2008\n204\n A.2. Established classifiers and their\ncorresponding studies\nA.2\nEstablished classifiers and their\ncorresponding studies\nTable A.2: Overview of established classifiers with corresponding studies.\nClassifier\nAbbr.\nNum.\nStudies\nStudies\nIndividual classifiers\nArtificial Neural Networks\nann\n12\n[78], [102]–[104], [111], [113], [116], [117], [119],\n[120], [124], [125]\nDecision Tree\ndt\n41\n[64], [73], [75], [76], [78]–[83], [87]–[102], [104],\n[107]–[110], [112]–[115], [118]–[122], [125]\nK-Nearest Neighbors\nknn\n22\n[74], [78]–[80], [82], [84], [87], [91], [93], [97],\n[100]–[102], [105], [109], [110], [113], [114],\n[118], [120], [121], [123]\nLinear Discriminant Analysis\nlda\n4\n[73], [93], [102], [113]\nLogistic Regression\nlr\n33\n[48], [73], [78], [80], [87]–[90], [92]–[97], [99]–\n[102],\n[104]–[107],\n[109],\n[110],\n[113]–[115],\n[119], [121]–[125]\nNaïve Bayes (Bayesian/Gaussian)\nbnb /gnb\n25\n[41], [75]–[81], [83], [87], [89], [91], [93], [95],\n[97], [99], [101], [102], [105], [109], [110], [113],\n[115], [120], [125]\nQuadratic Discriminant Analysis\nqda\n1\n[113]\nSupport Vector Machine\nsvm\n30\n[41], [78]–[80], [82], [84], [86]–[89], [91], [93],\n[94], [97], [98], [101], [102], [104], [105], [107]–\n[110], [113], [115], [118]–[120], [122], [124]\nEnsembles\nAdaBoost\nab\n7\n[88], [89], [92], [102], [112], [113], [117]\nGradient Boosting\ngb\n9\n[93], [95], [100], [105], [106], [111], [113], [116],\n[117]\nLightGBM\nlgbm\n3\n[96], [113], [117]\nRandom Forest\nrf\n36\n[41], [77]–[80], [82]–[84], [88], [90]–[100], [102],\n[105]–[107], [109], [111]–[118], [123]–[125]\nExtreme Gradient Boosting\nxgb\n12\n[85], [93], [95], [99], [101], [104], [106], [107],\n[112], [113], [115], [124]\n205\n Appendix A: Predicting employee turnover\nA.3\nHyperparameter search space\nTable A.3: Hyperparameter search space\nMethod\nHyperparameter\nValues\n#Models\nab\nn estimators\n{20, 50, 100, 200}\n24\nlearning rate\n{0.01, 0.1, 1}\nalgorithm\n{‘SAMME’, ‘SAMME.R’}\nann\nsolver\n{‘adam’, ’lbfgs’}\n720\nalpha\n{0.01, 0.1, 1}\nn hidden layers\n{1, 2, 3}\nn neurons\n{10, 20, 50, 100, 200}\nactivation\n{‘relu’, ‘logistic’}\nmax iter\n{100, 200, 500, 1000}\nbnb\nalpha\n{0.01, 0.1, 1}\n6\nfit prior\n{True, False}\ndt\ncriterion\n{‘gini’, ‘entropy’}\n72\nmax depth\n{5, 10, 20, 50}\nmin samples split\n{ 2, 5, 10}\nmin samples leaf\n{1, 5, 10}\ngb\nn estimators\n{20, 50, 100, 200}\n432\nlearning rate\n{0.01, 0.1, 1}\nmax depth\n{5, 10, 20, 50}\nmin samples split\n{2, 5, 10}\nmin samples leaf\n{1, 5, 10}\ngnb\nvar smoothing\n{1e-9, 1e-8, 1e-7, 1e-6, 1e-5}\n5\nknn\nn neighbors\n{1, 5, 10, 50}\n48\nweights\n{‘uniform’, ‘distance’}\nleaf size\n{10, 30, 50}\np\n{1, 2}\nlda\nshrinkage\n{None, ‘auto’, 0.1, 0.5, 1.0}\n20\nn components\n{None, 2, 5, 10}\nlgbm\nlearning rate\n{0.01, 0.1, 1}\n48\nn estimators\n{20, 50, 100, 200}\nmax depth\n{5, 10, 20, 50}\nlr\npenalty\n{‘none’,‘l1’, ‘l2’}\n48\nc\n{0.001, 0.01, 0.1, 1}\nmax iter\n{50, 100, 500, 1000}\nqda\nreg param\n{0.0, 0.01, 0.1, 0.5, 1.0}\n30\nstore covariance\n{True, False}\ntol\n{1e-5, 1e-4, 1e-3 }\nrf\nn estimators\n{20, 50, 100, 200}\n288\ncriterion\n{‘gini’, ‘entropy’}\nmax depth\n{5, 10, 20, 50}\nmin samples split\n{2, 5, 10}\nmin samples leaf\n{1, 5, 10}\nsvm\nc\n{0.01, 0.1, 1, 10}\n32\nkernel\n{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’}\ngamma\n{‘scale’, ‘auto’}\nxgb\nlearning rate\n{0.01, 0.1, 1}\n48\nn estimators\n{20, 50, 100, 200}\nmax depth\n{5, 10, 20, 50}\n206\n A.4. Detailed results per dataset\nA.4\nDetailed results per dataset\nTable A.4: Ranking Real1\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n7\n7\n8\n6\n7\n8\n8\n8\n11\nbnb\n14\n14\n11\n11\n14\n14\n14\n14\n1\ndt\n5\n5\n1\n2\n2\n3\n2\n3\n4\ngnb\n13\n12\n14\n12\n11\n11\n13\n9\n14\nknn\n9\n13\n9\n8\n12\n12\n9\n12\n8\nlda\n11\n8\n12\n9\n8\n7\n11\n7\n13\nlr\n10\n9\n10\n7\n13\n13\n10\n13\n9\nqda\n12\n11\n13\n10\n10\n10\n12\n10\n12\nsvm\n8\n10\n7\n14\n9\n9\n6\n11\n2\nab\n6\n6\n6\n13\n6\n6\n7\n6\n10\ngb\n2\n3\n3\n3\n3\n1\n5\n1\n7\nlgbm\n4\n4\n2\n1\n1\n2\n3\n2\n5\nrf\n3\n2\n5\n5\n5\n5\n1\n5\n3\nxgb\n1\n1\n4\n4\n4\n4\n4\n4\n6\nTable A.5: Ranking Real2\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n6\n4\n7\n6\n7\n8\n9\n9\n9\nbnb\n14\n11\n13\n11\n12\n9\n13\n10\n13\ndt\n5\n7\n5\n5\n5\n2\n8\n2\n10\ngnb\n13\n13\n14\n12\n14\n13\n14\n1\n14\nknn\n7\n14\n8\n7\n9\n12\n4\n13\n3\nlda\n11\n12\n6\n8\n6\n5\n7\n5\n8\nlr\n10\n8\n10\n9\n10\n11\n10\n11\n7\nqda\n12\n10\n11\n10\n11\n7\n12\n8\n12\nsvm\n8\n9\n9\n14\n8\n10\n5\n12\n5\nab\n9\n5\n12\n13\n13\n14\n11\n14\n11\ngb\n4\n2\n4\n4\n4\n3\n6\n4\n6\nlgbm\n3\n6\n2\n2\n2\n4\n2\n6\n1\nrf\n1\n1\n1\n1\n1\n1\n1\n3\n2\nxgb\n2\n3\n3\n3\n3\n6\n3\n7\n4\n207\n Appendix A: Predicting employee turnover\nTable A.6: Ranking Real3\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n8\n6\n10\n8\n6\n7\n10\n6\n10\nbnb\n14\n11\n13\n11\n4\n1\n13\n2\n13\ndt\n4\n9\n1\n6\n8\n9\n2\n9\n5\ngnb\n7\n13\n14\n13\n10\n2\n14\n1\n14\nknn\n12\n14\n8\n7\n13\n13\n7\n12\n3\nlda\n10\n8\n11\n9\n3\n4\n11\n4\n11\nlr\n5\n5\n3\n4\n9\n10\n3\n11\n4\nqda\n13\n10\n12\n10\n7\n3\n12\n3\n12\nsvm\n9\n12\n5\n14\n14\n14\n1\n14\n1\nab\n11\n7\n9\n12\n11\n11\n9\n10\n6\ngb\n1\n3\n2\n1\n2\n6\n4\n7\n7\nlgbm\n3\n2\n7\n3\n5\n8\n8\n8\n8\nrf\n6\n4\n5\n1\n12\n12\n5\n13\n2\nxgb\n2\n1\n4\n5\n1\n5\n6\n5\n9\nTable A.7: Ranking DS\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n3\n4\n5\n5\n7\n6\n7\n7\n7\nbnb\n14\n14\n14\n11\n11\n14\n14\n9\n12\ndt\n11\n7\n4\n4\n1\n1\n10\n3\n11\ngnb\n12\n11\n13\n12\n6\n7\n13\n1\n14\nknn\n13\n12\n12\n9\n12\n11\n11\n12\n5\nlda\n8\n8\n9\n8\n10\n10\n9\n10\n4\nlr\n7\n9\n11\n7\n14\n13\n4\n14\n2\nqda\n9\n10\n7\n10\n2\n2\n12\n2\n13\nsvm\n10\n13\n8\n14\n9\n9\n1\n11\n3\nab\n4\n6\n10\n13\n13\n12\n2\n13\n1\ngb\n2\n2\n3\n2\n5\n5\n6\n6\n9\nlgbm\n6\n3\n2\n3\n4\n4\n5\n4\n10\nrf\n1\n1\n1\n1\n3\n3\n3\n5\n8\nxgb\n5\n5\n6\n6\n8\n8\n8\n8\n6\nTable A.8: Ranking IBM\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n5\n9\n7\n6\n3\n3\n10\n3\n9\nbnb\n12\n12\n11\n8\n7\n7\n11\n5\n10\ndt\n14\n14\n13\n11\n11\n13\n13\n11\n11\ngnb\n9\n11\n14\n12\n4\n5\n14\n1\n14\nknn\n13\n13\n10\n10\n14\n14\n4\n14\n2\nlda\n3\n3\n2\n2\n1\n1\n5\n4\n7\nlr\n1\n1\n1\n1\n2\n2\n3\n6\n3\nqda\n10\n10\n12\n9\n5\n4\n12\n2\n12\nsvm\n2\n2\n6\n14\n13\n11\n1\n13\n13\nab\n4\n4\n3\n13\n6\n6\n6\n7\n6\ngb\n11\n8\n9\n7\n10\n10\n9\n10\n8\nlgbm\n8\n7\n4\n4\n9\n9\n7\n9\n4\nrf\n6\n5\n8\n3\n12\n12\n2\n12\n1\nxgb\n7\n6\n4\n5\n8\n8\n8\n8\n5\n208\n A.4. Detailed results per dataset\nTable A.9: Ranking Kaggle1\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n5\n4\n4\n5\n5\n5\n7\n3\n7\nbnb\n14\n14\n11\n11\n12\n12\n11\n12\n10\ndt\n8\n7\n5\n6\n6\n6\n6\n7\n6\ngnb\n11\n11\n14\n12\n11\n11\n14\n11\n14\nknn\n7\n9\n8\n7\n8\n8\n8\n5\n8\nlda\n12\n13\n13\n10\n14\n14\n13\n14\n12\nlr\n13\n12\n12\n9\n13\n13\n12\n13\n11\nqda\n10\n10\n10\n8\n10\n10\n10\n9\n13\nsvm\n6\n6\n6\n14\n7\n7\n3\n8\n4\nab\n9\n8\n9\n13\n9\n9\n9\n10\n9\ngb\n3\n3\n3\n4\n3\n2\n4\n2\n5\nlgbm\n4\n5\n7\n3\n4\n4\n5\n6\n3\nrf\n1\n1\n1\n2\n2\n3\n1\n4\n1\nxgb\n2\n2\n2\n1\n1\n1\n2\n1\n2\nTable A.10: Ranking Kaggle2\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n4\n5\n5\n5\n5\n5\n10\n5\n10\nbnb\n13\n13\n13\n11\n12\n12\n13\n12\n14\ndt\n9\n9\n4\n4\n4\n4\n7\n3\n8\ngnb\n14\n14\n14\n12\n14\n14\n14\n14\n11\nknn\n12\n12\n10\n9\n10\n10\n11\n9\n13\nlda\n11\n11\n12\n10\n13\n13\n8\n13\n7\nlr\n7\n7\n7\n7\n7\n7\n9\n7\n9\nqda\n10\n10\n11\n8\n11\n11\n12\n11\n12\nsvm\n8\n8\n9\n14\n9\n9\n1\n10\n1\nab\n6\n6\n6\n13\n6\n6\n6\n6\n6\ngb\n3\n3\n1\n2\n1\n1\n4\n1\n5\nlgbm\n1\n1\n2\n1\n2\n2\n2\n2\n2\nrf\n4\n4\n8\n6\n8\n8\n5\n8\n4\nxgb\n2\n1\n3\n3\n3\n3\n3\n4\n3\n209\n Appendix A: Predicting employee turnover\nTable A.11: Ranking Kaggle3\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n6\n7\n12\n6\n6\n6\n5\n6\n3\nbnb\n14\n12\n6\n11\n11\n11\n11\n11\n12\ndt\n7\n6\n10\n4\n8\n8\n10\n8\n7\ngnb\n9\n9\n14\n13\n4\n1\n9\n1\n11\nknn\n12\n14\n11\n10\n10\n10\n7\n10\n1\nlda\n10\n10\n6\n8\n11\n11\n11\n11\n12\nlr\n8\n8\n9\n7\n11\n11\n11\n11\n9\nqda\n11\n11\n13\n9\n9\n9\n8\n9\n2\nsvm\n13\n13\n6\n14\n11\n11\n11\n11\n12\nab\n4\n3\n3\n12\n5\n5\n4\n5\n8\ngb\n2\n2\n2\n2\n2\n3\n2\n3\n5\nlgbm\n1\n1\n1\n1\n3\n4\n1\n4\n4",
  "51": "3\n4\n4\n5\n6\n3\nrf\n1\n1\n1\n2\n2\n3\n1\n4\n1\nxgb\n2\n2\n2\n1\n1\n1\n2\n1\n2\nTable A.10: Ranking Kaggle2\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n4\n5\n5\n5\n5\n5\n10\n5\n10\nbnb\n13\n13\n13\n11\n12\n12\n13\n12\n14\ndt\n9\n9\n4\n4\n4\n4\n7\n3\n8\ngnb\n14\n14\n14\n12\n14\n14\n14\n14\n11\nknn\n12\n12\n10\n9\n10\n10\n11\n9\n13\nlda\n11\n11\n12\n10\n13\n13\n8\n13\n7\nlr\n7\n7\n7\n7\n7\n7\n9\n7\n9\nqda\n10\n10\n11\n8\n11\n11\n12\n11\n12\nsvm\n8\n8\n9\n14\n9\n9\n1\n10\n1\nab\n6\n6\n6\n13\n6\n6\n6\n6\n6\ngb\n3\n3\n1\n2\n1\n1\n4\n1\n5\nlgbm\n1\n1\n2\n1\n2\n2\n2\n2\n2\nrf\n4\n4\n8\n6\n8\n8\n5\n8\n4\nxgb\n2\n1\n3\n3\n3\n3\n3\n4\n3\n209\n Appendix A: Predicting employee turnover\nTable A.11: Ranking Kaggle3\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n6\n7\n12\n6\n6\n6\n5\n6\n3\nbnb\n14\n12\n6\n11\n11\n11\n11\n11\n12\ndt\n7\n6\n10\n4\n8\n8\n10\n8\n7\ngnb\n9\n9\n14\n13\n4\n1\n9\n1\n11\nknn\n12\n14\n11\n10\n10\n10\n7\n10\n1\nlda\n10\n10\n6\n8\n11\n11\n11\n11\n12\nlr\n8\n8\n9\n7\n11\n11\n11\n11\n9\nqda\n11\n11\n13\n9\n9\n9\n8\n9\n2\nsvm\n13\n13\n6\n14\n11\n11\n11\n11\n12\nab\n4\n3\n3\n12\n5\n5\n4\n5\n8\ngb\n2\n2\n2\n2\n2\n3\n2\n3\n5\nlgbm\n1\n1\n1\n1\n3\n4\n1\n4\n4\nrf\n3\n5\n5\n3\n7\n7\n6\n7\n10\nxgb\n5\n4\n4\n5\n1\n2\n3\n2\n6\nTable A.12: Ranking Kaggle4\nClassifier\nAUC-PR\nAUC-ROC\nAccuracy\nBrier-Score\nF1-Score\nH-Measure\nPrecision\nRecall\nSpecificity\nann\n6\n5\n6\n6\n6\n6\n7\n6\n7\nbnb\n12\n13\n12\n10\n12\n13\n12\n11\n12\ndt\n5\n6\n5\n5\n5\n5\n5\n5\n5\ngnb\n14\n14\n14\n13\n14\n14\n14\n7\n14\nknn\n9\n11\n7\n8\n7\n7\n8\n8\n9\nlda\n11\n10\n11\n9\n13\n11\n11\n14\n11\nlr\n10\n9\n10\n7\n10\n10\n10\n12\n10\nqda\n13\n12\n13\n11\n11\n12\n13\n9\n13\nsvm\n7\n7\n8\n14\n9\n9\n6\n13\n6\nab\n8\n8\n9\n12\n8\n8\n9\n10\n8\ngb\n3\n3\n3\n3\n3\n3\n2\n3\n2\nlgbm\n1\n1\n1\n1\n1\n1\n1\n1\n1\nrf\n4\n4\n4\n4\n4\n4\n4\n4\n4\nxgb\n2\n2\n2\n2\n2\n2\n3\n2\n3\n210\n B\nData-driven internal mobility:\nSimilarity regularization gets the\njob done\n211\n Appendix B: Data-driven internal mobility\nB.1\nEmployee journey map\nFigure B.1: An anonymized employee journey map of dataset 1 displayed as\na directly-follows graph in Disco. Each rectangle corresponds to a job and\neach arc to a possible transition between these jobs. The high-quality figure\nis avaiable on GitHub.\n212\n B.2. Collaborative filtering\nthrough matrix factorization\nB.2\nCollaborative filtering\nthrough matrix factorization\nAlgorithm 3: Matrix Factorization\nInput\n: observed ratings R, initial matrices U and V , learning\nrate α, regularization parameters λ1 and λ2, learning\nsteps n, stopping threshold t\nOutput : matrix ˆR = U T V with estimated ratings\nfor steps = 1, 2, . . . , n do\nfor each element Ri,j do\n;\n> i ∈{u1, u2, . . . , um}, j ∈{v1, v2, . . . , vn}\nif Ri,j > 0 then\nei,j ∶= Ri,j −ˆRi,j ;\n> calculate error\nUi ∶= Ui + α(ei,jVj\nÍÒÒÒÒÒÒÑÒÒÒÒÒÒÏ\n(i)\n+ λ1Ui)\nÍÒÒÒÒÒÒÑÒÒÒÒÒÒÒÏ\n(ii)\n;\n> update vector Ui\nVj ∶= Vj + α(ei,jUi\nÍÒÒÒÒÒÒÑÒÒÒÒÒÒÏ\n(i)\n+ λ2Vj)\nÍÒÒÒÒÒÒÑÒÒÒÒÒÒÒÏ\n(ii)\n;\n> update vector Vj\ne ∶= 1\n2 ∑m\ni=1 ∑n\nj=1 Iij (Rij −U T\ni Vj)\n2\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(i)\n+ λ1\n2 ∥U∥2\nF + λ2\n2 ∥V ∥2\nF\nÍÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÑÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÏ\n(ii)\n;\nif error e < t then\nbreak ;\n> stop if error falls below threshold t\nReturn : ˆR = U T V\n213\n Appendix B: Data-driven internal mobility\nB.3\nHyperparameter tuning\nTable B.1: This table shows the details on hyperparameter tuning for the\nthree model-based approaches MF, MFSR, SV D, and four memory-based\napproaches with different similarity metrics. The SlopeOne algorithm does\nnot have any hyperparameters to be tuned. The third column shows all the\noptions that we consider in the grid search. The three columns on the right\ndisplay the corresponding optimal hyperparameter values for each dataset.\nMethod\nHyperpara.\nValues\nDataset 1\nDataset 2\nDataset 3\nMFSR\nλ\n{0.01, 0.02, 0.05, 0.1}\n0.02\n0.05\n0.02\nβ\n{0, 0.05, 0.1, 0.2, 0.5}\n0.1\n0.2\n0.1\nα\n{0.0001, 0.001}\n0.001\n0.0001\n0.001\nL\n{2, 3, 5, 10}\n3\n3\n2\nsteps\n{1000, 2000, 3000}\n3000\n3000\n1000\nMF\nλ\n{0.01, 0.02, 0.05, 0.1}\n0.02\n0.05\n0.05\nα\n{0.0001, 0.001}\n0.001\n0.001\n0.001\nL\n{2, 3, 5, 10}\n3\n2\n5\nsteps\n{1000, 2000, 3000}\n1000\n1000\n3000\nSVD\nn_factors\n{2, 3, 5, 10}\n5\n2\n2\nn_epochs\n{1000, 2000, 3000}\n1000\n1000\n1000\nreg_pu\n{0.001, 0.005, 0.01, 0.02, 0.05}\n0.001\n0.001\n0.001\nreg_qi\n{0.001, 0.005, 0.01, 0.02, 0.05}\n0.001\n0.001\n0.001\nKNNc\nk\n{2, 3, 5, 10}\n2\n2\n2\nKNNp\nk\n{2, 3, 5, 10}\n2\n2\n2\nKNNsmd\nk\n{2, 3, 5, 10}\n3\n2\n2\nKNNta\nk\n{2, 3, 5, 10}\n5\n2\n2\n214\n C\nRobust instance-dependent\ncost-sensitive classification\n215\n Appendix C: Robust instance-dependent cost-sensitive classification\nC.1\nResults on synthetic data\nTable C.1: This table displays the results of tests on synthetic data with\nno outlier and an outlier of size 100. We apply a 2 × 5-fold cross-validation\nprocedure with a train/test split ratio of 0.8/0.2.\nIn this table, r-cslogit\nalways performs at least equally good in comparison to cslogit. In terms\nof the cost-sensitive metric Savings, logit is always outperformed by cslogit\nand r-cslogit. Logit performs best in terms of cost-insensitive metrics, and\nits performance remains stable after increasing the size of the outlier, given\nits cost-insensitive nature. An exception is Specificity with a 90/10 class\nimbalance and an outlier of 100. However, given the relatively high standard\ndeviation of 0.11, these results are rather volatile because of the high class\nimbalance.\nNo outlier\nAoutlier = 100\nImbal.\nMetric\nlogit\ncslogit\nr-cslogit\nlogit\ncslogit\nr-cslogit\n50/50\nSavings\n0.68 ± 0.08\n0.80 ± 0.05\n0.80 ± 0.05\n0.68 ± 0.08\n0.80 ± 0.05\n0.80 ± 0.05\nAUC\n0.86 ± 0.04\n0.77 ± 0.06\n0.77 ± 0.06\n0.86 ± 0.04\n0.77 ± 0.06\n0.77 ± 0.06\nF1\n0.84 ± 0.04\n0.77 ± 0.05\n0.77 ± 0.05\n0.84 ± 0.04\n0.77 ± 0.05\n0.77 ± 0.05\nSpec\n0.88 ± 0.05\n0.77 ± 0.11\n0.77 ± 0.11\n0.88 ± 0.05\n0.77 ± 0.11\n0.77 ± 0.11\nSens\n0.83 ± 0.04\n0.78 ± 0.03\n0.78 ± 0.03\n0.83 ± 0.04\n0.78 ± 0.03\n0.78 ± 0.03\nBrier\n0.14 ± 0.04\n0.23 ± 0.06\n0.23 ± 0.06\n0.14 ± 0.04\n0.23 ± 0.06\n0.23 ± 0.06\n60/40\nSavings\n0.64 ± 0.11\n0.75 ± 0.06\n0.75 ± 0.06\n0.64 ± 0.11\n0.75 ± 0.04\n0.75 ± 0.04\nAUC\n0.85 ± 0.05\n0.76 ± 0.07\n0.76 ± 0.07\n0.85 ± 0.05\n0.77 ± 0.06\n0.76 ± 0.06\nF1\n0.88 ± 0.04\n0.79 ± 0.04\n0.79 ± 0.04\n0.88 ± 0.04\n0.79 ± 0.04\n0.79 ± 0.04\nSpec\n0.79 ± 0.06\n0.77 ± 0.13\n0.77 ± 0.13\n0.79 ± 0.06\n0.77 ± 0.12\n0.77 ± 0.12\nSens\n0.90 ± 0.05\n0.76 ± 0.04\n0.76 ± 0.04\n0.90 ± 0.05\n0.76 ± 0.03\n0.76 ± 0.04\nBrier\n0.14 ± 0.05\n0.24 ± 0.06\n0.24 ± 0.06\n0.14 ± 0.05\n0.24 ± 0.06\n0.24 ± 0.05\n70/30\nSavings\n0.52 ± 0.05\n0.70 ± 0.05\n0.70 ± 0.05\n0.52 ± 0.05\n0.70 ± 0.05\n0.70 ± 0.05\nAUC\n0.81 ± 0.02\n0.74 ± 0.03\n0.74 ± 0.03\n0.81 ± 0.02\n0.74 ± 0.03\n0.74 ± 0.03\nF1\n0.89 ± 0.01\n0.84 ± 0.03\n0.84 ± 0.03\n0.89 ± 0.01\n0.84 ± 0.03\n0.84 ± 0.03\nSpec\n0.68 ± 0.03\n0.62 ± 0.03\n0.62 ± 0.03\n0.68 ± 0.03\n0.62 ± 0.03\n0.62 ± 0.03\nSens\n0.94 ± 0.02\n0.87 ± 0.06\n0.87 ± 0.06\n0.94 ± 0.02\n0.87 ± 0.05\n0.87 ± 0.06\nBrier\n0.15 ± 0.02\n0.21 ± 0.04\n0.21 ± 0.04\n0.15 ± 0.02\n0.22 ± 0.04\n0.21 ± 0.04\n80/20\nSavings\n0.37 ± 0.10\n0.58 ± 0.15\n0.58 ± 0.15\n0.37 ± 0.10\n0.58 ± 0.15\n0.58 ± 0.15\nAUC\n0.80 ± 0.04\n0.74 ± 0.05\n0.74 ± 0.05\n0.80 ± 0.04\n0.74 ± 0.05\n0.74 ± 0.05\nF1\n0.93 ± 0.02\n0.90 ± 0.01\n0.90 ± 0.01\n0.93 ± 0.02\n0.90 ± 0.01\n0.90 ± 0.01\nSpec\n0.64 ± 0.06\n0.57 ± 0.13\n0.57 ± 0.13\n0.64 ± 0.06\n0.57 ± 0.13\n0.57 ± 0.13\nSens\n0.95 ± 0.03\n0.91 ± 0.04\n0.91 ± 0.04\n0.95 ± 0.03\n0.91 ± 0.04\n0.91 ± 0.04\nBrier\n0.12 ± 0.04\n0.16 ± 0.01\n0.16 ± 0.01\n0.12 ± 0.04\n0.16 ± 0.01\n0.16 ± 0.01\n90/10\nSavings\n0.23 ± 0.12\n0.36 ± 0.11\n0.36 ± 0.11\n0.23 ± 0.12\n0.38 ± 0.14\n0.38 ± 0.14\nAUC\n0.68 ± 0.03\n0.62 ± 0.02\n0.62 ± 0.02\n0.68 ± 0.03\n0.66 ± 0.05\n0.64 ± 0.05\nF1\n0.95 ± 0.01\n0.89 ± 0.04\n0.89 ± 0.04\n0.95 ± 0.01\n0.91 ± 0.03\n0.89 ± 0.04\nSpec\n0.38 ± 0.06\n0.37 ± 0.06\n0.37 ± 0.06\n0.38 ± 0.06\n0.41 ± 0.11\n0.41 ± 0.11\nSens\n0.97 ± 0.01\n0.86 ± 0.08\n0.86 ± 0.08\n0.97 ± 0.01\n0.90 ± 0.06\n0.90 ± 0.06\nBrier\n0.10 ± 0.02\n0.19 ± 0.07\n0.19 ± 0.07\n0.10 ± 0.02\n0.16 ± 0.05\n0.18 ± 0.06\n216\n C.1. Results on synthetic data\nTable C.2: This table displays the results of tests on synthetic data with\nan outlier of size 1,000 and an outlier of size 10,000. We apply a 2 × 5-fold\ncross-validation procedure with a train/test split ratio of 0.8/0.2. In terms\nof Savings, r-cslogit always outperforms the other two methods and remains\nstable after increasing the size of the outlier. Also in terms of cost-insensitive\nmetrics, the performance of r-cslogit remains stable. After increasing the\noutlier size, cslogit performs worse. This is analogous to the results as dis-\nplayed in Figure 5.5. Logit performs best in terms of cost-insensitive metrics\nand, given its cost-insensitive nature, its performance remains stable after\nincreasing the size of the outlier. The few times that logit is outperformed\nby either cslogit or r-cslogit in terms of cost-insensitive metrics, the perfor-\nmance scores have a rather high volatility. This is predominantly the case\nfor tests with high class imbalance.\nAoutlier = 1, 000\nAoutlier = 10, 000\nImbal.\nMetric\nlogit\ncslogit\nr-cslogit\nlogit\ncslogit\nr-cslogit\n50/50\nSavings\n0.68 ± 0.08\n0.60 ± 0.15\n0.80±0.05\n0.68 ± 0.08\n0.58 ± 0.11\n0.80±0.05\nAUC\n0.86±0.04\n0.83 ± 0.05\n0.77 ± 0.06\n0.86±0.04\n0.84 ± 0.03\n0.77 ± 0.06\nF1\n0.84±0.04\n0.82 ± 0.05\n0.77 ± 0.05\n0.84±0.04\n0.83 ± 0.03\n0.77 ± 0.05\nSpec\n0.88±0.05\n0.82 ± 0.07\n0.77 ± 0.11\n0.88±0.05\n0.83 ± 0.05\n0.77 ± 0.11\nSens\n0.83±0.04\n0.81 ± 0.05\n0.78 ± 0.03\n0.83 ± 0.04\n0.85±0.07\n0.78 ± 0.03\nBrier\n0.14±0.04\n0.17 ± 0.05\n0.23 ± 0.06\n0.14±0.04\n0.16 ± 0.03\n0.23 ± 0.06\n60/40\nSavings\n0.64 ± 0.11\n0.69 ± 0.10\n0.75±0.04\n0.64 ± 0.11\n0.58 ± 0.18\n0.75±0.04\nAUC\n0.85±0.05\n0.81 ± 0.07\n0.77 ± 0.06\n0.85±0.05\n0.84 ± 0.06\n0.76 ± 0.06\nF1\n0.88±0.04\n0.83 ± 0.08\n0.79 ± 0.04\n0.88±0.04\n0.87 ± 0.05\n0.79 ± 0.04\nSpec\n0.79 ± 0.06\n0.80±0.08\n0.77 ± 0.12\n0.79±0.06\n0.78 ± 0.07\n0.77 ± 0.12\nSens\n0.90±0.05\n0.82 ± 0.13\n0.76 ± 0.03\n0.90±0.05\n0.90 ± 0.06\n0.76 ± 0.04\nBrier\n0.14±0.05\n0.19 ± 0.07\n0.23 ± 0.06\n0.14±0.05\n0.15 ± 0.06\n0.24 ± 0.05\n70/30\nSavings\n0.52 ± 0.05\n0.42 ± 0.16\n0.70±0.05\n0.52 ± 0.05\n0.32 ± 0.20\n0.70±0.05\nAUC\n0.80±0.02\n0.77 ± 0.05\n0.74 ± 0.03\n0.80±0.02\n0.79 ± 0.06\n0.74 ± 0.03\nF1\n0.89±0.01\n0.88 ± 0.02\n0.84 ± 0.03\n0.89±0.01\n0.88 ± 0.03\n0.84 ± 0.03\nSpec\n0.68±0.03\n0.60 ± 0.15\n0.61 ± 0.03\n0.67±0.03\n0.64 ± 0.07\n0.61 ± 0.03\nSens\n0.93±0.02\n0.92 ± 0.06\n0.87 ± 0.06\n0.93±0.02\n0.89 ± 0.06\n0.87 ± 0.06\nBrier\n0.15±0.02\n0.17 ± 0.03\n0.21 ± 0.04\n0.15±0.02\n0.16 ± 0.04\n0.21 ± 0.04\n80/20\nSavings\n0.37 ± ",
  "52": "07\n0.78 ± 0.03\nBrier\n0.14±0.04\n0.17 ± 0.05\n0.23 ± 0.06\n0.14±0.04\n0.16 ± 0.03\n0.23 ± 0.06\n60/40\nSavings\n0.64 ± 0.11\n0.69 ± 0.10\n0.75±0.04\n0.64 ± 0.11\n0.58 ± 0.18\n0.75±0.04\nAUC\n0.85±0.05\n0.81 ± 0.07\n0.77 ± 0.06\n0.85±0.05\n0.84 ± 0.06\n0.76 ± 0.06\nF1\n0.88±0.04\n0.83 ± 0.08\n0.79 ± 0.04\n0.88±0.04\n0.87 ± 0.05\n0.79 ± 0.04\nSpec\n0.79 ± 0.06\n0.80±0.08\n0.77 ± 0.12\n0.79±0.06\n0.78 ± 0.07\n0.77 ± 0.12\nSens\n0.90±0.05\n0.82 ± 0.13\n0.76 ± 0.03\n0.90±0.05\n0.90 ± 0.06\n0.76 ± 0.04\nBrier\n0.14±0.05\n0.19 ± 0.07\n0.23 ± 0.06\n0.14±0.05\n0.15 ± 0.06\n0.24 ± 0.05\n70/30\nSavings\n0.52 ± 0.05\n0.42 ± 0.16\n0.70±0.05\n0.52 ± 0.05\n0.32 ± 0.20\n0.70±0.05\nAUC\n0.80±0.02\n0.77 ± 0.05\n0.74 ± 0.03\n0.80±0.02\n0.79 ± 0.06\n0.74 ± 0.03\nF1\n0.89±0.01\n0.88 ± 0.02\n0.84 ± 0.03\n0.89±0.01\n0.88 ± 0.03\n0.84 ± 0.03\nSpec\n0.68±0.03\n0.60 ± 0.15\n0.61 ± 0.03\n0.67±0.03\n0.64 ± 0.07\n0.61 ± 0.03\nSens\n0.93±0.02\n0.92 ± 0.06\n0.87 ± 0.06\n0.93±0.02\n0.89 ± 0.06\n0.87 ± 0.06\nBrier\n0.15±0.02\n0.17 ± 0.03\n0.21 ± 0.04\n0.15±0.02\n0.16 ± 0.04\n0.21 ± 0.04\n80/20\nSavings\n0.37 ± 0.10\n0.37 ± 0.17\n0.58±0.15\n0.37 ± 0.10\n0.10 ± 0.47\n0.58±0.15\nAUC\n0.80±0.04\n0.72 ± 0.04\n0.74 ± 0.05\n0.80±0.04\n0.76 ± 0.08\n0.74 ± 0.05\nF1\n0.93±0.02\n0.91 ± 0.03\n0.90 ± 0.01\n0.93±0.02\n0.91 ± 0.04\n0.90 ± 0.01\nSpec\n0.64±0.06\n0.47 ± 0.06\n0.57 ± 0.13\n0.64±0.06\n0.60 ± 0.19\n0.57 ± 0.13\nSens\n0.95 ± 0.03\n0.96±0.07\n0.91 ± 0.04\n0.95±0.03\n0.92 ± 0.08\n0.91 ± 0.04\nBrier\n0.12±0.04\n0.14 ± 0.04\n0.16 ± 0.01\n0.12±0.04\n0.15 ± 0.05\n0.16 ± 0.01\n90/10\nSavings\n0.23 ± 0.12\n0.17 ± 0.13\n0.36±0.11\n0.23 ± 0.12\n0.19 ± 0.16\n0.36±0.11\nAUC\n0.68±0.03\n0.62 ± 0.05\n0.64 ± 0.05\n0.68±0.03\n0.66 ± 0.06\n0.64 ± 0.05\nF1\n0.95±0.01\n0.95 ± 0.00\n0.89 ± 0.04\n0.95±0.01\n0.94 ± 0.01\n0.89 ± 0.04\nSpec\n0.38 ± 0.06\n0.26 ± 0.10\n0.41±0.13\n0.38 ± 0.06\n0.34 ± 0.13\n0.41±0.13\nSens\n0.97 ± 0.01\n0.98±0.06\n0.87 ± 0.08\n0.97 ± 0.01\n0.98±0.06\n0.87 ± 0.08\nBrier\n0.10±0.02\n0.10 ± 0.01\n0.18 ± 0.06\n0.10±0.02\n0.11 ± 0.01\n0.18 ± 0.06\n217\n  D\nDecision-centric fairness:\nEvaluation and optimization for\nresource allocation problems\n219\n Appendix D: Decision-centric fairness\nD.1\nDataset details\nD.1.1\nLabel flipping for inducing additional bias\nAlgorithm 4: Informed Label Flipping for Bias Induction\nInput\n: D = {(xi, yi, si)}N\ni=1, protected group value s ∈{0, 1}, bias\nrate r ∈[0, 1]\nOutput: Biased dataset D′\nr\nTrain a classifier m on D ;\n˜y ←m(x, s) ;\n> predicted scores ˜yi ∈[0, 1]\nDs\n0 ←{(xi, yi, si, ˜yi) ∣si = s and yi = 0} ;\n> subset for bias\nk ←r ⋅∣Ds\n0∣;\nRank Ds\n0 by ˜y in descending order ;\nfor each (xi, yi) ∈top-k of Ds\n0 do\nyi ←1 ;\n> flip labels\nReturn: D′\nr = {(xi, yi, si)}N\ni=1 ;\n> return biased dataset\nD.1.2\nBaseline discriminatory behavior\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.0\n0.5\n1.0\n1.5\n2.0\nProbability density\nτ\ns=0\ns=1\n(a) Bias rate 0.25\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nProbability density\nτ\ns=0\ns=1\n(b) Bias rate 0.50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0\n1\n2\n3\n4\nProbability density\nτ\ns=0\ns=1\n(c) Bias rate 0.75\nFigure D.1:\nBaseline discriminatory behavior for the TelecomKaggle\ndatasets. This figure shows the score distributions (PDFs) for two groups\n(s = 0 and s = 1), as estimated by a model without unfairness penalty\n(λ = 0), under three semi-synthetic bias rates. The decision-making region\nis defined as τ = 0.7.\nHigher semi-synthetic bias rates lead to increased\nbaseline discriminatory behavior, as summarized below in Table D.1.\n220\n D.1. Dataset details\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nProbability density\nτ\ns=0\ns=1\nFigure D.2:\nBaseline discrimi-\nnatory behavior for the Churn\ndataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted score\n0\n5\n10\n15\n20\nProbability density\nτ\ns=0\ns=1\nFigure D.3:\nBaseline discrimi-\nnatory\nbehavior\nfor\nthe\nAdult\ndataset.\nTable D.1: Baseline discriminatory behavior for the various datasets for\nτ = 0.7, corresponding to Figures D.1-D.3.\nDataset\nBias rate\nABPCτ\nABCCτ\nTelecomKaggle\n0.25\n0.16\n0.06\n0.50\n0.30\n0.12\n0.75\n0.43\n0.19\nChurn\n-\n0.03\n0.01\nAdult\n-\n0.13\n0.02\n221\n Appendix D: Decision-centric fairness\nD.2\nImplementation\nWe employ a fully-connected MLP, implemented in PyTorch, where network\nparameters are optimized using Adam optimizer [396] with a learning rate\n0.01 for TelecomKaggle and Churn, and 0.001 for Adult. We use ReLu acti-\nvation functions in the hidden layers. For the final layer, we use a sigmoid\nactivation function to ensure outputs in the range [0, 1]. We use a standard\nBCE loss function for the first 15 epochs and then a composite loss function\nfor the remainder of the training. We employ early stopping after 20 epochs\nwithout model improvement in terms of composite loss on the validation\ndata. Per dataset, hyperparameters are selected for the case where λ = 0.\nTo ensure enough observations remain for computing the decision-centric\nfairness loss—given that each training batch considers only a subset of data\nrestricted to the top n% percentile, further divided across the two protected\ngroups and limited to the training split—we set the batch size to 1024.\nTable D.2: Hyperparameter search space.\nHyperparameter\nValues\nTelecomKaggle\nChurn\nAdult\n0.25\n0.50\n0.75\nNr. of hidden layers\n{2, 3, 4}\n3\n2\n4\n2\n2\nHidden layer size\n{16, 32, 64, 128}\n64\n64\n32\n32\n64\nDropout probability\n{0, 0.01, 0.1}\n0\n0\n0\n0.01\n0.01\nL2 regularization\n{0, 0.01, 0.05}\n0.01\n0.01\n0.01\n0\n0.01\n222\n D.3. Additional results\nD.3\nAdditional results\n223\n Appendix D: Decision-centric fairness\nABPCτ\n0.0\n0.2\n0.4\n0.6\n0.8\nAUC-PRτ\nBias: 0.25, τ: 0.5\nABPCτ\nAUC-PRτ\nBias: 0.25, τ: 0.7\nABPCτ\nAUC-PRτ\nBias: 0.25, τ: 0.8\nGlobal reg.\nLocal reg.\nBiased model\nABPCτ\n0.0\n0.2\n0.4\n0.6\n0.8\nAUC-PRτ\nBias: 0.5, τ: 0.5\nABPCτ\nAUC-PRτ\nBias: 0.5, τ: 0.7\nABPCτ\nAUC-PRτ\nBias: 0.5, τ: 0.8\n0.0\n0.2\n0.4\n0.6\nABPCτ\n0.0\n0.2\n0.4\n0.6\n0.8\nAUC-PRτ\nBias: 0.75, τ: 0.5\n0.0\n0.2\n0.4\n0.6\nABPCτ\nAUC-PRτ\nBias: 0.75, τ: 0.7\n0.0\n0.2\n0.4\n0.6\nABPCτ\nAUC-PRτ\nBias: 0.75, τ: 0.8\nFigure D.4: Results on the TelecomKaggle dataset for different bias rates,\nwith fairness measured by ABPCτ.\nThe figure illustrates the effect of a\nvarying size of the decision-making region with τ = 0.5, 0.7, 0.8 (columns) on\nthe decision-centric fairness-predictive performance trade-off.\nThe orange\nand blue lines represent decision-centric and global fairness induction, re-\nspectively, while the model without unfairness penalty (i.e., with λ = 0) is\nmarked with a red star.\n224\n D.3. Additional results\n0.02\n0.04\nABPCτ\n0.4\n0.6\nAUC-PRτ\nτ: 0.5\n0.02\n0.04\nABPCτ\nAUC-PRτ\nτ: 0.7\n0.02\n0.04\nABPCτ\nAUC-PRτ\nτ: 0.8\nGlobal reg.\nLocal reg.\nBiased model\nFigure D.5: Results on the Churn dataset with fairness measured by ABPCτ.\nThe figure illustrates the effect of a varying size of the decision-making region\nwith τ = 0.5, 0.7, 0.8 (columns) on the decision-centric fairness-predictive\nperformance trade-off. The orange and blue lines represent decision-centric\nand global fairness induction, respectively, while the model without unfair-\nness penalty (i.e., with λ = 0) is marked with a red star.\n0.00\n0.01\n0.02\nABCCτ\n0.4\n0.6\nAUC-PRτ\nτ: 0.5\n0.00\n0.01\n0.02\nABCCτ\nAUC-PRτ\nτ: 0.7\n0.00\n0.01\n0.02\nABCCτ\nAUC-PRτ\nτ: 0.8\nGlobal reg.\nLocal reg.\nBiased model\nFigure D.6: Results on the Churn dataset with fairness measured by ABCCτ.\nThe figure illustrates the effect of a varying size of the decision-making region\nwith τ = 0.5, 0.7, 0.8 (columns) on the decision-centric fairness-predictive\nperformance trade-off. The orange and blue lines represent decision-centric\nand global fairness induction, respectively, while the model without unfair-\nness penalty (i.e., with λ = 0) is marked with a red star.\n225\n Appendix D: Decision-centric fairness\n0.0\n0.1\n0.2\n0.3\nABPCτ\n0.2\n0.4\n0.6\nAUC-PRτ\nτ: 0.4\n0.0\n0.1\n0.2\n0.3\nABPCτ\nAUC-PRτ\nτ: 0.5\n0.0\n0.1\n0.2\n0.3\nABPCτ\nAUC-PRτ\nτ: 0.7\n0.0\n0.1\n0.2\n0.3\nABPCτ\nAUC-PRτ\nτ: 0.8\nGlobal reg.\nLocal reg.\nBiased model\nFigure D.7: Results on the Adult dataset with fairness measured by ABPCτ.\nThe figure illustrates the effect of a varying size of the decision-making region\nwith τ = 0.4, 0.5, 0.7, 0.8 (columns) on the decision-centric fairness-predictive\nperformance trade-off. The orange and blue lines represent decision-centric\nand global fairness induction, respectively, while the model without unfair-\nness penalty (i.e., with λ = 0) is marked with a red star.\n0.000\n0.025\n0.050\n0.075\nABCCτ\n0.2\n0.4\n0.6\nAUC-PRτ\nτ: 0.4\n0.000\n0.025\n0.050\n0.075\nABCCτ\nAUC-PRτ\nτ: 0.5\n0.000\n0.025\n0.050\n0.075\nABCCτ\nAUC-PRτ\nτ: 0.7\n0.000\n0.025\n0.050\n0.075\nABCCτ\nAUC-PRτ\nτ: 0.8\nGlobal reg.\nLocal reg.\nBiased model\nFigure D.8: Results on the Adult dataset with fairness measured by ABCCτ.\nThe figure illustrates the effect of a varying size of the decision-making region\nwith τ = 0.4, 0.5, 0.7, 0.8 (columns) on the decision-centric fairness-predictive\nperformance trade-off. The orange and blue lines represent decision-centric\nand global fairness induction, respectively, while the model without unfair-\nness penalty (i.e., with λ = 0) is marked with a red star.\n226\n E\nUplift modeling with continuous\ntreatments:\nA predict-then-optimize approach\n227\n Appendix E: Uplift modeling with continuous treatments\nE.1\nNotation overview\nTable E.1: This table summarizes the notation in this paper.\nD = {(xi, si, yi)N\ni=1}\nDataset containing N entities\nX\nFeature space\nS\nTreatment space\nY\nOutcome space\nX ∈X\nFeatures\nS ∈S\nTreatment\nY ∈Y\nOutcome\nµ ∶S × X →[0, 1]\nConditional average dose response function\nτ ∶S × X →[−1, 1]\nConditional average dose effect function\nτs(x)\nTreatment effect for the features-dose pair {x, s}\nˆµ ∶S × X →[0, 1]\nEstimated conditional average dose response function\nˆτ ∶S × X →[−1, 1]\nEstimated conditional average dose effect function\nˆτs(x)\nEstimated dose effect for the features-dose pair {x, s}\nδ\nNumber of dose bins\nˆτ(x) ∈[−1, 1]δ\nVector with δ CADE estimates\nπ ∶X →{0, 1}δ\nTreatment assignment policy (unconstrained)\nfor a single entity\nC ∈R(N×δ)\nTreatment cost matrix\nΨi(π) ∈R≥0\nTreatment cost for a single entity\nb ∈RN\n≥0\nBenefit vector\nT ∈[−1, 1](N×δ)\nMatrix with (N × δ) CADE values (ground-truth)\nˆT ∈[−1, 1](N×δ)\nMatrix with (N × δ) CADE values (estimated)\nUi ∶{0, 1}δ ×",
  "53": "tments\nE.1\nNotation overview\nTable E.1: This table summarizes the notation in this paper.\nD = {(xi, si, yi)N\ni=1}\nDataset containing N entities\nX\nFeature space\nS\nTreatment space\nY\nOutcome space\nX ∈X\nFeatures\nS ∈S\nTreatment\nY ∈Y\nOutcome\nµ ∶S × X →[0, 1]\nConditional average dose response function\nτ ∶S × X →[−1, 1]\nConditional average dose effect function\nτs(x)\nTreatment effect for the features-dose pair {x, s}\nˆµ ∶S × X →[0, 1]\nEstimated conditional average dose response function\nˆτ ∶S × X →[−1, 1]\nEstimated conditional average dose effect function\nˆτs(x)\nEstimated dose effect for the features-dose pair {x, s}\nδ\nNumber of dose bins\nˆτ(x) ∈[−1, 1]δ\nVector with δ CADE estimates\nπ ∶X →{0, 1}δ\nTreatment assignment policy (unconstrained)\nfor a single entity\nC ∈R(N×δ)\nTreatment cost matrix\nΨi(π) ∈R≥0\nTreatment cost for a single entity\nb ∈RN\n≥0\nBenefit vector\nT ∈[−1, 1](N×δ)\nMatrix with (N × δ) CADE values (ground-truth)\nˆT ∈[−1, 1](N×δ)\nMatrix with (N × δ) CADE values (estimated)\nUi ∶{0, 1}δ × [−1, 1]δ →R\nPolicy uplift of single entity\nExpected (U\nexp\ni\n), prescribed (U\npresc\ni\n),\nand optimal (U\nopt\ni\n)\nVi ∶{0, 1}δ × [−1, 1]δ × R →R\nPolicy value of single entity\nExpected (V\nexp\ni\n), prescribed (V\npresc\ni\n),\nand optimal (V\nopt\ni\n)\nB\nΠ∶R+ × X N →{0, 1}(N×δ)\nTreatment assignment policy (constrained) over all entities\nExpected optimal (\nB\nΠ\nexp\n), ground-truth optimal (\nB\nΠ\nopt\n)\nU∶{0, 1}(N×δ) × [−1, 1](N×δ) →R\nPolicy uplift over all entities\nExpected (U\nexp), prescribed (U\npresc), and optimal (U\nopt)\nV ∶{0, 1}(N×δ) × [−1, 1](N×δ) × RN →R\nPolicy value over all entities\nExpected (V\nexp), prescribed (V\npresc), and optimal (V\nopt)\nA\nProtected sensitive attribute\nϵDT ∈[0, 1]\nSlack for allocation fairness (Disparate Treatment)\nϵDO ∈[0, 1]\nSlack for outcome fairness (Disparate Outcome)\n228\n E.2. Assumptions and mathematical justification\nof CADE identification\nE.2\nAssumptions and mathematical\njustification of CADE identification\nUnder the Rubin-Neyman potential outcomes framework, the CADE τs(x) =\nE[Y (s) −Y (0) ∣X = x] quantifies the expected outcome of a continuous\ntreatment dose s relative to a baseline (i.e., no treatment). Based on the\nproof for the binary treatment setting by Neal [397], we prove that τs(x)\nis identifiable as (E.5) and estimable via causal machine learning methods\nunder three assumptions: Consistency, Ignorability, Overlap (Section 7.4.1).\nProof. From the definition of τs(x) and linearity of expectation:\nτs(x) = E[Y (s) −Y (0) ∣X = x]\n(E.1)\n= E[Y (s) ∣X = x] −E[Y (0) ∣X = x]\n(E.2)\nBy Ignorability, potential outcomes are independent of treatment assign-\nment given X:\nE[Y (s) ∣X = x] = E[Y (s) ∣X = x, S = s].\n(E.3)\nApplying Consistency to (E.3):\nE[Y (s) ∣X = x, S = s] = E[Y ∣X = x, S = s].\n(E.4)\nCombining (E.2)–(E.4):\nτs(x) = E[Y ∣X = x, S = s] −E[Y ∣X = x, S = 0],\n(E.5)\nwhere Overlap ensures the conditional expectations in (E.3)-(E.5) are well-\ndefined.\n229\n Appendix E: Uplift modeling with continuous treatments\nE.3\nDetails regarding semi-synthetic data\nThe original IHDP dataset contains 747 observations and 25 features. Fol-\nlowing [340], synthetic counterfactuals are generated by Equations E.6-E.9\nwhere Scon = {1, 2, 3, 5, 6} is the index set of continuous features, Sbin,1 =\n{4, 7, 8, 9, 10, 11, 12, 13, 14, 15} and\nSbin,2 = {16, 17, 18, 19, 20, 21, 22, 23, 24, 25} are two sets of binary features,\nc1 = E [\n∑i∈Sdis,1 xi\n∣Sbin,1∣\n], and c2 = E [\n∑i∈Sdis,2 xi\n∣Sbin,2∣\n].\n˜t ∣x =\nx1\n1 + x2 +\nmax(x3, x5, x6)\n0.2 + min(x3, x5, x6)\n+ tanh (5 ⋅\n∑i∈Sbin,2(xi −c2)\n∣Sbin,2∣\n) −2 + N(0, 0.25)\n(E.6)\nt = (1 + exp(−2˜t))\n−1\n(E.7)\n˜y ∣x, t = sin(3πt)\n1.2 −t ⋅( tanh (5 ⋅\n∑i∈Sbin,1(xi −c1)\n∣Sbin,1∣\n)\n+\nexp (0.2(x1 −x6))\n0.5 + 5 ⋅min(x2, x3, x5)) + N(0, 0.25)\n(E.8)\ny =\n˜y −min(˜y)\nmax( ˜y) −min(˜y)\n(E.9)\nFollowing the literature, features are preprocessed to follow a standard\nnormal distribution, and the generated treatments are normalized to fall\nwithin the range [0, 1] [340], [398]. Additionally, we also standardize the\noutcomes so that y ∈[0, 1].\n230\n E.4. Number of dose bins δ\nE.4\nNumber of dose bins δ\nThroughout the experiments, the number of available treatment bins δ is\nset to 10.\nFigure E.1 illustrates the relationship between the number of\ntreatment bins δ, U presc, and calculation time in seconds. For the semi-\nsynthetic IHDP dataset, the increased benefit of using smaller-grained bins\nquickly caps out, while the required computation time continues to rise.\nFrom an application perspective, also not each granularity of doses should\nbe considered. For instance, lending costs occur only in specific increments,\nand in an HR setting, training hours are typically scheduled in multiples of\nthe session duration. Given these considerations, δ is set to 10.\nFigure E.1: Effect of available dose bins on U presc and calculation time\n231\n Appendix E: Uplift modeling with continuous treatments\nE.5\nHyperparameter tuning\nTable E.2: Hyperparameter search space. The selected hyperparameters are\nunderlined.\nMethod\nHyperparameter\nValues\n#Models\nS-Learner (rf)\nEstimators\n{10, 20, 50, 100, 200, 500}\n72\nCriterion\n{Sq. error, Abs. error}\nMax depth\n{5, 15, None}\nMax features\n{Sqrt, Log2}\nS-Learner (mlp)\nLearning rate\n{0.001, 0.01}\n128\nL2 regularization\n{0.0, 0.1}\nBatch size\n{64, 128}\nNum layers\n{2, 3}\nHidden size\n{32, 64}\nSteps\n{500, 1000, 2000, 5000}\nOptimizer\n{Adam}\nDRNet\nLearning rate\n{0.001, 0.01}\n256\nL2 regularization\n{0.0, 0.1}\nBatch size\n{64, 128}\nNum repr. layers\n{2, 3}\nNum inf. layers\n{1, 2}\nHidden size\n{32, 64}\nSteps\n{500, 1000, 2000, 5000}\nNum dose strata\n{10}\nOptimizer\n{Adam}\nVCNet\nLearning rate\n{0.001, 0.01}\n32\nBatch size\n{64, 128}\nHidden size\n{32, 64}\nSteps\n{500, 1000, 2000, 5000}\nOptimizer\n{Adam}\n232\n E.6. More details on Experiment 1\nE.6\nMore details on Experiment 1\nE.6.1\nDose-response estimation\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nY\nCADR: S-Learner (rf)\nTrue response\nEstimated response\nTrue CADR\nEstimated CADR\n(a)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nY\nCADR: S-Learner (mlp)\nTrue response\nEstimated response\nTrue CADR\nEstimated CADR\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nY\nCADR: DRNet\nTrue response\nEstimated response\nTrue CADR\nEstimated CADR\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nY\nCADR: VCNet\nTrue response\nEstimated response\nTrue CADR\nEstimated CADR\n(d)\nFigure E.2: This figure presents the ground-truth dose responses in blue,\nwith the blue dots representing the factual outcomes, and their correspond-\ning estimates in orange, for four different predictive methods on a separate\ntest set from the IHDP dataset.\nE.6.2\nScalability\nWe provide additional runtime analysis for increasing dataset sizes.\nThe\ndataset size is semi-synthetically scaled by a factor N (with N = 1 corre-\nsponding to the original dataset), by randomly oversampling observations\nand adding slight noise to ensure that each observation remains unique. We\nadopt the setup of Experiment 1 (Section 7.5.3), where the available bud-\nget is also scaled by N, and compare the runtime of the ILP solution with\nthat of the heuristic approach. Figure E.3 shows the runtime (in seconds)\nas a function of the problem size (measured by the scaling factor N). Both\n233\n Appendix E: Uplift modeling with continuous treatments\n1\n2\n4\n8\n16\n32\n64\n96\nN\n100\n101\n102\n103\nCalc. time (sec.)\nScalability\nMethod\nHeuristic\nILP\nFigure E.3: Runtime of the ILP and heuristic as a function of problem size.\nThe original dataset size corresponds with N = 1.\naxes are shown on a logarithmic scale. The clear advantage of the heuristic\nmethod is that it scales well. However, it does not take into account any\nside constraints, which are allowed by the ILP. The downside of the ILP is\nthat runtimes increase rapidly with the problem size, becoming impractical\nfor large-scale instances. This highlights the classic trade-off: while the ILP\nallows for more expressive modeling and constraint handling, its computa-\ntional cost means that problem size is limited. In contrast, the heuristic\nremains feasible in terms of computation time even as the dataset grows.\n234\n F\nIndustry co-creation\n235\n Appendix F: Industry co-creation\nF.1\nImplementation of EJMs\n1\nFigure F.1: An EJM as implemented in PowerBI. Filters on the right allow\nfor any subselection of data and dynamically regenerating a new EJM.\n236\n F.2. Implementation of turnover prediction\nF.2\nImplementation of turnover prediction\nFigure F.2:\nTurnover prediction as implemented in PowerBI, with a\nbeeswarm plot displaying SHAP values.\nFilters on the right and select-\ning bars in the subfigures allow for any subselection of data and dynamically\nregenerating a new EJM.\n237\n  Publication list\nJournal articles (peer reviewed)\nS. De Vos, C. Bockel-Rickermann, J. Van Belle, and W. Verbeke, \"Predict-\ning Employee Turnover: Scoping and Benchmarking the State-of-the-\nArt,\" Business & Information Systems Engineering, 2024. doi: 10.1007/\ns12599-024-00898-z.\nS. De Vos, J. De Smedt, M. Verbruggen, and W. Verbeke, \"Data-driven in-\nternal mobility: Similarity regularization gets the job done,\" Knowledge-\nBased Systems, vol.\n295, Art.\nno.\n111824, 2024.\ndoi:\n10.1016/\nj.knosys.2024.111824.\nS. De Vos, T. Vanderschueren, T. Verdonck, and W. Verbeke, \"Robust\ninstance-dependent cost-sensitive classification,\" Advances in Data Anal-\nysis and Classification, 2023. doi: 10.1007/ s11634-022-00533-3.\nConference proceedings (peer reviewed)\nJ. Peeperkorn, S. De Vos, Placeholder CAISE paper: accepted, not yet\npublished\nBook chapters (peer reviewed)\nS. De Vos, J. De Smedt, C. Wuytens, and W. Verbeke, \"Leveraging Process\nMining to Optimize Internal Employee Mobility Strategies,\" in Business\nProcess Management Cases Vol. 3: Implementation in Practice, Cham,\nSwitzerland: Springer Nature, 2025, pp. 15–28. doi: 10.1007/ 978-3-\n031-80793-0. ISBN: 978-3-031-80792-3.\nPreprints\nS. De Vos, J. Van Belle, A. Algaba, W. Verbeke, S. Verboven, \"Decision-\ncentric fairness:\nEvaluation and optimization for resource allocation\nproblems,\" arXiv, 2024. doi: 10.485",
  "54": ": Similarity regularization gets the job done,\" Knowledge-\nBased Systems, vol.\n295, Art.\nno.\n111824, 2024.\ndoi:\n10.1016/\nj.knosys.2024.111824.\nS. De Vos, T. Vanderschueren, T. Verdonck, and W. Verbeke, \"Robust\ninstance-dependent cost-sensitive classification,\" Advances in Data Anal-\nysis and Classification, 2023. doi: 10.1007/ s11634-022-00533-3.\nConference proceedings (peer reviewed)\nJ. Peeperkorn, S. De Vos, Placeholder CAISE paper: accepted, not yet\npublished\nBook chapters (peer reviewed)\nS. De Vos, J. De Smedt, C. Wuytens, and W. Verbeke, \"Leveraging Process\nMining to Optimize Internal Employee Mobility Strategies,\" in Business\nProcess Management Cases Vol. 3: Implementation in Practice, Cham,\nSwitzerland: Springer Nature, 2025, pp. 15–28. doi: 10.1007/ 978-3-\n031-80793-0. ISBN: 978-3-031-80792-3.\nPreprints\nS. De Vos, J. Van Belle, A. Algaba, W. Verbeke, S. Verboven, \"Decision-\ncentric fairness:\nEvaluation and optimization for resource allocation\nproblems,\" arXiv, 2024. doi: 10.48550/ arXiv.2504.20642.\nJ. Peeperkorn, S. De Vos, \"Achieving Group Fairness through Indepen-\ndence in Predictive Process Monitoring,\" arXiv, 2024. doi: 10.48550/\n239\n Publication list\narXiv.2412.04914\nS. De Vos, C. Bockel-Rickermann, S. Lessmann, and W. Verbeke, \"Up-\nlift modeling with continuous treatments: A predict-then-optimize ap-\nproach,\" arXiv, 2024. doi: 10.48550/ arXiv.2412.09232.\nD. Caljon, J. Vercauteren, S. De Vos, W. Verbeke, and J. Van Belle, \"Using\ndynamic loss weighting to boost improvements in forecast stability,\"\narXiv, 2024. doi: 10.48550/ arXiv.2409.18267.\nAbstracts / Presentations / Posters\nS. De Vos, J. Van Belle, A. Algaba, W. Verbeke, and S. Verboven, \"Decision-\nCentric Fairness: Evaluation and Optimization for Classification Prob-\nlems,\" presented at the Joint ORBEL-NGB Conference on Operations\nResearch, Maastricht, Netherlands, Jan. 29–31, 2025.\nS. De Vos and W. Verbeke, \"A predict-then-optimize approach for uplift\nmodeling with continuous individual treatment effects,\" presented at\nthe 33rd European Conference on Operational Research, Copenhagen,\nDenmark, Jun. 30–Jul. 3, 2024.\nS. De Vos, J. De Smedt, M. Verbruggen, and W. Verbeke, \"A survey and\nbenchmarking experiment of the state-of-the-art in employee turnover\nprediction,\" presented at the 37th Annual Conference of the Belgian\nOperational Research Society, ORBEL 37, Liège, May 25–26, 2023.\nD. Caljon, J. Vercauteren, S. De Vos, and J. Van Belle, \"Using adaptive\nloss balancing to boost improvements in forecast stability,\" presented at\nORBEL 37, Liège, May 25–26, 2023.\nS. De Vos, C. Wuytens, J. De Smedt, and W. Verbeke, \"Process Mining-\nDriven Analytics of Human Resources,\" presented at the 4th Interna-\ntional Conference on Process Mining (ICPM), Bolzano, Italy, 2022.\nS. De Vos, J. De Smedt, and W. Verbeke, \"Internal Placement: Job Recom-\nmender Systems with Social Regularization,\" presented at The Control\nRoom of the Future: AI Empowered Dashboards, Ghent, Belgium, 2022.\nS. De Vos, J. De Smedt, M. Verbruggen, and W. Verbeke, \"Internal Place-\nment:\nJob Recommender Systems with Social Regularization,\" pre-\nsented at the European Conference on Machine Learning and Princi-\nples and Practice of Knowledge Discovery in Databases (ECML-PKDD),\nGrenoble, France, Sep. 19–23, 2022.\n240\n S. De Vos, J. De Smedt, and W. Verbeke, \"Human Resource Analytics: Em-\nployee Journeys from a Process Perspective,\" presented at ORBEL36,\nGhent, Belgium, Sep. 12–13, 2022.\nS. De Vos, T. Vanderschueren, T. Verdonck, and W. Verbeke, \"Robust\nInstance-dependent Cost-sensitive Learning,\" presented at the 32nd Eu-\nropean Conference on Operational Research (EURO), Espoo, Finland,\nJul. 3–6, 2022.\n241\n  Code availability\nGitHub repositories\nThe code and supplementary documentation for each chapter are available\nin the following GitHub repositories:\n⋅Chapter 3: https://github.com/SimonDeVos/turnover_prediction\n⋅Chapter 4: https://github.com/SimonDeVos/RecSys_SR\n⋅Chapter 5: https://github.com/SimonDeVos/Robust-IDCS\n⋅Chapter 6: https://github.com/SimonDeVos/DCF\n⋅Chapter 7: https://github.com/SimonDeVos/UMCT\nA list of our lab’s repositories is available at: https://github.com/VerbekeLab.\n243\n  Use of generative AI\nThe text, code, and images in this thesis are my own (unless otherwise\nspecified) and generative AI has only been used in accordance with the KU\nLeuven guidelines and appropriate references have been added. I have re-\nviewed and edited the content as needed and I take full responsibility for\nthe content of the thesis. Throughout this thesis, generative AI assistance\ntools (ChatGPT and Grammarly) were used to assist in the writing process\nto check the writing’s grammar, spelling, and readability.\n A full list of the doctoral dissertations from the Faculty of Economics and\nBusiness can be found at:\nhttps://www.kuleuven.be/english/research/\ndoctoraldefences/archive\n",
  "55": "SIMON DE VOS\nLinkedin: Simon De Vos GitHub: SimonDeVos\n+32 487 74 85 28 – devos.simon@gmail.com –\n10/03/1996 – Leuven\nSUMMARY\nSocial and technical business professional with a background in applied science and a strong interest in turning ML\nresearch into practical solutions. Experienced in data-driven decision-making and AI/ML development in cross-\nfunctional teams. Holds a B.Sc. and M.Sc. in Business Engineering and an Advanced Master’s in Artificial\nIntelligence. Currently finalizing a PhD in Data Science, focused on ML in business contexts. Eager to apply this\nexperience beyond business, including in healthcare or other high-impact domains. Amateur musician, runner,\nand fantasy reader.\nSKILLS\n· Technical: A/B testing, Artificial intelligence, Business process management (BPM), Causal machine learning, Cost-\nsensitive learning, Data-driven decision-making, Decision-focused learning, Deep learning, Demographic parity, Dynamic\nhyperparameter tuning, HR analytics, Individual treatment effects, Fairness, (Mixed) (Integer) Linear Programming,\nMulti-task learning, Predictive analytics, Predictive process monitoring, Prescriptive analytics, Process mining, Rec-\nommender systems, Robust statistics, Time series forecasting, Uplift modeling\nSoft: Academic writing: wrote 8 scientific articles. Public speaking: presented at top-tier international conferences.\nMentoring: supervised 20+ master’s theses (50+ students).\nCross-functional collaboration: bridged business and\ntechnical teams at KU Leuven and Acerta. Business-to-tech communication (and vice-versa): explained complex ideas\nto non-technical managers and clients. Stakeholder management: aligning academic and company interests during PhD.\n· Tech stack: Python ••••• , SQL •••◦◦, Gurobi ••••◦, CPLEX ••◦◦◦, Power BI •••◦◦, Java ••◦◦◦, Prolog •◦◦◦◦\n· Languages: Dutch (native), English (full professional), French (working), Chinese (elementary)\nWORK EXPERIENCE\nAcerta - Data Scientist [Part-time]\nSep 2021 - Sep 2025\nBelgium\n· PhD industry partner (Baekeland Mandate), bridging academic research and business impact.\n· Developed employee journey mapping frameworks for internal use and client delivery.\n· Built predictive models for employee turnover — both tailored for individual organizations and a general model trained\non payroll data from 600+ companies, now used for lead-generation of analytics services.\n· Built a proof-of-concept recommender system for internal mobility to support talent reallocation.\n· Conducted a causal deep dive for a client to identify drivers of unwanted turnover to recommend interventions.\n· Led cross-functional focus groups (HR, IT, and data teams), and briefed C-level executives on strategic insights.\nBelgian Consulate in Shanghai - Flanders Investment and Trade [Intern]\nJuly 2018 - Aug 2018\nChina\n· Matched Flemish companies with Chinese partners; facilitated a deal for one of Belgium’s largest breweries.\n· Represented FIT at seminars and trade events.\n· Authored market reports on trade opportunities and sector trends.\nEDUCATION\nKU Leuven - Doctor of Philosophy, Data Science (PhD)\nSept 2021 - Sept 2025\nBelgium\n· Faculty of Economics and Business, Research Center for Information Systems Engineering (LIRIS),\nsupervised by Professor Wouter Verbeke\n· Title: Essays on data-driven decision support: Applications in HRM and methodological advances\n· Baekeland Mandate holder, Acerta as partner company, acquired ∼400K EUR in funding\n· Specific research interests: Fairness, Causal ML, Cost-Sensitive Learning, HR analytics\n· Authored papers in top-tier international journals and presented at conferences (see below).\n· Research stay at Humboldt University of Berlin, hosted by Professor Stefan Lessmann\n· Contributed to teaching through exam design, guest lectures at (advanced) master’s level, and assistantship for the\nmaster’s course Data Science for Business.\nKU Leuven - Advanced Master’s in Artificial Intelligence\nSept 2020 - June 2021\nBelgium\n· Major: Engineering and Computer Science (ECS)\n· Research Internship at LIRIS, KU Leuven: Instance-Dependent Cost-Sensitive Learning for Detecting Transfer Fraud\nKU Leuven - B.Sc and M.Sc. in Business Engineering\nSept 2015 - June 2020\nBelgium\n· Majored in Risk & Finance and Data Science.\n· Thesis: Populism and credit: Effect of the 2016 presidential campaign in the US on minorities’ access to mortgages\n· Master exchange semester: Stellenbosch University, South Africa (THE Africa ranking #2)\n· Bachelor exchange semester: Tsinghua University, Beijing, China (THE worldwide ranking #12)\n PUBLICATIONS\nS. De Vos, J. Van Belle, A. Algaba, W. Verbeke, S. Verboven, “Decision-centric fairness: Evaluation and optimization\nfor resource allocation problems,” arXiv, 2024. doi: 10.48550/arXiv.2504.20642. [Under revision at EJOR]\nS. De Vos, J. De Smedt, C. Wuytens, and W. Verbeke, “Leveraging Process Mining to Optimize Internal Employee\nMobility Strategies,” in Business Process Management Cases Vol. 3: Implementation in Practice, Cham, Switzerland:\nSpringer Nature, 2025, pp. 15–28. doi: 10.1007/ 978-3-031-80793-0. ISBN: 978-3-031-80792-3.\nJ. Peeperkorn, S. De Vos, “Achieving Group Fairness through Independence in Predictive Process Monitoring,” arXiv,\n2024. doi: 10.48550/arXiv.2412.04914 [Accepted at CAiSE 2025, main track]\nS. De Vos, C. Bockel-Rickermann, S. Lessmann, and W. Verbeke, “Uplift modeling with continuous treatments: A\npredict-then-optimize approach,” arXiv, 2024. doi: 10.48550/arXiv.2412.09232. [Under revision at EJOR]\nD. Caljon, J. Vercauteren, S. De Vos, W. Verbeke, and J. Van Belle, “Using dynamic loss weighting to boost improve-\nments in forecast stability,” arXiv, 2024. doi: 10.48550/arXiv.2409.18267. [Accepted at IJF]\nS. De Vos, C. Bockel-Rickermann, J. Van Belle, and W. Verbeke, “Predicting Employee Turnover: Scoping and\nBenchmarking the State-of-the-Art,” Business & Information Systems Engineering, 2024. doi: 10.1007/ s12599-024-\n00898-z.\nS. De Vos, J. De Smedt, M. Verbruggen, and W. Verbeke, “Data-driven internal mobility: Similarity regularization\ngets the job done,” Knowledge-Based Systems, vol. 295, Art. no. 111824, 2024. doi: 10.1016/ j.knosys.2024.111824.\nS. De Vos, T. Vanderschueren, T. Verdonck, and W. Verbeke, “Robust instance-dependent cost-sensitive classification,”\nAdvances in Data Analysis and Classification, 2023. doi: 10.1007/ s11634-022-00533-3.\nA full publication list is available on Google Scholar.\nMISCELLANEOUS\nAcademic Services\n· Conducted over 15+ peer reviews for leading journals and conferences, including ICML, NeurIPS, Management Science,\nEuropean Journal of Operational Research, Scientific Reports, Omega, Journal of Business Analytics, Engineering\nApplications of Artificial Intelligence, Decision Analytics Journal, and Array.\nGrants\n· Secured ∼400K EUR in funding through Flanders Innovation & Entrepreneurship (VLAIO – HBC.2021.0833) for a\n4-year multidisciplinary research project on HR analytics.\n· Awarded a 3,400 EUR grant for an exchange at Stellenbosch University through the Priority Country Program.\nPERSONAL\nAmateur Musician\n· Instruments: Trombone, oboe, and cor anglais.\n· 2022–present: Founded a 6-piece jazz/funk band (Spoorvos) where I play trombone and compose/adapt arrangements.\n· 2018–2020: Member of KU Leuven Student Symphonic Orchestra.\nYouth Work - Chiro Vlierbeek\n· 2014-2019: Led weekly activities for 200+ children and organized large-scale events.\n· 2015-2017: Group leadership - Coordinating and motivating a group of 40 peers\nOther interests\n· Regularly play strategic board games; currently involved in multiple long-form games.\n· Enjoy reading both fiction and non-fiction — from data science literature to epic multi-volume fantasy.\n· Recreational runner; recently completed my first marathon.\n"
}